{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70274,"status":"ok","timestamp":1672722664164,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"},"user_tz":300},"id":"N3FRzwgslIOS","outputId":"36316b2b-179e-4f70-a460-458999a55a7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/My Drive/GraphRL_v2/')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64652,"status":"ok","timestamp":1672722728810,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"},"user_tz":300},"id":"ZIISUviFqNd8","outputId":"471c92fa-63bd-481b-a863-e5e9ad6ebea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting networkx==2.6.3\n","  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 6.7 MB/s \n","\u001b[?25hInstalling collected packages: networkx\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 2.8.8\n","    Uninstalling networkx-2.8.8:\n","      Successfully uninstalled networkx-2.8.8\n","Successfully installed networkx-2.6.3\n","\u001b[K     |████████████████████████████████| 9.4 MB 6.9 MB/s \n","\u001b[K     |████████████████████████████████| 4.5 MB 7.6 MB/s \n","\u001b[K     |████████████████████████████████| 512 kB 6.9 MB/s \n","\u001b[K     |████████████████████████████████| 280 kB 51.4 MB/s \n","\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 74 kB 2.3 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 47 kB 3.7 MB/s \n","\u001b[?25h  Building wheel for Ripser (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Imports\n","\n","!pip install networkx==2.6.3\n","import networkx as nx\n","import itertools\n","from functools import lru_cache\n","import random\n","import numpy as np\n","import numpy.matlib\n","from numpy import inf, ix_\n","import time\n","import copy\n","from copy import deepcopy\n","import time\n","from typing import NamedTuple\n","from tqdm import tqdm\n","import glob\n","import json\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","\n","plt.rcParams[\"animation.html\"] = \"jshtml\"\n","\n","import os\n","\n","import torch # get version using torch.__version__ for PyG wheels\n","import torch.nn as nn\n","import torch.nn.functional as F\n","os.environ['TORCH'] = torch.__version__\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","\n","import torch_geometric\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, SAGEConv\n","from torch_geometric import utils, transforms\n","\n","!pip install -q Cython\n","!pip install -q Ripser\n","\n","from ripser import ripser\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J0UzSHnUqPXn"},"outputs":[],"source":["#@title Environments\n","\n","@lru_cache(maxsize = 100000)\n","def get_NX_subgraph(frozen_set_of_nodes, graph_NX):\n","\n","  return graph_NX.subgraph(list(frozen_set_of_nodes))\n","\n","@lru_cache(maxsize = 500000)\n","def get_PyG_subgraph(frozen_set_of_nodes, graph_NX):\n","\n","  subgraph_NX = get_NX_subgraph(frozen_set_of_nodes, graph_NX)\n","  subgraph_PyG = utils.from_networkx(subgraph_NX, group_node_attrs = all)\n","\n","  return subgraph_PyG\n","\n","@lru_cache(maxsize = 100000)\n","def compute_feature_value(feature_function, graph_NX):\n","\n","  return feature_function(graph_NX)\n","\n","@lru_cache(maxsize = 100000)\n","def get_neighbors(frozen_set_of_nodes, graph_NX):\n","  \"\"\"\n","  Returns the neighborhood of a set of nodes.\n","  \"\"\"\n","\n","  nodes = list(frozen_set_of_nodes)\n","  all_neighbors = set()\n","\n","  for node in nodes:\n","    neighbors = set([n for n in graph_NX.neighbors(node)])\n","    all_neighbors.update(neighbors) \n","    \n","  all_neighbors = all_neighbors - set(nodes) # remove input nodes from their own neighborhood\n","\n","  return list(all_neighbors)\n","\n","class GraphEnvironment():\n","  \n","  def __init__(self, ID, graph_NX, steps_per_episode, feature):\n","    super().__init__()\n","\n","    self.ID = ID # identifier for the environment\n","\n","    self.graph_NX = graph_NX # environment graph (NetworkX Graph object)  \n","    self.steps_per_episode = steps_per_episode\n","    self.num_nodes = self.graph_NX.number_of_nodes()\n","    self.visited = [random.choice(list(self.graph_NX.nodes()))] # store a list of visited nodes\n","    self.state_NX = get_NX_subgraph(frozenset(self.visited), self.graph_NX)\n","    self.feature_function = feature # handle to network feature-of-interest\n","    \n","  def step(self, action):\n","    \"\"\"\n","    Execute an action in the environment, i.e. visit a new node.\n","    \"\"\"\n","\n","    assert action in self.get_actions(self.visited), \"Invalid action!\"\n","    self.visited = self.visited + [action] # add new node to list of visited nodes\n","    self.state_NX = get_NX_subgraph(frozenset(self.visited), self.graph_NX)\n","    reward = self.compute_reward()\n","    terminal = self.is_terminal()\n","\n","    return self.get_state_dict(), reward, terminal, self.get_info()\n","\n","  def unstep(self):\n","    \"\"\"\n","    Undo the most recent action. Multiple calls will undo multiple actions.\n","    \"\"\"\n","\n","    self.visited = self.visited[:-1] # remove most recently visited node\n","    self.state_NX = get_NX_subgraph(frozenset(self.visited), self.graph_NX)\n","\n","    return self.get_state_dict(), self.get_info()\n","  \n","  def is_terminal(self):\n","\n","    case_1 = len(self.visited) == self.steps_per_episode\n","    case_2 = len(self.visited) == self.num_nodes\n","    \n","    return bool(case_1 or case_2)\n","\n","  def compute_reward(self):\n","    \n","    return compute_feature_value(self.feature_function, self.state_NX)\n","\n","  def reset(self):\n","    \"\"\"\n","    Reset to initial state.\n","    \"\"\"\n","\n","    self.visited = [random.choice(list(self.graph_NX.nodes()))] # empty the list of visited nodes\n","    self.state_NX = get_NX_subgraph(frozenset(self.visited), self.graph_NX)\n","    terminal = False\n","\n","    return self.get_state_dict(), terminal, self.get_info()\n","\n","  def get_state_dict(self):\n","\n","    return {'visited': self.visited, \n","            'state_NX': self.state_NX}\n","      \n","  def get_info(self):\n","\n","    return {'environment_ID': self.ID, # useful for DQN training\n","            'feature_value': compute_feature_value(self.feature_function, self.state_NX)}\n","  \n","  def get_actions(self, visited_nodes):\n","    \"\"\" \n","    Returns available actions given a list of visited nodes.\n","    \"\"\"\n","\n","    # get neighbors of most recently visited node\n","    actions = get_neighbors(frozenset(visited_nodes[-1:]), self.graph_NX)\n","    # remove the set of already visited nodes\n","    actions = set(actions) - set(visited_nodes)\n","    # if no neighbors are available to visit then allow jumps to distant nodes\n","    if not actions: \n","      actions = set(self.graph_NX.nodes()) - set(visited_nodes)\n","\n","    return list(actions)\n","  \n","  def render(self):\n","    \"\"\"\n","    Render current state to the screen.\n","    \"\"\"\n","\n","    plt.figure()\n","    nx.draw(self.state_NX, with_labels = True)\n","\n","class MultipleEnvironments():\n","\n","  def __init__(self, environments):\n","    \n","    self.environments = environments\n","    self.num_environments = len(self.environments)\n","\n","  def reset(self):\n","\n","    state_dicts = []\n","    terminals = []\n","    all_info = []\n","\n","    for environment in self.environments:\n","      state_dict, terminal, info = environment.reset()\n","      state_dicts.append(state_dict)\n","      terminals.append(terminal)\n","      all_info.append(info)\n","\n","    return state_dicts, terminals, all_info\n","  \n","  def step(self, actions):\n","\n","    state_dicts = []\n","    rewards = []\n","    terminals = []\n","    all_info = []\n","\n","    for idx, environment in enumerate(self.environments):\n","      state_dict, reward, terminal, info = environment.step(actions[idx])\n","      state_dicts.append(state_dict)\n","      rewards.append(reward)\n","      terminals.append(terminal)\n","      all_info.append(info)\n","\n","    return state_dicts, rewards, terminals, all_info\n","  \n","  def __len__(self):\n","    return self.num_environments"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zQxsBrFvqTiW"},"outputs":[],"source":["#@title Baseline Agents\n","\n","class RandomAgent():\n","  \"\"\"\n","  RandomAgent() chooses an action at random. The agent is not deterministic.\n","  \"\"\"\n","  \n","  def __init__(self):\n","    super().__init__()\n","    \n","    self.environments = None # should be instance of MultipleEnvironments() class\n","    self.is_trainable = False # useful to manage control flow during simulations\n","\n","  def choose_action(self):\n","\n","    if not self.environments:\n","      assert False, \"Supply environment(s) for the agent to interact with.\"\n","\n","    actions = []\n","\n","    for environment in self.environments.environments:\n","      available_actions = environment.get_actions(environment.visited)\n","      action = random.choice(available_actions)\n","      actions.append(action)\n","\n","    return actions\n","\n","class HighestDegreeAgent():\n","  \"\"\"\n","  HighestDegreeAgent() chooses the action with the highest node degree. The \n","  agent is deterministic.\n","  \"\"\"\n","\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.environments = None # should be instance of MultipleEnvironments() class\n","    self.is_trainable = False # useful to manage control flow during simulations\n","\n","  def choose_action(self):\n","\n","    if not self.environments:\n","      assert False, \"Supply environment(s) for the agent to interact with.\"\n","\n","    actions = []\n","\n","    for environment in self.environments.environments:\n","      available_actions = environment.get_actions(environment.visited)\n","      all_degrees = list(zip(*(environment.graph_NX.degree(available_actions))))[1]\n","      action_idx = all_degrees.index(max(all_degrees)) # first largest when ties\n","      action = available_actions[action_idx]\n","      actions.append(action)\n","\n","    return actions\n","\n","class LowestDegreeAgent():\n","  \"\"\"\n","  LowestDegreeAgent() chooses the action with the lowest node degree. The \n","  agent is deterministic.\n","  \"\"\"\n","\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.environments = None # should be instance of MultipleEnvironments() class\n","    self.is_trainable = False # useful to manage control flow during simulations\n","\n","  def choose_action(self):\n","\n","    if not self.environments:\n","      assert False, \"Supply environment(s) for the agent to interact with.\"\n","\n","    actions = []\n","\n","    for environment in self.environments.environments:\n","      available_actions = environment.get_actions(environment.visited)\n","      all_degrees = list(zip(*(environment.graph_NX.degree(available_actions))))[1]\n","      action_idx = all_degrees.index(min(all_degrees)) # first smallest when ties\n","      action = available_actions[action_idx]\n","      actions.append(action)\n","\n","    return actions\n","\n","class GreedyAgent():\n","  \"\"\"\n","  GreedyAgent() chooses the action that would result in the greatest reward.\n","  The agent uses a copy of the environment to simulate each available action and \n","  returns the best performing action. The agent is deterministic.\n","  \"\"\"\n","\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.environments = None # should be instance of MultipleEnvironments() class\n","    self.is_trainable = False # useful to manage control flow during simulations\n","\n","  def choose_action(self):\n","\n","    if not self.environments:\n","      assert False, \"Supply environment(s) for the agent to interact with.\"\n","\n","    actions = []\n","\n","    for environment in self.environments.environments:\n","      available_actions = environment.get_actions(environment.visited)\n","      best_reward = float('-inf')\n","      best_action = None\n","\n","      for action in available_actions:\n","        state_dict, reward, terminal, info = environment.step(action)\n","\n","        if reward > best_reward:\n","          best_reward = reward\n","          best_action = action\n","        \n","        environment.unstep() # undo the action\n","\n","      actions.append(best_action)\n","\n","    return actions"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"i--bwPnhqYFE"},"outputs":[],"source":["#@title DQN Agent\n","\n","class GNN(nn.Module):\n","\n","  def __init__(self, hyperparameters):\n","    super().__init__()\n","    \n","    self.conv1 = SAGEConv(\n","        hyperparameters['num_node_features'],\n","        hyperparameters['GNN_latent_dimensions'],\n","        aggr = 'mean')\n","    self.conv2 = SAGEConv(\n","        hyperparameters['GNN_latent_dimensions'],\n","        hyperparameters['GNN_latent_dimensions'],\n","        aggr = 'mean')\n","    self.conv3 = SAGEConv(\n","        hyperparameters['GNN_latent_dimensions'],\n","        hyperparameters['embedding_dimensions'],\n","        aggr = 'mean')\n","\n","  def forward(self, x, edge_index, batch = None):\n","\n","    x = self.conv1(x, edge_index)\n","    x = F.relu(x)\n","    x = self.conv2(x, edge_index)\n","    x = F.relu(x) \n","    x = self.conv3(x, edge_index)\n","    x = F.relu(x) # node embeddings\n","    x = torch_geometric.nn.global_add_pool(x, batch = batch) # graph embedding\n","\n","    return x\n","\n","class QN(nn.Module):\n","\n","  def __init__(self, hyperparameters):\n","    super().__init__()\n","\n","    self.fc1 = nn.Linear(hyperparameters['embedding_dimensions'], \n","                         hyperparameters['QN_latent_dimensions'])\n","    self.fc2 = nn.Linear(hyperparameters['QN_latent_dimensions'], 1)\n","\n","  def forward(self, x):\n","\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","\n","    return x\n","\n","class DQNAgent():\n","\n","  def __init__(self, embedding_module, q_net, \n","               replay_buffer, train_start, batch_size, learn_every,\n","               optimizer, \n","               epsilon, epsilon_decay_rate, epsilon_min):\n","    super().__init__()\n","\n","    self.environments = None # should be instance of MultipleEnvironments() class\n","    self.is_trainable = True # useful to manage control flow during simulations\n","    \n","    self.embedding_module = embedding_module\n","    self.q_net = q_net\n","    \n","    self.target_embedding_module = deepcopy(embedding_module)\n","    self.target_q_net = deepcopy(q_net)\n","    \n","    # disable gradients for target networks\n","    for parameter in self.target_embedding_module.parameters():\n","      parameter.requires_grad = False\n","\n","    for parameter in self.target_q_net.parameters():\n","      parameter.requires_grad = False\n","    \n","    self.replay_buffer = replay_buffer\n","    self.train_start = train_start # specify burn-in period\n","    self.batch_size = batch_size\n","    self.learn_every = learn_every # steps between updates to target nets\n","\n","    self.optimizer = optimizer\n","\n","    self.epsilon = epsilon # probability with which to select a non-greedy action\n","    self.epsilon_decay_rate = epsilon_decay_rate\n","    self.epsilon_min = epsilon_min\n","\n","    self.step = 0\n","\n","  def choose_action(self):\n","    \"\"\"\n","    Choose an action to perform for each environment in self.environments.\n","    \"\"\"\n","\n","    if not self.environments:\n","      assert False, \"Supply environment(s) for the agent to interact with.\"\n","\n","    actions = []\n","\n","    for environment in self.environments.environments:\n","      available_actions = environment.get_actions(environment.visited)\n","      new_subgraphs = [] # list to store all possible next states\n","\n","      for action in available_actions:\n","        visited_nodes_new = environment.visited + [action]\n","        new_subgraph = get_PyG_subgraph(frozenset(visited_nodes_new), environment.graph_NX)\n","        new_subgraphs.append(new_subgraph)\n","\n","      # create a batch to allow for a single forward pass\n","      batch = Batch.from_data_list(new_subgraphs)\n","\n","      # gradients for the target networks are disabled\n","      with torch.no_grad(): # redundant because gradients are already disabled\n","        q_values = self.target_q_net(self.target_embedding_module(batch.x, \n","                                                                  batch.edge_index, \n","                                                                  batch.batch))\n","      if torch.rand(1) < self.epsilon: # explore\n","        action = np.random.choice(available_actions)\n","      else: # exploit\n","        action_idx = torch.argmax(q_values).item()\n","        action = available_actions[action_idx]\n","\n","      actions.append(action)\n","\n","    return actions\n","\n","  def train(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n","\n","    self.replay_buffer.add(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n","    self.step += 1\n","    \n","    if self.step < self.train_start: # inside the burn-in period\n","      return \n","\n","    # (1) Get lists of experiences from memory\n","    states, actions, next_states, rewards, discounts, all_info = self.replay_buffer.sample(self.batch_size)\n","    \n","    # (2) Build state + action = new_subgraph (technically identical to next state)\n","    new_subgraphs = []\n","    for idx, state_dict in enumerate(states):\n","      visited_nodes_new = state_dict['visited'] + [actions[idx]]\n","      assert visited_nodes_new == next_states[idx]['visited'], \"train() assertion failed.\"\n","      new_subgraph = get_PyG_subgraph(frozenset(visited_nodes_new), \n","                                      self.environments.environments[all_info[idx]['environment_ID']].graph_NX)\n","      new_subgraphs.append(new_subgraph)\n","\n","    batch = Batch.from_data_list(new_subgraphs)\n","\n","    # (3) Pass batch of next_state subgraphs through ANN to get predicted q-values\n","    q_predictions = self.q_net(self.embedding_module(batch.x, \n","                                                     batch.edge_index, \n","                                                     batch.batch))\n","\n","    # (4) Compute target q-values for batch\n","    q_targets = []\n","    for idx, next_state_dict in enumerate(next_states):\n","      available_actions = self.environments.environments[all_info[idx]['environment_ID']].get_actions(next_state_dict['visited'])\n","\n","      if available_actions: # terminal states have no available actions, redundant in a lot of settings\n","\n","        new_subgraphs = [] # each available action results in a new state\n","\n","        for action in available_actions:\n","          visited_nodes_new = next_state_dict['visited'] + [action]\n","          new_subgraph = get_PyG_subgraph(frozenset(visited_nodes_new), \n","                                          self.environments.environments[all_info[idx]['environment_ID']].graph_NX)\n","          new_subgraphs.append(new_subgraph)\n","\n","        batch = Batch.from_data_list(new_subgraphs)\n","\n","        with torch.no_grad(): # technically, no_grad() is unnecessary\n","          q_target = self.target_q_net(self.target_embedding_module(batch.x, \n","                                                                    batch.edge_index, \n","                                                                    batch.batch))\n","          q_target = q_target.max().view(-1, 1) # get the largest next q-value\n","          q_target = rewards[idx] + discounts[idx] * q_target\n","          q_targets.append(q_target)\n","\n","      else:\n","        q_targets.append(rewards[idx])\n","\n","    q_targets = torch.Tensor(q_targets).view(-1, 1)\n","      \n","    # (5) Compute MSE loss between predicted and target q-values\n","    loss = F.mse_loss(q_predictions, q_targets).mean()\n","\n","    # (6) Backpropagate gradients\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","\n","    # (7) Copy parameters from source to target networks\n","    if self.step % self.learn_every == 0: \n","      copy_parameters_from_to(self.embedding_module, self.target_embedding_module)\n","      copy_parameters_from_to(self.q_net, self.target_q_net)\n","      \n","    # (8) Decrease exploration rate\n","    self.epsilon *= self.epsilon_decay_rate\n","    self.epsilon = max(self.epsilon, self.epsilon_min)\n","\n","    return loss.item() # for logging"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tXMIVOLKqdJY"},"outputs":[],"source":["#@title Helper Functions: Miscellaneous\n","\n","def initialize_weights(m):\n","  \"\"\"\n","  Xavier initialization of model weights.\n","  \"\"\"\n","\n","  if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n","    m.weight.data.fill_(1.0)\n","    m.bias.data.zero_()\n","\n","  elif isinstance(m, SAGEConv):\n","    m.lin_l.weight.data = nn.init.xavier_uniform_(\n","        m.lin_l.weight.data, gain = nn.init.calculate_gain('relu'))\n","    \n","    if m.lin_l.bias is not None:\n","      m.lin_l.bias.data.zero_()\n","\n","    m.lin_r.weight.data = nn.init.xavier_uniform_(\n","        m.lin_r.weight.data, gain = nn.init.calculate_gain('relu'))\n","    \n","    if m.lin_r.bias is not None: # redundant\n","      m.lin_r.bias.data.zero_()\n","\n","  elif isinstance(m, nn.Linear):\n","    m.weight.data = nn.init.xavier_uniform_(\n","        m.weight.data, gain = nn.init.calculate_gain('relu'))\n","    \n","    if m.bias is not None:\n","      m.bias.data.zero_()\n","\n","def compute_Frobenius_norm(network):\n","    \"\"\"\n","    Compute the Frobenius norm of all network tensors.\n","    \"\"\"\n","    norm = 0.0\n","\n","    for name, param in network.named_parameters():\n","        norm += torch.norm(param).data  \n","               \n","    return norm.item()\n","\n","def copy_parameters_from_to(source_network, target_network):\n","  \"\"\"\n","  Update the parameters of the target network by copying values from the source\n","  network.\n","  \"\"\"\n","\n","  for source, target in zip(source_network.parameters(), target_network.parameters()):\n","    target.data.copy_(source.data)\n","\n","  return\n","\n","def average_area_under_the_curve(all_feature_values):\n","  \"\"\"\n","  Returns the average area under the curve given a list of list of feature \n","  values. Each list inside all_feature_values corresponds to an environment. \n","  Each list inside that list corresponds to an episode. Each element of the \n","  inner list is a feature value at a given step during an episode.\n","  \"\"\"\n","\n","  all_areas = []\n","  for env_results in all_feature_values:\n","    areas = [sum(feature_values) for feature_values in env_results]\n","    all_areas.append(sum(areas)/len(areas))\n","  \n","  return sum(all_areas)/len(all_areas)\n","\n","def generate_video(plotting_dict):\n","\n","  feature_values_random = plotting_dict['random']\n","  feature_values_degree = plotting_dict['degree']\n","  feature_values_greedy = plotting_dict['greedy']\n","  feature_values_DQN = np.array(plotting_dict['DQN'])\n","\n","  xlim = feature_values_DQN.shape[1]\n","  x = np.arange(xlim) # number of nodes\n","\n","  ylim = max(max(feature_values_random), \n","             max(feature_values_degree), \n","             max(feature_values_greedy), \n","             np.max(feature_values_DQN))\n","\n","  fig, ax = plt.subplots()\n","  ax.axis([0, xlim, 0, ylim + 0.01 * ylim])\n","\n","  line1, = ax.plot(x, feature_values_random, label = 'random', color = 'blue')\n","  line2, = ax.plot(x, feature_values_degree, label = 'max degree', color = 'orange')\n","  line3, = ax.plot(x, feature_values_greedy, label = 'greedy', color = 'green')\n","  line4, = ax.plot([], [], label = 'DQN', color = 'black')\n","\n","  ax.legend()\n","\n","  plt.xlabel('Step')\n","  plt.ylabel('Value')\n","\n","  def animate(i):\n","    line4.set_data(x, feature_values_DQN[i])\n","    \n","  anim_handle = animation.FuncAnimation(fig, animate, \n","                                        frames = len(feature_values_DQN),\n","                                        interval = 100,  \n","                                        blit = False, repeat = False, \n","                                        repeat_delay = 10000)\n","  plt.close() # do not show extra figure\n","\n","  return anim_handle\n","\n","def node_featurizer(graph_NX, mode = 'LDP'):\n","\n","  graph_NX = deepcopy(graph_NX)\n","\n","  attributes = {}\n","\n","  for node in graph_NX.nodes():\n","    \n","    node_attributes = {}\n","\n","    if mode == 'LDP':\n","\n","      neighborhood = list(set([n for n in graph_NX.neighbors(node)]))\n","\n","      if neighborhood:\n","        neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n","      else: # no neighbors\n","        neighborhood_degrees = [0]\n","\n","      node_attributes['feature_1'] = graph_NX.degree(node)\n","      node_attributes['feature_2'] = min(neighborhood_degrees)\n","      node_attributes['feature_3'] = max(neighborhood_degrees)\n","      node_attributes['feature_4'] = float(np.mean(neighborhood_degrees))\n","      node_attributes['feature_5'] = float(np.std(neighborhood_degrees))\n","\n","    if mode == 'random':\n","\n","      node_attributes['feature_1'] = random.random()\n","      node_attributes['feature_2'] = random.random()\n","      node_attributes['feature_3'] = random.random()\n","      node_attributes['feature_4'] = random.random()\n","      node_attributes['feature_5'] = random.random()\n","\n","    if mode == 'constant':\n","\n","      node_attributes['feature_1'] = 1.0\n","      node_attributes['feature_2'] = 1.0\n","      node_attributes['feature_3'] = 1.0\n","      node_attributes['feature_4'] = 1.0\n","      node_attributes['feature_5'] = 1.0\n","\n","    attributes[node] = node_attributes\n","    \n","  nx.set_node_attributes(graph_NX, attributes)\n","\n","  return graph_NX\n","\n","def node_defeaturizer(graph_NX):\n","\n","  graph_NX = deepcopy(graph_NX)\n","\n","  attrs = set([k for n in graph_NX.nodes for k in graph_NX.nodes[n].keys()])\n","\n","  for (n, d) in graph_NX.nodes(data = True):\n","\n","    for attr in attrs:\n","      del d[attr]\n","\n","  return graph_NX\n","\n","class ReplayBuffer():\n","  \n","  def __init__(self, buffer_size):\n","\n","    self.buffer_size = buffer_size\n","    self.ptr = 0 # index to latest experience in memory\n","    self.num_experiences = 0 # number of experiences stored in memory\n","    self.states = [None] * self.buffer_size\n","    self.actions = [None] * self.buffer_size\n","    self.next_states = [None] * self.buffer_size\n","    self.rewards = [None] * self.buffer_size\n","    self.discounts = [None] * self.buffer_size\n","    self.all_info = [None] * self.buffer_size\n","\n","  def add(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n","\n","    # check if arguments are lists\n","    if not isinstance(state_dicts, list): # i.e. a single experience\n","      state_dicts = [state_dicts]\n","      actions = [actions]\n","      next_state_dicts = [next_state_dicts]\n","      rewards = [rewards]\n","      discounts = [discounts]\n","      all_info = [all_info]\n","\n","    for i in range(len(state_dicts)):\n","      self.states[self.ptr] = state_dicts[i]\n","      self.actions[self.ptr] = actions[i]\n","      self.next_states[self.ptr] = next_state_dicts[i]\n","      self.rewards[self.ptr] = rewards[i]\n","      self.discounts[self.ptr] = discounts[i]\n","      self.all_info[self.ptr] = all_info[i]\n","      \n","      if self.num_experiences < self.buffer_size:\n","        self.num_experiences += 1\n","\n","      self.ptr = (self.ptr + 1) % self.buffer_size \n","      # if (ptr + 1) exceeds buffer size then begin overwriting older experiences\n","\n","  def sample(self, batch_size):      \n","\n","    indices = np.random.choice(self.num_experiences, batch_size)   \n","    states = [self.states[index] for index in indices] \n","    actions = [self.actions[index] for index in indices] \n","    next_states = [self.next_states[index] for index in indices] \n","    rewards = [self.rewards[index] for index in indices] \n","    discounts = [self.discounts[index] for index in indices] \n","    all_info = [self.all_info[index] for index in indices] \n","    \n","    return states, actions, next_states, rewards, discounts, all_info\n","\n","def save_checkpoint(embedding_module, q_net, \n","                    optimizer, \n","                    replay_buffer, \n","                    returns, feature_values_train,\n","                    validation_steps, validation_scores, feature_values_val,\n","                    step,\n","                    save_path):\n","  \n","  save_dict = {'embedding_module_state_dict': embedding_module.state_dict(),\n","               'q_net_state_dict': q_net.state_dict(),\n","               'optimizer_state_dict': optimizer.state_dict(),\n","               'buffer_ptr': replay_buffer.ptr,\n","               'buffer_num_experience': replay_buffer.num_experiences,\n","               'buffer_states': replay_buffer.states,\n","               'buffer_actions': replay_buffer.actions,\n","               'buffer_next_states': replay_buffer.next_states,\n","               'buffer_rewards': replay_buffer.rewards,\n","               'buffer_discounts': replay_buffer.discounts,\n","               'buffer_all_info': replay_buffer.all_info,\n","               'returns': returns,\n","               'feature_values_train': feature_values_train,\n","               'validation_steps': validation_steps,\n","               'validation_scores': validation_scores,\n","               'feature_values_val': feature_values_val,\n","               'step': step}\n","\n","  torch.save(save_dict, save_path)\n","\n","def load_checkpoint(load_path, embedding_module, q_net, \n","                    optimizer = None, \n","                    replay_buffer = None):\n","  \n","  checkpoint = torch.load(load_path)\n","\n","  embedding_module.load_state_dict(checkpoint['embedding_module_state_dict'])\n","  q_net.load_state_dict(checkpoint['q_net_state_dict'])\n","\n","  if optimizer:\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","  if replay_buffer:\n","    replay_buffer.ptr = checkpoint['buffer_ptr']\n","    replay_buffer.num_experiences = checkpoint['buffer_num_experience']\n","    replay_buffer.states = checkpoint['buffer_states']\n","    replay_buffer.actions = checkpoint['buffer_actions']\n","    replay_buffer.next_states = checkpoint['buffer_next_states']\n","    replay_buffer.rewards = checkpoint['buffer_rewards']\n","    replay_buffer.discounts = checkpoint['buffer_discounts']\n","    replay_buffer.all_info = checkpoint['buffer_all_info']\n","\n","  returns = checkpoint['returns']\n","  feature_values_train = checkpoint['feature_values_train']\n","  validation_steps = checkpoint['validation_steps']\n","  validation_scores = checkpoint['validation_scores']\n","  feature_values_val = checkpoint['feature_values_val']\n","\n","  train_results = {'returns': returns,\n","                   'feature_values_train': feature_values_train}\n","\n","  val_results = {'validation_steps': validation_steps,\n","                 'validation_scores': validation_scores,\n","                 'feature_values_val': feature_values_val}\n","\n","  return train_results, val_results"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"J17IRcgoq2Ar"},"outputs":[],"source":["#@title Helper Functions: Simulation\n","\n","def simulate(agent, environments, num_episodes = 100, verbose = True):\n","  \"\"\"\n","  Simulate agent in multiple environments for a specified number of episodes.\n","  This function assumes that episodes in each environment have the same number \n","  of steps.\n","  \"\"\"\n","\n","  agent = deepcopy(agent)\n","  agent.environments = environments \n","\n","  all_feature_values = [[] for _ in range(environments.num_environments)]\n","\n","  state_dicts, terminals, all_info = environments.reset()\n","\n","  pbar = tqdm(range(num_episodes), unit = 'Episode', disable = not verbose)\n","\n","  for _ in pbar:\n","    episode_feature_values = [[] for _ in range(environments.num_environments)]\n","\n","    while not any(terminals):\n","      assert any(terminals) == all(terminals), \"Simulation error!\"\n","      actions = agent.choose_action() # choose an action for each environment\n","      state_dicts, rewards, terminals, all_info = environments.step(actions)\n","      episode_feature_values.append(all_info)\n","\n","      for idx, info in enumerate(all_info):\n","        episode_feature_values[idx].append(info['feature_value'])\n","        \n","    for idx, terminal in enumerate(terminals):\n","      if terminal: # this should always be true when control is here\n","        all_feature_values[idx].append(episode_feature_values[idx])\n","    \n","    state_dicts, terminals, all_info = environments.reset()\n","\n","  return all_feature_values\n","\n","def learn_environments(agent, train_environments, val_environments, \n","                       num_steps, discount_factor, val_every,\n","                       save_folder = None,\n","                       log_val_results = True, verbose = True):\n","  \"\"\"\n","  Train agent on multiple environments by simulating agent-environment \n","  interactions for a specified number of steps.\n","  \"\"\"\n","\n","  agent.environments = train_environments # supply the agent with environments\n","\n","  # training logs\n","  all_episode_returns_train = [[] for _ in range(train_environments.num_environments)]\n","  all_episode_feature_values_train = [[] for _ in range(train_environments.num_environments)]\n","  episode_returns_train = [0] * train_environments.num_environments\n","  episode_feature_values_train = [[] for _ in range(train_environments.num_environments)]\n","\n","  # validation logs\n","  all_episode_feature_values_val = []\n","  if not val_environments: \n","    log_val_results = False\n","  val_steps = []\n","  val_scores = []\n","  val_score = -float('inf')\n","\n","  state_dicts, terminals, all_info = train_environments.reset()\n","\n","  pbar = tqdm(range(num_steps), unit = 'Step', disable = not verbose)\n","\n","  for step in pbar:\n","    actions = agent.choose_action() # choose an action for each environment\n","    next_state_dicts, rewards, terminals, all_info = train_environments.step(actions)\n","    episode_returns_train = [sum(x) for x in zip(rewards, episode_returns_train)]\n","\n","    for idx, info in enumerate(all_info):\n","      episode_feature_values_train[idx].append(info['feature_value'])\n","\n","    if agent.is_trainable:\n","      discounts = [discount_factor * (1 - terminal) for terminal in terminals]\n","      loss = agent.train(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n","      \n","      if log_val_results and step % val_every == 0: # evaluate validation performance \n","        all_feature_values_val = simulate(agent, val_environments,\n","                                          num_episodes = 10, verbose = False)\n","        val_score = average_area_under_the_curve(all_feature_values_val)\n","        val_steps.append(step)\n","        val_scores.append(val_score)\n","        all_episode_feature_values_val.append(all_feature_values_val)\n","\n","        if save_folder is not None: # save checkpoint\n","          save_path = os.path.join(save_folder, 'model_' + str(step) + '.pt')\n","          save_checkpoint(agent.embedding_module, agent.q_net, \n","                          agent.optimizer,\n","                          agent.replay_buffer,\n","                          all_episode_returns_train, all_episode_feature_values_train,\n","                          val_steps, val_scores, all_episode_feature_values_val, \n","                          step, \n","                          save_path)       \n","\n","      if loss: \n","        pbar.set_description('Loss: %0.5f, Val. Score: %0.5f' % (loss, val_score))\n","\n","      else: # no loss value is returned inside the burn-in period\n","        pbar.set_description('Loss: %0.5f, Val. Score: %0.5f' % (float('inf'), val_score))\n","\n","    state_dicts = next_state_dicts\n","\n","    for idx, terminal in enumerate(terminals):\n","      # if terminal then gather episode results for this environment and reset\n","      if terminal: \n","        all_episode_returns_train[idx].append(episode_returns_train[idx])\n","        episode_returns_train[idx] = 0\n","        all_episode_feature_values_train[idx].append(episode_feature_values_train[idx])\n","        episode_feature_values_train[idx] = []\n","        state_dict, terminal, info = train_environments.environments[idx].reset()\n","        state_dicts[idx] = state_dict\n","\n","  train_environments.reset()\n","\n","  train_results = {'returns': all_episode_returns_train,\n","                   'feature_values_train': all_episode_feature_values_train}\n","\n","  # validate final model and save checkpoint\n","  all_feature_values_val = simulate(agent, val_environments,\n","                                    num_episodes = 10, verbose = False)\n","  val_score = average_area_under_the_curve(all_feature_values_val)\n","  val_steps.append(num_steps)\n","  val_scores.append(val_score)\n","  all_episode_feature_values_val.append(all_feature_values_val)\n","\n","  if save_folder is not None: # save checkpoint\n","    save_path = os.path.join(save_folder, 'model_' + str(num_steps) + '.pt')\n","    save_checkpoint(agent.embedding_module, agent.q_net, \n","                    agent.optimizer,\n","                    agent.replay_buffer,\n","                    all_episode_returns_train, all_episode_feature_values_train,\n","                    val_steps, val_scores, all_episode_feature_values_val, \n","                    num_steps, \n","                    save_path)\n","\n","  val_results = {'validation_steps': val_steps,\n","                 'validation_scores': val_scores,\n","                 'feature_values_val': all_episode_feature_values_val}\n","\n","  return train_results, val_results"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9INNjqwEq5tu"},"outputs":[],"source":["#@title Helper Functions: Rewards\n","\n","def make_filtration_matrix(G):\n","    \"\"\"\n","    Takes in adjacency matrix and returns a filtration matrix for Ripser\n","    \"\"\"\n","\n","    N = G.shape[0]\n","    weighted_G = np.ones([N, N])\n","    for col in range(N):\n","        weighted_G[:col, col] = weighted_G[:col, col] * col\n","        weighted_G[col, :col] = weighted_G[col, :col] * col\n","    weighted_G += 1 # pushes second node's identifier to 2\n","    # removes diagonals, simultaneously resetting first node's identifier to 0\n","    weighted_G = np.multiply(G, weighted_G) \n","    # place 1 to N along the diagonal\n","    np.fill_diagonal(weighted_G, list(range(1, N + 1)))\n","    # set all zeros to be non-edges (i.e. at inf distance)\n","    weighted_G[weighted_G == 0] = np.inf\n","    # remove 1 from everywhere to ensure first node has identifier 0\n","    weighted_G -= 1\n","    \n","    return weighted_G\n","\n","def betti_numbers(G, maxdim = 2, dim = 1):\n","  \"\"\"\n","  Given a NetworkX graph object, computes number of topological cycles \n","  (i.e. Betti numbers) of various dimensions upto maxdim.\n","  \"\"\"\n","\n","  adj = nx.to_numpy_array(G)\n","  adj[adj == 0] = np.inf # set unconnected nodes to be infinitely apart\n","  np.fill_diagonal(adj, 1) # set diagonal to 1 to indicate all nodes are born at once\n","  bars = ripser(adj, distance_matrix = True, maxdim = maxdim)['dgms'] # returns barcodes\n","  bars_list = list(zip(range(maxdim + 1), bars))\n","  bettis_dict = dict([(dim, len(cycles)) for (dim, cycles) in bars_list])\n","\n","  return bettis_dict[dim] # return Betti number for dimension of interest\n","\n","def get_barcode(filt_mat, maxdim = 2):\n","    \"\"\"\n","    Calculates the persistent homology for a given filtration matrix\n","    ``filt_mat``, default dimensions 0 through 2. Wraps ripser.\n","    \"\"\"\n","\n","    b = ripser(filt_mat, distance_matrix = True, maxdim = maxdim)['dgms']\n","\n","    return list(zip(range(maxdim + 1), b))\n","\n","def betti_curves(bars, length):\n","    \"\"\"\n","    Takes in bars and returns the betti curves\n","    \"\"\"\n","\n","    bettis = np.zeros((len(bars), length))\n","\n","    for i in range(bettis.shape[0]):\n","        bn = bars[i][1]\n","\n","        for bar in bn:\n","            birth = int(bar[0])\n","            death = length+1 if np.isinf(bar[1]) else int(bar[1]+1)\n","            bettis[i][birth:death] += 1\n","\n","    return bettis\n","\n","def plot_bettis(bettis):\n","  \n","  N = bettis.shape[1]\n","  colors = ['xkcd:emerald green', 'xkcd:tealish', 'xkcd:peacock blue']\n","  \n","  for i in range(3):\n","    plt.plot(list(range(N)), bettis[i], color = colors[i], \n","             label = '$\\\\beta_{}$'.format(i), \n","             linewidth = 1)\n","  plt.xlabel('Nodes')\n","  plt.ylabel('Number of Cycles')\n","  plt.legend()\n","\n","TOL = 1E-8\n","\n","def repmat(M, m, n):\n","    return np.transpose(np.matlib.repmat(np.array([M]), m, n))\n","\n","def rate_distortion_upper_info(G, setting = 1):\n","\n","    assert setting == 1, \"Only setting 1 available.\"\n","    assert np.all(np.abs(G - G.T) < TOL), \"Network must be symmetric.\"\n","\n","    # network size\n","    N = len(G)\n","    E  = np.sum(G)/2\n","\n","    # variables to save\n","    S = np.zeros((N)) # upper bound on entropy rate after clustering\n","    S_low = np.zeros((N)) # lower bound on entropy rate\n","    clusters = [[] for i in range(N)] # clusters[n] lists the nodes in each of the n clusters\n","    Gs = [[] for i in range(N)] # Gs[n] is the joint transition probability matrix for n clusters\n","\n","    P_old = numpy.divide(G, np.transpose(np.matlib.repmat(np.array([np.sum(G, 1)]), N, 1)))\n","\n","    # compute steady-state probabilities (works for undirected and possibly disconnected networks)\n","    p_ss = np.sum(G, 1)/np.sum(G)\n","    p_ss_old = copy.copy(p_ss)\n","\n","    # compute initial entropy\n","    logP_old = np.log2(P_old, where = P_old > 0)\n","    logP_old[logP_old == -inf] = 0\n","    S_old = -np.sum(np.multiply(p_ss_old, np.sum(np.multiply(P_old, logP_old), 1)))\n","    P_joint = np.multiply(P_old, np.transpose(np.matlib.repmat(p_ss_old, N, 1)))\n","    P_low = P_old\n","\n","    # record initial values\n","    S[-1] = S_old\n","    S_low[-1] = S_old\n","    clusters[-1] = [[i] for i in range(N)]\n","    Gs[-1] = G\n","\n","    for n in reversed(range(2, N)):\n","        pairs = np.array(list(itertools.combinations([k for k in range(1, n + 2)], 2)))\n","        I = pairs[:, 0]\n","        J = pairs[:, 1]\n","\n","        # number of pairs\n","        num_pairs_temp = len(I)\n","\n","        # track all entropies\n","        S_all = np.zeros((num_pairs_temp))\n","\n","        for ind in range(num_pairs_temp):\n","                i = I[ind]\n","                j = J[ind]\n","                inds_not_ij = [v - 1 for v in  list(range(1, i)) + list(range(i + 1, j)) + list(range(j + 1, n + 2))]\n","\n","                # compute new stationary distribution\n","                p_ss_temp = [p_ss_old[inds_not_ij], p_ss_old[i - 1] + p_ss_old[j - 1]]\n","                \n","                # compute new transition probabilities\n","                P_temp_1 = np.sum(np.multiply(repmat(p_ss_old[inds_not_ij], 2, 1), P_old[ix_(inds_not_ij, [i - 1, j - 1])] ), 1)\n","                P_temp_1 = np.divide(P_temp_1, p_ss_temp[:-1])\n","\n","                P_temp_2 = np.sum(np.multiply(repmat(p_ss_old[[i - 1, j - 1]].T, n - 1, 1), P_old[ix_([i - 1, j - 1], inds_not_ij)]), 0)\n","                P_temp_2 = P_temp_2/p_ss_temp[-1]\n","            \n","                P_temp_3 = np.sum(np.sum(np.multiply(repmat(p_ss_old[[i - 1, j - 1]], 2, 1), P_old[ ix_([i - 1, j - 1], [i - 1, j - 1])])))\n","                P_temp_3 = P_temp_3/p_ss_temp[-1]\n","\n","                logP_temp_1 = np.log2(P_temp_1, where = P_temp_1 > 0)\n","                logP_temp_1[logP_temp_1 == -inf] = 0\n","                logP_temp_2 = np.log2(P_temp_2, where = P_temp_2 > 0)\n","                logP_temp_2[logP_temp_2 == -inf] = 0\n","                logP_temp_3 = np.array(np.log2(P_temp_3, where = P_temp_3 > 0))\n","                logP_temp_3[logP_temp_3 == -inf] = 0\n","\n","                # compute change in upper bound on mutual information\n","                dS = - np.sum(np.multiply(np.multiply(p_ss_temp[:-1], P_temp_1), logP_temp_1))  \n","                dS = dS - p_ss_temp[-1] * np.sum(np.multiply(P_temp_2, logP_temp_2))\n","                dS = dS - p_ss_temp[-1] * P_temp_3 * logP_temp_3 \n","                dS = dS + np.sum(np.multiply(np.multiply(p_ss_old, P_old[:, i - 1]), logP_old[:, i - 1]))\n","                dS = dS + np.sum(np.multiply(np.multiply(p_ss_old, P_old[:, j - 1]), logP_old[:, j - 1]))\n","                dS = dS + p_ss_old[i - 1] * np.sum(np.multiply(P_old[i - 1, :], logP_old[i - 1, :]))\n","                dS = dS + p_ss_old[j - 1] * np.sum(np.multiply(P_old[j - 1, :], logP_old[j - 1, :]))\n","                dS = dS - p_ss_old[i - 1] * (P_old[i - 1, i - 1] * logP_old[i - 1, i - 1] + P_old[i - 1, j - 1] * logP_old[i - 1, j - 1]) \n","                dS = dS - p_ss_old[j - 1] * (P_old[j - 1, j - 1] * logP_old[j - 1, j - 1] + P_old[j - 1, i - 1] * logP_old[j - 1, i - 1]) \n","                \n","                S_temp = S_old + dS\n","\n","                # track all entropies\n","                S_all[ind] = S_temp\n","\n","        # find minimum entropy\n","        min_inds = np.where(S_all == min(S_all))\n","        iidx = random.randint(0, len(min_inds[0]) - 1)\n","        min_ind = [min_inds[0][iidx]]\n","\n","        # save mutual information\n","        S_old = S_all[min_ind]\n","        S[n - 1] = S_old\n","        \n","        i_new = I[min_ind][0]\n","        j_new = J[min_ind][0]\n","        inds_not_ij = [v - 1 for v in list(range(1, i_new)) + list(range(i_new + 1, j_new)) + list(range(j_new + 1, n + 2))]\n","        p_ss_new = numpy.concatenate((p_ss_old[inds_not_ij], np.array([p_ss_old[i_new - 1] + p_ss_old[j_new - 1]])))\n","      \n","        P_joint = np.multiply(repmat(p_ss_old.T, n + 1, 1), P_old)\n","        AB = P_joint[ix_(inds_not_ij, inds_not_ij)]\n","        A = np.expand_dims(np.sum(P_joint[ix_(inds_not_ij, [i_new - 1, j_new - 1])], 1), 1)\n","        B = np.expand_dims(np.sum(P_joint[ix_([i_new - 1, j_new - 1], inds_not_ij)], 0), 0)\n","        C = np.sum(np.sum(P_joint[ix_([i_new - 1, j_new - 1], [i_new - 1, j_new - 1])]))\n","        P_joint = np.block([[AB, A], [B, C]])\n","\n","        P_old = np.divide(P_joint, repmat(p_ss_new.T, n, 1))\n","        p_ss_old = p_ss_new\n","\n","        logP_old = np.log2(P_old, where = P_old > 0)\n","        logP_old[logP_old == -inf] = 0\n","\n","        # record clusters and graph\n","        cls1_ind = [v - 1 for v in list(range(1, (i_new))) + list(range((i_new), (j_new - 1))) + list(range((j_new), (n + 1)))]\n","        cls1 = [clusters[n][v] for v in  cls1_ind]\n","        cls2 = [list(clusters[n][i_new - 1]) + list(clusters[n][j_new - 1])]\n","        clusters[n - 1] = cls1 + cls2\n","        Gs[n - 1] = P_joint * 2 * E\n","        sp = numpy.expand_dims(P_low[:, i_new - 1] + P_low[:, j_new - 1], 1)\n","        P_low = np.concatenate((P_low[:, cls1_ind], sp), 1)\n","        logP_low = np.log2(P_low, where = P_low > 0)\n","        logP_low[logP_low == -inf] = 0\n","        S_low[n-1] = -np.sum(np.multiply(p_ss, np.sum(np.multiply(P_low, logP_low), 1)))\n","    \n","    return S, S_low, clusters, Gs\n","\n","def compressibility(graph_NX):\n","  \"\"\"\n","  Computes network compressibility given a NetworkX graph object.\n","  \"\"\"\n","\n","  graph_NX_copy = nx.Graph(graph_NX) # make an unfrozen copy\n","  for node in graph_NX_copy.nodes(): # add self-loops\n","    graph_NX_copy.add_edge(node, node)\n","  G = nx.to_numpy_array(graph_NX_copy)\n","  S, S_low, clusters, Gs = rate_distortion_upper_info(G)\n","  C = np.mean(S[-1] - S)\n","\n","  return C"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yJSxdkL4rHbl"},"outputs":[],"source":["#@title Parameters from Darvariu et al. \n","\n","# Data\n","# |G_train| = 10000\n","# |G_val| = 100\n","# |G_test| = 100\n","\n","# Model\n","# 3 message passing rounds\n","# 128 hidden units in MLP\n","# linear exploration decay from 1 to 0.1 for steps/2 and then 0.1  \n","\n","# Training\n","# steps = 40000 (*1 or *2 or *5)\n","# gamma = 1 (finite horizon)\n","# learn_every = 50\n","# weights initialized using Glorot scheme\n","# learning rate = 0.0001\n","# rewards scaled by 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlKnhF02f7be"},"outputs":[],"source":["network_type = 'wikipedia' # wikipedia, synthetic_ER, synthetic_BA\n","size = 'large' # size of dataset; large, medium, small, mini\n","\n","feature = compressibility # betti_numbers, compressibility, nx.average_clustering\n","steps_per_episode = 10 # average KNOT session has ~9 unique node visits\n","\n","num_episodes = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cnUrdHIyyZhi"},"outputs":[],"source":["#@title Preparatory\n","\n","base_path = '/content/drive/My Drive/GraphRL_v2/'\n","\n","data_load_path = os.path.join(base_path, 'Environments', network_type + '_' + size + '.json')\n","with open(data_load_path, 'r') as f:\n","  all_data = json.load(f)\n","\n","train_data = all_data['train']\n","val_data = all_data['val']\n","test_data = all_data['test']\n","\n","# build training environments\n","\n","train_graphs = []\n","train_environments = []\n","\n","for idx in range(len(train_data)):\n","\n","  base_G = nx.node_link_graph(train_data[str(idx)])\n","  base_G = node_defeaturizer(base_G) # \n","  train_graphs.append(base_G)\n","\n","  G = node_featurizer(base_G, mode = 'LDP')\n","  environment = GraphEnvironment(idx, G, steps_per_episode, feature)\n","  train_environments.append(environment)\n","\n","train_environments = MultipleEnvironments(train_environments)\n","\n","# build validation environments\n","\n","val_graphs = []\n","val_environments = []\n","\n","for idx in range(len(val_data)):\n","\n","  base_G = nx.node_link_graph(val_data[str(idx)])\n","  base_G = node_defeaturizer(base_G)\n","  val_graphs.append(base_G)\n","\n","  G = node_featurizer(base_G, mode = 'LDP')\n","  environment = GraphEnvironment(idx, G, steps_per_episode, feature)\n","  val_environments.append(environment)\n","\n","val_environments = MultipleEnvironments(val_environments)\n","\n","# build test environments\n","\n","test_graphs = []\n","test_environments = []\n","\n","for idx in range(len(test_data)):\n","\n","  base_G = nx.node_link_graph(test_data[str(idx)])\n","  base_G = node_defeaturizer(base_G)\n","  test_graphs.append(base_G)\n","\n","  G = node_featurizer(base_G, mode = 'LDP')\n","  environment = GraphEnvironment(idx, G, steps_per_episode, feature)\n","  test_environments.append(environment)\n","\n","test_environments = MultipleEnvironments(test_environments)"]},{"cell_type":"code","source":["print(len(train_environments), len(val_environments), len(test_environments))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YD29hhTFTUqA","executionInfo":{"status":"ok","timestamp":1672722762828,"user_tz":300,"elapsed":25,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"}},"outputId":"eefc3f3f-c280-4d38-a9c2-9f956979f075"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1000 100 100\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1672722762828,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"},"user_tz":300},"id":"iZicL_Jc2DaK","outputId":"cfc7c226-6336-48b9-f6e3-0578eaabfd74"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.48"]},"metadata":{},"execution_count":13}],"source":["G = train_environments.environments[0].graph_NX\n","np.mean([b for (a, b) in list(G.degree)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672722762829,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"},"user_tz":300},"id":"AQ6_kMYU2IQr","outputId":"a5ec10cd-ade6-47a9-eb4b-90d974a65434"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.68"]},"metadata":{},"execution_count":14}],"source":["G = val_environments.environments[0].graph_NX\n","np.mean([b for (a, b) in list(G.degree)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1672722762830,"user":{"displayName":"Shubhankar Patankar","userId":"11255472835598982192"},"user_tz":300},"id":"84L-MnVdx5mL","outputId":"39898ce4-7fb4-4206-d727-667d1ee515a3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.48"]},"metadata":{},"execution_count":15}],"source":["G = test_environments.environments[0].graph_NX\n","np.mean([b for (a, b) in list(G.degree)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8EL4Rn94ydC9","outputId":"570ac2de-47bd-478d-8564-3bd19b14a19e","cellView":"form"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Random\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [3:24:21<00:00, 122.61s/Episode]\n","100%|██████████| 100/100 [20:03<00:00, 12.03s/Episode]\n","100%|██████████| 100/100 [19:32<00:00, 11.73s/Episode]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Max Degree\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [2:29:25<00:00, 89.65s/Episode]\n","100%|██████████| 100/100 [07:47<00:00,  4.68s/Episode]\n","100%|██████████| 100/100 [07:38<00:00,  4.58s/Episode]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Min Degree\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [2:19:37<00:00, 83.78s/Episode]\n","100%|██████████| 100/100 [06:40<00:00,  4.00s/Episode]\n","100%|██████████| 100/100 [06:38<00:00,  3.98s/Episode]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Greedy\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▉       | 29/100 [7:45:27<19:10:34, 972.31s/Episode]"]}],"source":["#@title Run Simulations: Baseline Agents\n","\n","baseline_agents = {}\n","random_agent = {}\n","max_degree_agent = {}\n","min_degree_agent = {}\n","greedy_agent = {}\n","\n","print('Random')\n","agent = RandomAgent()\n","all_feature_values_train = simulate(agent, train_environments, num_episodes)\n","all_feature_values_val = simulate(agent, val_environments, num_episodes)\n","all_feature_values_test = simulate(agent, test_environments, num_episodes)\n","random_agent['train'] = all_feature_values_train\n","random_agent['val'] = all_feature_values_val\n","random_agent['test'] = all_feature_values_test\n","baseline_agents['random'] = random_agent\n","\n","print('Max Degree')\n","agent = HighestDegreeAgent()\n","all_feature_values_train = simulate(agent, train_environments, num_episodes)\n","all_feature_values_val = simulate(agent, val_environments, num_episodes)\n","all_feature_values_test = simulate(agent, test_environments, num_episodes)\n","max_degree_agent['train'] = all_feature_values_train\n","max_degree_agent['val'] = all_feature_values_val\n","max_degree_agent['test'] = all_feature_values_test\n","baseline_agents['max_degree'] = max_degree_agent\n","\n","print('Min Degree')\n","agent = LowestDegreeAgent()\n","all_feature_values_train = simulate(agent, train_environments, num_episodes)\n","all_feature_values_val = simulate(agent, val_environments, num_episodes)\n","all_feature_values_test = simulate(agent, test_environments, num_episodes)\n","min_degree_agent['train'] = all_feature_values_train\n","min_degree_agent['val'] = all_feature_values_val\n","min_degree_agent['test'] = all_feature_values_test\n","baseline_agents['min_degree'] = min_degree_agent\n","\n","print('Greedy')\n","agent = GreedyAgent()\n","all_feature_values_train = simulate(agent, train_environments, num_episodes)\n","all_feature_values_val = simulate(agent, val_environments, num_episodes)\n","all_feature_values_test = simulate(agent, test_environments, num_episodes)\n","greedy_agent['train'] = all_feature_values_train\n","greedy_agent['val'] = all_feature_values_val\n","greedy_agent['test'] = all_feature_values_test\n","baseline_agents['greedy'] = greedy_agent"]},{"cell_type":"code","source":["data_save_path = os.path.join(base_path, 'Baselines/')\n","save_filename = data_save_path + network_type + '_' + size + '_' + feature.__name__ + '_baselines.json'"],"metadata":{"id":"q2cn1Omy2zJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write\n","\n","with open(save_filename, 'w') as f:\n","  json.dump(baseline_agents, f)"],"metadata":{"id":"67hye--7l4b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read\n","\n","with open(save_filename, 'r') as f:\n","  baseline_agents_load = json.load(f)"],"metadata":{"id":"sIqWrcCx0hL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_agent = baseline_agents_load['random']['train']\n","max_degree_agent = baseline_agents_load['max_degree']['train']\n","min_degree_agent = baseline_agents_load['min_degree']['train']\n","greedy_agent = baseline_agents_load['greedy']['train']\n","\n","feature_values_mean_random = np.mean(np.mean(np.array(random_agent), axis = 0), axis = 0)\n","feature_values_mean_max_degree = np.mean(np.mean(np.array(max_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_min_degree = np.mean(np.mean(np.array(min_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_greedy = np.mean(np.mean(np.array(greedy_agent), axis = 0), axis = 0)"],"metadata":{"id":"08Cg8ten0usy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tN5WdiayCxE"},"outputs":[],"source":["plt.title('Train Networks')\n","plt.xlabel('Step')\n","plt.ylabel('Feature')\n","plt.plot(feature_values_mean_random, label = 'Random', linestyle = 'dashed')\n","plt.plot(feature_values_mean_max_degree, label = 'Max Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_min_degree, label = 'Min Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_greedy, label = 'Greedy', linestyle = 'dashed')\n","plt.legend()"]},{"cell_type":"code","source":["random_agent = baseline_agents_load['random']['val']\n","max_degree_agent = baseline_agents_load['max_degree']['val']\n","min_degree_agent = baseline_agents_load['min_degree']['val']\n","greedy_agent = baseline_agents_load['greedy']['val']\n","\n","feature_values_mean_random = np.mean(np.mean(np.array(random_agent), axis = 0), axis = 0)\n","feature_values_mean_max_degree = np.mean(np.mean(np.array(max_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_min_degree = np.mean(np.mean(np.array(min_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_greedy = np.mean(np.mean(np.array(greedy_agent), axis = 0), axis = 0)"],"metadata":{"id":"8vx-pxyU3H6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title('Validation Networks')\n","plt.xlabel('Step')\n","plt.ylabel('Feature')\n","plt.plot(feature_values_mean_random, label = 'Random', linestyle = 'dashed')\n","plt.plot(feature_values_mean_max_degree, label = 'Max Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_min_degree, label = 'Min Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_greedy, label = 'Greedy', linestyle = 'dashed')\n","plt.legend()"],"metadata":{"id":"ROsgC7DO3lBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_agent = baseline_agents_load['random']['test']\n","max_degree_agent = baseline_agents_load['max_degree']['test']\n","min_degree_agent = baseline_agents_load['min_degree']['test']\n","greedy_agent = baseline_agents_load['greedy']['test']\n","\n","feature_values_mean_random = np.mean(np.mean(np.array(random_agent), axis = 0), axis = 0)\n","feature_values_mean_max_degree = np.mean(np.mean(np.array(max_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_min_degree = np.mean(np.mean(np.array(min_degree_agent), axis = 0), axis = 0)\n","feature_values_mean_greedy = np.mean(np.mean(np.array(greedy_agent), axis = 0), axis = 0)"],"metadata":{"id":"rGeVI31Y3qKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title('Test Networks')\n","plt.xlabel('Step')\n","plt.ylabel('Feature')\n","plt.plot(feature_values_mean_random, label = 'Random', linestyle = 'dashed')\n","plt.plot(feature_values_mean_max_degree, label = 'Max Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_min_degree, label = 'Min Degree', linestyle = 'dashed')\n","plt.plot(feature_values_mean_greedy, label = 'Greedy', linestyle = 'dashed')\n","plt.legend()"],"metadata":{"id":"eLpfnzhP3rqU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1aV0VzdckckGTDZctVUu6lNNtIOKwqVfB","timestamp":1672697072381},{"file_id":"1GAjGZiNJ_P0v9nDPzfthmSEqhgGRrfXe","timestamp":1672533416401},{"file_id":"1NBk_yQp15dLu31gpaS7CE5dgx5pgZFD8","timestamp":1672460612806}],"authorship_tag":"ABX9TyM3Od08y/Vg9tzyVc/foYwv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}