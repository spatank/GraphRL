{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/GraphRL/blob/main/test_on_larger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVZ0BnjpJXJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/GraphRL/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rXItXVrjp9nj"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import networkx as nx\n",
        "from functools import lru_cache\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from typing import NamedTuple\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "import torch # check version using torch.__version__ before using PyG wheels\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torch.utils import checkpoint # unused\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-geometric\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric import utils, transforms\n",
        "\n",
        "!{sys.executable} -m pip install -q Cython\n",
        "!{sys.executable} -m pip install -q Ripser\n",
        "\n",
        "from ripser import ripser\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pyvCvGEpzLS8"
      },
      "outputs": [],
      "source": [
        "#@title Environments\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def get_NX_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_NX.subgraph(list(frozen_set_of_nodes))\n",
        "\n",
        "@lru_cache(maxsize = 500000)\n",
        "def get_PyG_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_PyG.subgraph(torch.tensor(list(frozen_set_of_nodes)))\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def compute_feature_value(environment, state_subgraph_NX):\n",
        "\n",
        "  return environment.feature_function(state_subgraph_NX)\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def get_neighbors(environment, frozen_set_of_nodes, cutoff = 1):\n",
        "  \"\"\"\n",
        "  Returns the n-th degree neighborhood of a set of nodes, where degree \n",
        "  is specified by the cutoff argument.\n",
        "  \"\"\"\n",
        "\n",
        "  nodes = list(frozen_set_of_nodes)\n",
        "  neighbors = set()\n",
        "\n",
        "  for node in nodes:\n",
        "    neighbors.update(set(nx.single_source_shortest_path_length(environment.graph_NX, \n",
        "                                                               node, \n",
        "                                                               cutoff = cutoff).keys()))\n",
        "    \n",
        "  neighbors = neighbors - set(nodes) # remove input nodes from their own neighborhood\n",
        "\n",
        "  if not neighbors:\n",
        "    neighbors = set(environment.graph_NX.nodes()) - set(environment.visited)\n",
        "\n",
        "  return list(neighbors)\n",
        "\n",
        "class GraphEnvironment():\n",
        "  \n",
        "  def __init__(self, ID, graph_NX, feature):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ID = ID # identifier for the environment\n",
        "\n",
        "    self.graph_NX = graph_NX # environment graph (NetworkX Graph object)  \n",
        "    self.graph_PyG = utils.from_networkx(graph_NX, group_node_attrs = all)\n",
        "    self.num_nodes = self.graph_NX.number_of_nodes()\n",
        "\n",
        "    self.visited = [random.choice(list(self.graph_NX.nodes()))] # list of visited nodes\n",
        "\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "\n",
        "    self.feature_function = feature # function handle to network feature-of-interest\n",
        "    self.feature_values = [self.feature_function(self.state_NX)] # list to store values of the feature-of-interest\n",
        "    \n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Execute an action in the environment, i.e. visit a new node.\n",
        "    \"\"\"\n",
        "\n",
        "    assert action in self.get_actions(self.visited), \"Invalid action!\"\n",
        "    visited_new = deepcopy(self.visited)\n",
        "    visited_new.append(action) # add new node to list of visited nodes\n",
        "    self.visited = visited_new\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    reward = self.compute_reward()\n",
        "    terminal = bool(len(self.visited) == self.graph_NX.number_of_nodes())\n",
        "\n",
        "    return self.get_state_dict(), reward, terminal, self.get_info()\n",
        "\n",
        "  def compute_reward(self):\n",
        "\n",
        "    self.feature_values.append(compute_feature_value(self, self.state_NX))\n",
        "    reward = sum(self.feature_values)/len(self.visited)\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset to initial state.\n",
        "    \"\"\"\n",
        "\n",
        "    self.visited = [random.choice(list(self.graph_NX.nodes()))] # empty the list of visited nodes\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    self.feature_values = [compute_feature_value(self, self.state_NX)]\n",
        "    terminal = False\n",
        "\n",
        "    return self.get_state_dict(), terminal, self.get_info()\n",
        "\n",
        "  def get_state_dict(self):\n",
        "\n",
        "    return {'visited': self.visited, \n",
        "            'state_NX': self.state_NX, \n",
        "            'state_PyG': self.state_PyG}\n",
        "      \n",
        "  def get_info(self):\n",
        "    \n",
        "    return {'environment_ID': self.ID, # useful for DQN training\n",
        "            'feature_value': compute_feature_value(self, self.state_NX)}\n",
        "  \n",
        "  def get_actions(self, nodes):\n",
        "    \"\"\" \n",
        "    Returns available actions given a list of nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    return get_neighbors(self, frozenset(nodes))\n",
        "  \n",
        "  def render(self):\n",
        "    \"\"\"\n",
        "    Render current state to the screen.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    nx.draw(self.state_NX, with_labels = True)\n",
        "\n",
        "class MultipleEnvironments():\n",
        "\n",
        "  def __init__(self, environments):\n",
        "    \n",
        "    self.environments = environments\n",
        "    self.num_environments = len(self.environments)\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    state_dicts = []\n",
        "    terminals = []\n",
        "    all_info = []\n",
        "\n",
        "    for environment in self.environments:\n",
        "      state_dict, terminal, info = environment.reset()\n",
        "      state_dicts.append(state_dict)\n",
        "      terminals.append(terminal)\n",
        "      all_info.append(info)\n",
        "\n",
        "    return state_dicts, terminals, all_info\n",
        "  \n",
        "  def step(self, actions):\n",
        "\n",
        "    state_dicts = []\n",
        "    rewards = []\n",
        "    terminals = []\n",
        "    all_info = []\n",
        "\n",
        "    for idx, environment in enumerate(self.environments):\n",
        "      state_dict, reward, terminal, info = environment.step(actions[idx])\n",
        "      state_dicts.append(state_dict)\n",
        "      rewards.append(reward)\n",
        "      terminals.append(terminal)\n",
        "      all_info.append(info)\n",
        "\n",
        "    return state_dicts, rewards, terminals, all_info\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.num_environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "koDnpGaHzOPv"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Agents\n",
        "\n",
        "class RandomAgent():\n",
        "  \"\"\"\n",
        "  RandomAgent() chooses an action at random. The agent is not deterministic.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.environments = None # should be instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    if not self.environments:\n",
        "      assert False, \"Supply environment(s) for the agent to interact with.\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      action = random.choice(available_actions)\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "class HighestDegreeAgent():\n",
        "  \"\"\"\n",
        "  HighestDegreeAgent() chooses the action with the highest node degree. The \n",
        "  agent is deterministic.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = None # should be instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    if not self.environments:\n",
        "      assert False, \"Supply environment(s) for the agent to interact with.\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      all_degrees = list(zip(*(environment.graph_NX.degree(available_actions))))[1]\n",
        "      action_idx = all_degrees.index(max(all_degrees)) # first largest when ties\n",
        "      action = available_actions[action_idx]\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "class LowestDegreeAgent():\n",
        "  \"\"\"\n",
        "  LowestDegreeAgent() chooses the action with the lowest node degree. The \n",
        "  agent is deterministic.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = None # should be instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    if not self.environments:\n",
        "      assert False, \"Supply environment(s) for the agent to interact with.\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      all_degrees = list(zip(*(environment.graph_NX.degree(available_actions))))[1]\n",
        "      action_idx = all_degrees.index(min(all_degrees)) # first smallest when ties\n",
        "      action = available_actions[action_idx]\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "class GreedyAgent():\n",
        "  \"\"\"\n",
        "  GreedyAgent() chooses the action that would result in the greatest reward.\n",
        "  The agent uses a copy of the environment to simulate each available action and \n",
        "  returns the best performing action. The agent is deterministic.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = None # should be instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    if not self.environments:\n",
        "      assert False, \"Supply environment(s) for the agent to interact with.\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      best_reward = float('-inf')\n",
        "      best_action = None\n",
        "\n",
        "      for action in available_actions:\n",
        "        environment_copy = deepcopy(environment)\n",
        "        state_dict, reward, terminal, info = environment_copy.step(action)\n",
        "\n",
        "        if reward > best_reward:\n",
        "          best_reward = reward\n",
        "          best_action = action\n",
        "\n",
        "      actions.append(best_action)\n",
        "\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PsZuP6S0Vo68"
      },
      "outputs": [],
      "source": [
        "#@title DQN Agent\n",
        "\n",
        "class GNN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = SAGEConv(\n",
        "        hyperparameters['num_node_features'],\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        aggr = 'mean')\n",
        "    self.conv2 = SAGEConv(\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        hyperparameters['embedding_dimensions'],\n",
        "        aggr = 'mean')\n",
        "\n",
        "  def forward(self, x, edge_index, batch = None):\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.relu(x) # node embeddings\n",
        "    x = torch_geometric.nn.global_add_pool(x, batch = batch) # graph embedding\n",
        "\n",
        "    return x\n",
        "\n",
        "class QN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(hyperparameters['embedding_dimensions'], \n",
        "                         hyperparameters['QN_latent_dimensions'])\n",
        "    self.fc2 = nn.Linear(hyperparameters['QN_latent_dimensions'], 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DQNAgent():\n",
        "\n",
        "  def __init__(self, embedding_module, q_net, \n",
        "               replay_buffer, train_start, batch_size, learn_every,\n",
        "               optimizer, \n",
        "               epsilon, epsilon_decay_rate, epsilon_min):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = None # should be instance of MultipleEnvironments() class\n",
        "    self.is_trainable = True # useful to manage control flow during simulations\n",
        "    \n",
        "    self.embedding_module = embedding_module\n",
        "    self.q_net = q_net\n",
        "    \n",
        "    self.target_embedding_module = deepcopy(embedding_module)\n",
        "    self.target_q_net = deepcopy(q_net)\n",
        "    \n",
        "    # disable gradients for target networks\n",
        "    for parameter in self.target_embedding_module.parameters():\n",
        "      parameter.requires_grad = False\n",
        "\n",
        "    for parameter in self.target_q_net.parameters():\n",
        "      parameter.requires_grad = False\n",
        "    \n",
        "    self.replay_buffer = replay_buffer\n",
        "    self.train_start = train_start # specify burn-in period\n",
        "    self.batch_size = batch_size\n",
        "    self.learn_every = learn_every # steps between updates to target nets\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "    self.epsilon_decay_rate = epsilon_decay_rate\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "    self.step = 0\n",
        "\n",
        "  def choose_action(self):\n",
        "    \"\"\"\n",
        "    Choose an action to perform for each environment in self.environments.\n",
        "    \"\"\"\n",
        "\n",
        "    if not self.environments:\n",
        "      assert False, \"Supply environment(s) for the agent to interact with.\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      new_subgraphs = [] # list to store all possible next states\n",
        "\n",
        "      for action in available_actions:\n",
        "        visited_nodes_new = deepcopy(environment.visited)\n",
        "        visited_nodes_new.append(action)\n",
        "        new_subgraph = get_PyG_subgraph(environment, frozenset(visited_nodes_new))\n",
        "        new_subgraphs.append(new_subgraph)\n",
        "\n",
        "      # create a batch to allow for a single forward pass\n",
        "      batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "      # gradients for the target networks are disabled\n",
        "      with torch.no_grad(): # technically redundant\n",
        "        q_values = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                  batch.edge_index, \n",
        "                                                                  batch.batch))\n",
        "      if torch.rand(1) < self.epsilon: # explore\n",
        "        action = np.random.choice(available_actions)\n",
        "      else: # exploit\n",
        "        action_idx = torch.argmax(q_values).item()\n",
        "        action = available_actions[action_idx]\n",
        "\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "  def train(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n",
        "\n",
        "    self.replay_buffer.add(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n",
        "    self.step += 1\n",
        "    \n",
        "    if self.step < self.train_start: # inside the burn-in period\n",
        "      return \n",
        "\n",
        "    # (1) Get lists of experiences from memory\n",
        "    states, actions, next_states, rewards, discounts, all_info = self.replay_buffer.sample(self.batch_size)\n",
        "    \n",
        "    # (2) Build state + action = new subgraph (technically identical to next state)\n",
        "    new_subgraphs = []\n",
        "    for idx, state_dict in enumerate(states):\n",
        "      visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "      visited_nodes_new.append(actions[idx])\n",
        "      assert visited_nodes_new == next_states[idx]['visited'], \"train() assertion failed.\"\n",
        "      new_subgraph = get_PyG_subgraph(self.environments.environments[all_info[idx]['environment_ID']], \n",
        "                                      frozenset(visited_nodes_new))\n",
        "      new_subgraphs.append(new_subgraph)\n",
        "\n",
        "    batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "    # (3) Pass batch of next_state subgraphs through ANN to get predicted q-values\n",
        "    q_predictions = self.q_net(self.embedding_module(batch.x, \n",
        "                                                     batch.edge_index, \n",
        "                                                     batch.batch))\n",
        "\n",
        "    # (4) Compute target q-values for batch\n",
        "    q_targets = []\n",
        "    for idx, next_state_dict in enumerate(next_states):\n",
        "      available_actions = self.environments.environments[all_info[idx]['environment_ID']].get_actions(next_state_dict['visited'])\n",
        "\n",
        "      if available_actions: # terminal states have no available actions\n",
        "        new_subgraphs = [] # each available action results in a new state\n",
        "\n",
        "        for action in available_actions:\n",
        "          visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "          visited_nodes_new.append(action)\n",
        "          new_subgraph = get_PyG_subgraph(self.environments.environments[all_info[idx]['environment_ID']], \n",
        "                                          frozenset(visited_nodes_new))\n",
        "          new_subgraphs.append(new_subgraph)\n",
        "\n",
        "        batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "        with torch.no_grad(): # technically, no_grad() is unnecessary\n",
        "          q_target = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                    batch.edge_index, \n",
        "                                                                    batch.batch))\n",
        "          q_target = q_target.max().view(-1, 1) # get the largest next q-value\n",
        "          q_target = rewards[idx] + discounts[idx] * q_target\n",
        "          q_targets.append(q_target)\n",
        "\n",
        "      else:\n",
        "        q_targets.append(rewards[idx])\n",
        "\n",
        "    q_targets = torch.Tensor(q_targets).view(-1, 1)\n",
        "      \n",
        "    # (5) Compute MSE loss between predicted and target q-values\n",
        "    loss = F.mse_loss(q_predictions, q_targets).mean()\n",
        "\n",
        "    # (6) Backpropagate gradients\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # (7) Copy parameters from source to target networks\n",
        "    if self.step % self.learn_every == 0: \n",
        "      copy_parameters_from_to(self.embedding_module, self.target_embedding_module)\n",
        "      copy_parameters_from_to(self.q_net, self.target_q_net)\n",
        "      \n",
        "    # (8) Decrease exploration rate\n",
        "    self.epsilon *= self.epsilon_decay_rate\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "    return loss.item() # if needed for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TsIkpourYxFy"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions: Miscellaneous\n",
        "\n",
        "def initialize_weights(m):\n",
        "  \"\"\"\n",
        "  Xavier initialization of model weights.\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "    m.weight.data.fill_(1.0)\n",
        "    m.bias.data.zero_()\n",
        "\n",
        "  elif isinstance(m, SAGEConv):\n",
        "    m.lin_l.weight.data = nn.init.xavier_uniform_(\n",
        "        m.lin_l.weight.data, gain = nn.init.calculate_gain('relu'))\n",
        "    \n",
        "    if m.lin_l.bias is not None:\n",
        "      m.lin_l.bias.data.zero_()\n",
        "\n",
        "    m.lin_r.weight.data = nn.init.xavier_uniform_(\n",
        "        m.lin_r.weight.data, gain = nn.init.calculate_gain('relu'))\n",
        "    \n",
        "    if m.lin_r.bias is not None: # redundant\n",
        "      m.lin_r.bias.data.zero_()\n",
        "\n",
        "  elif isinstance(m, nn.Linear):\n",
        "    m.weight.data = nn.init.xavier_uniform_(\n",
        "        m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
        "    \n",
        "    if m.bias is not None:\n",
        "      m.bias.data.zero_()\n",
        "\n",
        "def compute_Frobenius_norm(network):\n",
        "    \"\"\"\n",
        "    Compute the Frobenius norm of all network tensors.\n",
        "    \"\"\"\n",
        "    norm = 0.0\n",
        "\n",
        "    for name, param in network.named_parameters():\n",
        "        norm += torch.norm(param).data  \n",
        "               \n",
        "    return norm.item()\n",
        "\n",
        "def copy_parameters_from_to(source_network, target_network):\n",
        "  \"\"\"\n",
        "  Update the parameters of the target network by copying values from the source\n",
        "  network.\n",
        "  \"\"\"\n",
        "\n",
        "  for source, target in zip(source_network.parameters(), target_network.parameters()):\n",
        "    target.data.copy_(source.data)\n",
        "\n",
        "  return\n",
        "\n",
        "def average_area_under_the_curve(all_feature_values):\n",
        "  \"\"\"\n",
        "  Returns the average area under the curve given a list of list of feature \n",
        "  values. Each list inside all_feature_values corresponds to an environment. \n",
        "  Each list inside that list corresponds to an episode. Each element of the \n",
        "  inner list is a feature value at a given step during an episode.\n",
        "  \"\"\"\n",
        "\n",
        "  all_areas = []\n",
        "  for env_results in all_feature_values:\n",
        "    areas = [sum(feature_values) for feature_values in env_results]\n",
        "    all_areas.append(sum(areas)/len(areas))\n",
        "  \n",
        "  return sum(all_areas)/len(all_areas)\n",
        "\n",
        "def generate_video(plotting_dict):\n",
        "\n",
        "  feature_values_random = plotting_dict['random']\n",
        "  feature_values_degree = plotting_dict['degree']\n",
        "  feature_values_greedy = plotting_dict['greedy']\n",
        "  feature_values_DQN = np.array(plotting_dict['DQN'])\n",
        "\n",
        "  xlim = feature_values_DQN.shape[1]\n",
        "  x = np.arange(xlim) # number of nodes\n",
        "\n",
        "  ylim = max(max(feature_values_random), \n",
        "             max(feature_values_degree), \n",
        "             max(feature_values_greedy), \n",
        "             np.max(feature_values_DQN))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.axis([0, xlim, 0, ylim + 0.01 * ylim])\n",
        "\n",
        "  line1, = ax.plot(x, feature_values_random, label = 'random', color = 'blue')\n",
        "  line2, = ax.plot(x, feature_values_degree, label = 'max degree', color = 'orange')\n",
        "  line3, = ax.plot(x, feature_values_greedy, label = 'greedy', color = 'green')\n",
        "  line4, = ax.plot([], [], label = 'DQN', color = 'black')\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.xlabel('Step')\n",
        "  plt.ylabel('Value')\n",
        "\n",
        "  def animate(i):\n",
        "    line4.set_data(x, feature_values_DQN[i])\n",
        "    \n",
        "  anim_handle = animation.FuncAnimation(fig, animate, \n",
        "                                        frames = len(feature_values_DQN),\n",
        "                                        interval = 100,  \n",
        "                                        blit = False, repeat = False, \n",
        "                                        repeat_delay = 10000)\n",
        "  plt.close() # do not show extra figure\n",
        "\n",
        "  return anim_handle\n",
        "\n",
        "def node_featurizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  attributes = {}\n",
        "\n",
        "  for node in graph_NX.nodes():\n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 1).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "\n",
        "    if neighborhood:\n",
        "      neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    else: # no neighbors\n",
        "      neighborhood_degrees = [0]\n",
        "\n",
        "    node_attributes = {}\n",
        "    node_attributes['degree_1'] = graph_NX.degree(node)\n",
        "    node_attributes['min_degree_1'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_1'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_1'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_1'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 2).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "\n",
        "    if neighborhood:\n",
        "      neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    else: # no neighbors\n",
        "      neighborhood_degrees = [0]\n",
        "\n",
        "    node_attributes['min_degree_2'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_2'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_2'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_2'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    attributes[node] = node_attributes\n",
        "    \n",
        "  nx.set_node_attributes(graph_NX, attributes)\n",
        "\n",
        "  return graph_NX\n",
        "\n",
        "def node_defeaturizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  for (n, d) in graph_NX.nodes(data = True):\n",
        "\n",
        "    del d[\"degree_1\"]\n",
        "    del d[\"min_degree_1\"]\n",
        "    del d[\"max_degree_1\"]\n",
        "    del d[\"mean_degree_1\"]\n",
        "    del d[\"std_degree_1\"]\n",
        "    del d[\"min_degree_2\"]\n",
        "    del d[\"max_degree_2\"]\n",
        "    del d[\"mean_degree_2\"]\n",
        "    del d[\"std_degree_2\"]\n",
        "\n",
        "    return graph_NX\n",
        "\n",
        "class ReplayBuffer():\n",
        "  \n",
        "  def __init__(self, buffer_size):\n",
        "\n",
        "    self.buffer_size = buffer_size\n",
        "    self.ptr = 0 # index to latest experience in memory\n",
        "    self.num_experiences = 0 # number of experiences stored in memory\n",
        "    self.states = [None] * self.buffer_size\n",
        "    self.actions = [None] * self.buffer_size\n",
        "    self.next_states = [None] * self.buffer_size\n",
        "    self.rewards = [None] * self.buffer_size\n",
        "    self.discounts = [None] * self.buffer_size\n",
        "    self.all_info = [None] * self.buffer_size\n",
        "\n",
        "  def add(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n",
        "\n",
        "    # check if arguments are lists\n",
        "    if not isinstance(state_dicts, list): # i.e. a single experience\n",
        "      state_dicts = [state_dicts]\n",
        "      actions = [actions]\n",
        "      next_state_dicts = [next_state_dicts]\n",
        "      rewards = [rewards]\n",
        "      discounts = [discounts]\n",
        "      all_info = [all_info]\n",
        "\n",
        "    for i in range(len(state_dicts)):\n",
        "      self.states[self.ptr] = state_dicts[i]\n",
        "      self.actions[self.ptr] = actions[i]\n",
        "      self.next_states[self.ptr] = next_state_dicts[i]\n",
        "      self.rewards[self.ptr] = rewards[i]\n",
        "      self.discounts[self.ptr] = discounts[i]\n",
        "      self.all_info[self.ptr] = all_info[i]\n",
        "      \n",
        "      if self.num_experiences < self.buffer_size:\n",
        "        self.num_experiences += 1\n",
        "\n",
        "      self.ptr = (self.ptr + 1) % self.buffer_size \n",
        "      # if (ptr + 1) exceeds buffer size then begin overwriting older experiences\n",
        "\n",
        "  def sample(self, batch_size):      \n",
        "\n",
        "    indices = np.random.choice(self.num_experiences, batch_size)   \n",
        "    states = [self.states[index] for index in indices] \n",
        "    actions = [self.actions[index] for index in indices] \n",
        "    next_states = [self.next_states[index] for index in indices] \n",
        "    rewards = [self.rewards[index] for index in indices] \n",
        "    discounts = [self.discounts[index] for index in indices] \n",
        "    all_info = [self.all_info[index] for index in indices] \n",
        "    \n",
        "    return states, actions, next_states, rewards, discounts, all_info\n",
        "\n",
        "def save_checkpoint(embedding_module, q_net, \n",
        "                    optimizer, \n",
        "                    replay_buffer, \n",
        "                    returns, feature_values_train,\n",
        "                    validation_scores, feature_values_val,\n",
        "                    step,\n",
        "                    save_path):\n",
        "  \n",
        "  save_dict = {'embedding_module_state_dict': embedding_module.state_dict(),\n",
        "               'q_net_state_dict': q_net.state_dict(),\n",
        "               'optimizer_state_dict': optimizer.state_dict(),\n",
        "               'buffer_ptr': replay_buffer.ptr,\n",
        "               'buffer_num_experience': replay_buffer.num_experiences,\n",
        "               'buffer_states': replay_buffer.states,\n",
        "               'buffer_actions': replay_buffer.actions,\n",
        "               'buffer_next_states': replay_buffer.next_states,\n",
        "               'buffer_rewards': replay_buffer.rewards,\n",
        "               'buffer_discounts': replay_buffer.discounts,\n",
        "               'buffer_all_info': replay_buffer.all_info,\n",
        "               'returns': returns,\n",
        "               'feature_values_train': feature_values_train,\n",
        "               'validation_scores': validation_scores,\n",
        "               'feature_values_val': feature_values_val,\n",
        "               'step': step}\n",
        "\n",
        "  torch.save(save_dict, save_path)\n",
        "\n",
        "def load_checkpoint(load_path, embedding_module, q_net, \n",
        "                    optimizer = None, \n",
        "                    replay_buffer = None):\n",
        "  \n",
        "  checkpoint = torch.load(load_path)\n",
        "\n",
        "  embedding_module.load_state_dict(checkpoint['embedding_module_state_dict'])\n",
        "  q_net.load_state_dict(checkpoint['q_net_state_dict'])\n",
        "\n",
        "  if optimizer:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  if replay_buffer:\n",
        "    replay_buffer.ptr = checkpoint['buffer_ptr']\n",
        "    replay_buffer.num_experiences = checkpoint['buffer_num_experience']\n",
        "    replay_buffer.states = checkpoint['buffer_states']\n",
        "    replay_buffer.actions = checkpoint['buffer_actions']\n",
        "    replay_buffer.next_states = checkpoint['buffer_next_states']\n",
        "    replay_buffer.rewards = checkpoint['buffer_rewards']\n",
        "    replay_buffer.discounts = checkpoint['buffer_discounts']\n",
        "    replay_buffer.all_info = checkpoint['buffer_all_info']\n",
        "\n",
        "  returns = checkpoint['returns']\n",
        "  feature_values_train = checkpoint['feature_values_train']\n",
        "  validation_scores = checkpoint['validation_scores']\n",
        "  feature_values_val = checkpoint['feature_values_val']\n",
        "\n",
        "  train_results = {'returns': returns,\n",
        "                   'feature_values_train': feature_values_train}\n",
        "\n",
        "  val_results = {'validation_scores': validation_scores,\n",
        "                 'feature_values_val': feature_values_val}\n",
        "\n",
        "  return train_results, val_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDU0sMoB2vfd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions: Simulation\n",
        "\n",
        "def simulate(agent, environments, num_episodes = 100, verbose = True):\n",
        "  \"\"\"\n",
        "  Simulate agent in multiple environments for a specified number of episodes.\n",
        "  We do not use methods from the MultipleEnvironment() class because each \n",
        "  environment may have a different number of nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  agent = deepcopy(agent) # do not alter the original agent's environments\n",
        "  agent.environments = environments # supply the agent with different environments\n",
        "\n",
        "  all_feature_values = []\n",
        "\n",
        "  for idx, environment in enumerate(tqdm(environments.environments, \n",
        "                                         disable = not verbose)):\n",
        "    \n",
        "    state_dict, terminal, info = environment.reset()\n",
        "    environment_feature_values = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "      episode_rewards = []\n",
        "      episode_feature_values = []\n",
        "      \n",
        "      while not terminal:\n",
        "        actions = agent.choose_action() \n",
        "        action = actions[idx] # agent chooses an action for each environment\n",
        "        state_dict, reward, terminal, info = environment.step(action)\n",
        "        episode_feature_values.append(info['feature_value'])\n",
        "      \n",
        "      state_dict, terminal, info = environment.reset() # reset environment after use\n",
        "\n",
        "      environment_feature_values.append(episode_feature_values)\n",
        "      \n",
        "    all_feature_values.append(environment_feature_values)\n",
        "  \n",
        "  environments.reset() # redundant\n",
        "\n",
        "  return all_feature_values\n",
        "\n",
        "def learn_environments(agent, train_environments, val_environments, \n",
        "                       num_steps, discount_factor, base_save_path,\n",
        "                       log_val_results = True, verbose = True):\n",
        "  \"\"\"\n",
        "  Train agent on multiple environments by simulating agent-environment \n",
        "  interactions for a specified number of steps.\n",
        "  \"\"\"\n",
        "\n",
        "  agent.environments = train_environments # supply the agent with environments\n",
        "\n",
        "  # training logs\n",
        "  all_episode_returns_train = [[] for i in range(train_environments.num_environments)]\n",
        "  all_episode_feature_values_train = [[] for i in range(train_environments.num_environments)]\n",
        "  episode_returns_train = [0] * train_environments.num_environments\n",
        "  episode_feature_values_train = [[] for i in range(train_environments.num_environments)]\n",
        "\n",
        "  # validation logs\n",
        "  all_episode_feature_values_val = []\n",
        "  if not val_environments: \n",
        "    log_val_results = False\n",
        "  val_scores = []\n",
        "  val_score = -float('inf')\n",
        "\n",
        "  state_dicts, terminals, all_info = train_environments.reset()\n",
        "\n",
        "  pbar = tqdm(range(num_steps), unit = 'Step', disable = not verbose)\n",
        "\n",
        "  for step in pbar:\n",
        "    actions = agent.choose_action() # choose an action for each environment\n",
        "    next_state_dicts, rewards, terminals, all_info = train_environments.step(actions)\n",
        "    episode_returns_train = [sum(x) for x in zip(rewards, episode_returns_train)]\n",
        "\n",
        "    for idx, info in enumerate(all_info):\n",
        "      episode_feature_values_train[idx].append(info['feature_value'])\n",
        "\n",
        "    if agent.is_trainable:\n",
        "      discounts = [discount_factor * (1 - terminal) for terminal in terminals]\n",
        "      loss = agent.train(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n",
        "      \n",
        "      if log_val_results and step % 2000 == 0 or step == num_steps:\n",
        "        all_feature_values_val = simulate(agent, val_environments,\n",
        "                                          num_episodes = 10, verbose = False)\n",
        "        val_score = average_area_under_the_curve(all_feature_values_val)\n",
        "        val_scores.append(val_score)\n",
        "        all_episode_feature_values_val.append(all_feature_values_val)\n",
        "\n",
        "      if loss: \n",
        "        pbar.set_description('Loss: %0.5f, Val. Score: %0.5f' % (loss, val_score))\n",
        "      else: # no loss value is returned inside the burn-in period\n",
        "        pbar.set_description('Loss: %0.5f, Val. Score: %0.5f' % (float('inf'), val_score))\n",
        "\n",
        "    state_dicts = next_state_dicts\n",
        "\n",
        "    for idx, terminal in enumerate(terminals):\n",
        "      # if terminal then gather episode results for this environment and reset\n",
        "      if terminal: \n",
        "        all_episode_returns_train[idx].append(episode_returns_train[idx])\n",
        "        episode_returns_train[idx] = 0\n",
        "        all_episode_feature_values_train[idx].append(episode_feature_values_train[idx])\n",
        "        episode_feature_values_train[idx] = []\n",
        "        state_dict, terminal, info = train_environments.environments[idx].reset()\n",
        "        state_dicts[idx] = state_dict\n",
        "\n",
        "    if step % 2500 == 0 or step == num_steps - 1: # save model every 2500 steps\n",
        "      checkpoint_name = 'checkpoint_' + str(step) + '.pt'\n",
        "      save_path = os.path.join(base_save_path, checkpoint_name) \n",
        "      save_checkpoint(agent.embedding_module, agent.q_net, \n",
        "                      agent.optimizer,\n",
        "                      agent.replay_buffer,\n",
        "                      all_episode_returns_train, all_episode_feature_values_train,\n",
        "                      val_scores, all_episode_feature_values_val, \n",
        "                      step, \n",
        "                      save_path)\n",
        "\n",
        "  train_environments.reset()\n",
        "\n",
        "  train_results = {'returns': all_episode_returns_train,\n",
        "                   'feature_values_train': all_episode_feature_values_train}\n",
        "\n",
        "  val_results = {'validation_scores': val_scores,\n",
        "                 'feature_values_val': all_episode_feature_values_val}\n",
        "\n",
        "  return train_results, val_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "igNip2xJstvs"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions: Rewards\n",
        "\n",
        "def make_filtration_matrix(G):\n",
        "    \"\"\"\n",
        "    Takes in adjacency matrix and returns a filtration matrix for Ripser\n",
        "    \"\"\"\n",
        "\n",
        "    N = G.shape[0]\n",
        "    weighted_G = np.ones([N, N])\n",
        "    for col in range(N):\n",
        "        weighted_G[:col, col] = weighted_G[:col, col] * col\n",
        "        weighted_G[col, :col] = weighted_G[col, :col] * col\n",
        "    weighted_G += 1 # pushes second node's identifier to 2\n",
        "    # removes diagonals, simultaneously resetting first node's identifier to 0\n",
        "    weighted_G = np.multiply(G, weighted_G) \n",
        "    # place 1 to N along the diagonal\n",
        "    np.fill_diagonal(weighted_G, list(range(1, N + 1)))\n",
        "    # set all zeros to be non-edges (i.e. at inf distance)\n",
        "    weighted_G[weighted_G == 0] = np.inf\n",
        "    # remove 1 from everywhere to ensure first node has identifier 0\n",
        "    weighted_G -= 1\n",
        "    \n",
        "    return weighted_G\n",
        "\n",
        "def betti_numbers(G, maxdim = 2, dim = 1):\n",
        "  \"\"\"\n",
        "  Given a NetworkX graph object, computes number of topological cycles \n",
        "  (i.e. Betti numbers) of various dimensions upto maxdim.\n",
        "  \"\"\"\n",
        "  adj = nx.to_numpy_array(G)\n",
        "  adj[adj == 0] = np.inf # set unconnected nodes to be infinitely apart\n",
        "  np.fill_diagonal(adj, 1) # set diagonal to 1 to indicate all nodes are born at once\n",
        "  bars = ripser(adj, distance_matrix = True, maxdim = maxdim)['dgms'] # returns barcodes\n",
        "  bars_list = list(zip(range(maxdim + 1), bars))\n",
        "  bettis_dict = dict([(dim, len(cycles)) for (dim, cycles) in bars_list])\n",
        "\n",
        "  return bettis_dict[dim] # return Betti number for dimension of interest\n",
        "\n",
        "def get_barcode(filt_mat, maxdim = 2):\n",
        "    \"\"\"\n",
        "    Calculates the persistent homology for a given filtration matrix\n",
        "    ``filt_mat``, default dimensions 0 through 2. Wraps ripser.\n",
        "    \"\"\"\n",
        "\n",
        "    b = ripser(filt_mat, distance_matrix = True, maxdim = maxdim)['dgms']\n",
        "\n",
        "    return list(zip(range(maxdim + 1), b))\n",
        "\n",
        "def betti_curves(bars, length):\n",
        "    \"\"\"\n",
        "    Takes in bars and returns the betti curves\n",
        "    \"\"\"\n",
        "\n",
        "    bettis = np.zeros((len(bars), length))\n",
        "    for i in range(bettis.shape[0]):\n",
        "        bn = bars[i][1]\n",
        "        for bar in bn:\n",
        "            birth = int(bar[0])\n",
        "            death = length+1 if np.isinf(bar[1]) else int(bar[1]+1)\n",
        "            bettis[i][birth:death] += 1\n",
        "\n",
        "    return bettis\n",
        "\n",
        "def plot_bettis(bettis):\n",
        "  \n",
        "  N = bettis.shape[1]\n",
        "  colors = ['xkcd:emerald green', 'xkcd:tealish', 'xkcd:peacock blue']\n",
        "  for i in range(3):\n",
        "    plt.plot(list(range(N)), bettis[i], color = colors[i], \n",
        "             label = '$\\\\beta_{}$'.format(i), \n",
        "             linewidth = 1)\n",
        "  plt.xlabel('Nodes')\n",
        "  plt.ylabel('Number of Cycles')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EajUnBPkD143"
      },
      "outputs": [],
      "source": [
        "#@title Parameters from Darvariu et al. \n",
        "\n",
        "# Data\n",
        "# |G_train| = 10000\n",
        "# |G_val| = 100\n",
        "# |G_test| = 100\n",
        "\n",
        "# Model\n",
        "# 3 message passing rounds\n",
        "# 128 hidden units in MLP\n",
        "# linear exploration decay from 1 to 0.1 for steps/2 and then 0.1  \n",
        "\n",
        "# Training\n",
        "# steps = 40000 (*1 or *2 or *5)\n",
        "# gamma = 1 (finite horizon)\n",
        "# learn_every = 50\n",
        "# weights initialized using Glorot scheme\n",
        "# learning rate = 0.0001\n",
        "# rewards scaled by 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcoqPfdOi3n5"
      },
      "outputs": [],
      "source": [
        "#@title Load Networks + Build Environments\n",
        "\n",
        "base_path = '/content/drive/My Drive/GraphRL/Networks/'\n",
        "\n",
        "network_type = 'Synthetic'\n",
        "# generator_type = 'BA'\n",
        "generator_type = 'ER'\n",
        "\n",
        "# feature = nx.average_clustering\n",
        "feature = betti_numbers\n",
        "\n",
        "# build test environments\n",
        "mode = 'LargerTest'\n",
        "full_path = os.path.join(base_path, network_type, generator_type, mode)\n",
        "all_test_net_paths = glob.glob(full_path + '/*.gml')\n",
        "\n",
        "test_environments = []\n",
        "for idx, net_path in enumerate(all_test_net_paths):\n",
        "  G = nx.read_gml(net_path, destringizer = int)\n",
        "  G = node_featurizer(G)\n",
        "  environment = GraphEnvironment(idx, G, feature)\n",
        "  test_environments.append(environment)\n",
        "\n",
        "test_environments = MultipleEnvironments(test_environments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QAH4jsyQUMB"
      },
      "source": [
        "# Run Simulations: Larger Test Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vq4L_7PvVQg"
      },
      "outputs": [],
      "source": [
        "num_episodes = 10\n",
        "\n",
        "agent = RandomAgent()\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_random = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = HighestDegreeAgent()\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_max_degree = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = LowestDegreeAgent()\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_min_degree = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = GreedyAgent()\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_greedy = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "# load trained DQN from checkpoint\n",
        "checkpoint = 'checkpoint_59999.pt'\n",
        "load_path = os.path.join(base_path, network_type, generator_type, \n",
        "                         'Model', feature.__name__, 'Checkpoints', checkpoint)\n",
        "\n",
        "hyperparameters = {'num_node_features': 9,\n",
        "                   'GNN_latent_dimensions': 64,\n",
        "                   'embedding_dimensions': 64,\n",
        "                   'QN_latent_dimensions': 32,\n",
        "                   'buffer_size': 500000,\n",
        "                   'train_start': 320,\n",
        "                   'batch_size': 32,\n",
        "                   'learn_every': 16,\n",
        "                   'epsilon_initial': 0.1,\n",
        "                   'epsilon_decay_rate': 1,\n",
        "                   'epsilon_min': 0.1,\n",
        "                   'discount_factor': 0.75,\n",
        "                   'learning_rate': 3e-4}\n",
        "\n",
        "embedding_module = GNN(hyperparameters)\n",
        "q_net = QN(hyperparameters)\n",
        "\n",
        "_, _ = load_checkpoint(load_path, embedding_module, q_net)\n",
        "\n",
        "agent = DQNAgent(embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_DQN = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'checkpoint_0.pt'\n",
        "load_path = os.path.join(base_path, network_type, generator_type, \n",
        "                         'Model', feature.__name__, 'Checkpoints', checkpoint)\n",
        "\n",
        "embedding_module_untrained = GNN(hyperparameters)\n",
        "q_net_untrained = QN(hyperparameters)\n",
        "\n",
        "_, _ = load_checkpoint(load_path, embedding_module_untrained, q_net_untrained)\n",
        "\n",
        "agent = DQNAgent(embedding_module_untrained, q_net_untrained, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 1, epsilon_decay_rate = None, epsilon_min = None)\n",
        "all_feature_values = simulate(agent, test_environments, num_episodes)\n",
        "feature_values_mean_DQN_untrained = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "f9wSFBMNHgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Test Performance (Larger Networks)')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'Random', color = 'blue')\n",
        "plt.plot(feature_values_mean_max_degree, label = 'Max Degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_min_degree, label = 'Min Degree', color = 'red')\n",
        "plt.plot(feature_values_mean_greedy, label = 'Greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.plot(feature_values_mean_DQN_untrained, label = 'DQN (untrained)', color = 'black', linestyle = 'dashed')\n",
        "plt.legend()\n",
        "save_path = os.path.join(base_path, \n",
        "                         network_type, generator_type, \n",
        "                         'Model', feature.__name__, 'Figures', 'LargerTest')\n",
        "plt.savefig(os.path.join(save_path, 'larger_test_performance.eps'), format = 'eps')"
      ],
      "metadata": {
        "id": "dAdCZ8QKJKXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv0ybLUvYH_f"
      },
      "outputs": [],
      "source": [
        "plt.title('Test Performance (Larger Networks)')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'Random', color = 'blue')\n",
        "plt.plot(feature_values_mean_max_degree, label = 'Max Degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_min_degree, label = 'Min Degree', color = 'red')\n",
        "plt.plot(feature_values_mean_greedy, label = 'Greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.plot(feature_values_mean_DQN_untrained, label = 'DQN (untrained)', color = 'black', linestyle = 'dashed')\n",
        "plt.legend()\n",
        "save_path = os.path.join(base_path, \n",
        "                         network_type, generator_type, \n",
        "                         'Model', feature.__name__, 'Figures', 'LargerTest')\n",
        "plt.savefig(os.path.join(save_path, 'larger_test_performance.eps'), format = 'eps')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNuHZnP8LDV/luO88quqs3G",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}