{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/GraphRL/blob/main/GraphRL_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVZ0BnjpJXJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/GraphRL/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXItXVrjp9nj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import networkx as nx\n",
        "from functools import lru_cache\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from typing import NamedTuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "import torch # check version using torch.__version__ before using PyG wheels\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-geometric\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric import utils, transforms\n",
        "\n",
        "!{sys.executable} -m pip install -q Cython\n",
        "!{sys.executable} -m pip install -q Ripser\n",
        "\n",
        "from ripser import ripser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyvCvGEpzLS8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Environment\n",
        "\n",
        "@lru_cache(maxsize = 10000)\n",
        "def get_NX_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_NX.subgraph(list(frozen_set_of_nodes))\n",
        "\n",
        "@lru_cache(maxsize = 50000)\n",
        "def get_PyG_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_PyG.subgraph(torch.tensor(list(frozen_set_of_nodes)))\n",
        "\n",
        "@lru_cache(maxsize = 10000)\n",
        "def compute_feature_value(environment, state_subgraph_NX):\n",
        "\n",
        "  return environment.feature_function(state_subgraph_NX)\n",
        "\n",
        "@lru_cache(maxsize = 10000)\n",
        "def get_neighbors(environment, frozen_set_of_nodes, cutoff = 1):\n",
        "  \"\"\"Returns the n-th degree neighborhood of a set of nodes, where degree \n",
        "  is specified by the cutoff argument.\n",
        "  \"\"\"\n",
        "\n",
        "  nodes = list(frozen_set_of_nodes)\n",
        "  neighbors = set()\n",
        "  for node in nodes:\n",
        "    neighbors.update(set(nx.single_source_shortest_path_length(environment.graph_NX, \n",
        "                                                               node, \n",
        "                                                               cutoff = cutoff).keys()))\n",
        "  neighbors = neighbors - set(nodes) # remove input nodes from their own neighborhood\n",
        "\n",
        "  return list(neighbors)\n",
        "\n",
        "class GraphEnvironment():\n",
        "  \n",
        "  def __init__(self, graph_NX, feature, start_node = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.graph_NX = graph_NX # environment graph (NetworkX Graph object)  \n",
        "    self.graph_PyG = utils.from_networkx(graph_NX, group_node_attrs = all)\n",
        "    self.num_nodes = self.graph_NX.number_of_nodes()\n",
        "\n",
        "    self.start_node = start_node\n",
        "    self.visited = [self.start_node] # list of visited nodes\n",
        "\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "\n",
        "    self.feature_function = feature # function handle to network feature-of-interest\n",
        "    self.feature_values = [self.feature_function(self.state_NX)] # list to store values of the feature-of-interest\n",
        "    \n",
        "  def step(self, action):\n",
        "    \"\"\"Execute an action in the environment, i.e. visit a new node.\"\"\"\n",
        "\n",
        "    assert action in self.get_actions(self.visited), \"Invalid action!\"\n",
        "    visited_new = deepcopy(self.visited)\n",
        "    visited_new.append(action) # add new node to list of visited nodes\n",
        "    self.visited = visited_new\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    reward = self.compute_reward()\n",
        "    terminal = bool(len(self.visited) == self.graph_NX.number_of_nodes())\n",
        "\n",
        "    return self.get_state_dict(), reward, terminal, self.get_info()\n",
        "\n",
        "  def compute_reward(self):\n",
        "\n",
        "    self.feature_values.append(compute_feature_value(self, self.state_NX))\n",
        "    reward = sum(self.feature_values)/len(self.visited)\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset to initial state.\"\"\"\n",
        "\n",
        "    self.visited = [self.start_node] # empty the list of visited nodes\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    self.feature_values = [compute_feature_value(self, self.state_NX)]\n",
        "    terminal = False\n",
        "\n",
        "    return self.get_state_dict(), terminal, self.get_info()\n",
        "\n",
        "  def get_state_dict(self):\n",
        "\n",
        "    return {'visited': self.visited, \n",
        "            'state_NX': self.state_NX, \n",
        "            'state_PyG': self.state_PyG}\n",
        "      \n",
        "  def get_info(self):\n",
        "    \n",
        "    return {'feature_value': compute_feature_value(self, self.state_NX)}\n",
        "  \n",
        "  def get_actions(self, nodes):\n",
        "    \"\"\" Returns available actions given a list of nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    return get_neighbors(self, frozenset(nodes))\n",
        "  \n",
        "  def render(self):\n",
        "    \"\"\"Render current state to the screen.\"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    nx.draw(self.state_NX, with_labels = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "koDnpGaHzOPv"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Agents\n",
        "\n",
        "class RandomAgent():\n",
        "  \"\"\"RandomAgent() chooses an action at random. The agent is not deterministic.\"\"\"\n",
        "  \n",
        "  def __init__(self, environment):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.environment = environment\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    available_actions = self.environment.get_actions(self.environment.visited)\n",
        "    action = random.choice(available_actions)\n",
        "\n",
        "    return action\n",
        "\n",
        "class HighestDegreeAgent():\n",
        "  \"\"\"HighestDegreeAgent() chooses the action with the highest node degree. The \n",
        "  agent is deterministic.\"\"\"\n",
        "\n",
        "  def __init__(self, environment):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environment = environment\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    available_actions = self.environment.get_actions(self.environment.visited)\n",
        "    all_degrees = list(zip(*(self.environment.graph_NX.degree(available_actions))))[1]\n",
        "    action_idx = all_degrees.index(max(all_degrees)) # first largest when ties\n",
        "    action = available_actions[action_idx]\n",
        "\n",
        "    return action\n",
        "\n",
        "class GreedyAgent():\n",
        "  \"\"\"GreedyAgent() chooses the action that would result in the greatest reward.\n",
        "  The agent uses a copy of the environment to simulate each available action and \n",
        "  returns the best performing action. The agent is deterministic.\"\"\"\n",
        "\n",
        "  def __init__(self, environment):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environment = environment\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    available_actions = self.environment.get_actions(self.environment.visited)\n",
        "\n",
        "    best_reward = float('-inf')\n",
        "    best_action = None\n",
        "\n",
        "    for action in available_actions:\n",
        "\n",
        "      environment_copy = deepcopy(self.environment)\n",
        "      state_dict, reward, terminal, info = environment_copy.step(action)\n",
        "\n",
        "      if reward > best_reward:\n",
        "        best_reward = reward\n",
        "        best_action = action\n",
        "\n",
        "    return best_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDU0sMoB2vfd"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions\n",
        "\n",
        "def make_filtration_matrix(G):\n",
        "    \"\"\"\n",
        "    Takes in adjacency matrix and returns a filtration matrix for Ripser\n",
        "    \"\"\"\n",
        "\n",
        "    N = G.shape[0]\n",
        "    weighted_G = np.ones([N, N])\n",
        "    for col in range(N):\n",
        "        weighted_G[:col, col] = weighted_G[:col, col] * col\n",
        "        weighted_G[col, :col] = weighted_G[col, :col] * col\n",
        "    weighted_G += 1 # pushes second node's identifier to 2\n",
        "    # removes diagonals, simultaneously resetting first node's identifier to 0\n",
        "    weighted_G = np.multiply(G, weighted_G) \n",
        "    # place 1 to N along the diagonal\n",
        "    np.fill_diagonal(weighted_G, list(range(1, N + 1)))\n",
        "    # set all zeros to be non-edges (i.e. at inf distance)\n",
        "    weighted_G[weighted_G == 0] = np.inf\n",
        "    # remove 1 from everywhere to ensure first node has identifier 0\n",
        "    weighted_G -= 1\n",
        "    \n",
        "    return weighted_G\n",
        "\n",
        "def betti_numbers(G, maxdim = 2, dim = 1):\n",
        "  \"\"\"\n",
        "  Given a NetworkX graph object, computes number of topological cycles \n",
        "  (i.e. Betti numbers) of various dimensions upto maxdim.\n",
        "  \"\"\"\n",
        "  adj = nx.to_numpy_array(G)\n",
        "  adj[adj == 0] = np.inf # set unconnected nodes to be infinitely apart\n",
        "  np.fill_diagonal(adj, 1) # set diagonal to 1 to indicate all nodes are born at once\n",
        "  bars = ripser(adj, distance_matrix = True, maxdim = maxdim)['dgms'] # returns barcodes\n",
        "  bars_list = list(zip(range(maxdim + 1), bars))\n",
        "  bettis_dict = dict([(dim, len(cycles)) for (dim, cycles) in bars_list])\n",
        "\n",
        "  return bettis_dict[dim] # return Betti number for dimension of interest\n",
        "\n",
        "def get_barcode(filt_mat, maxdim = 2):\n",
        "    \"\"\"\n",
        "    Calculates the persistent homology for a given filtration matrix\n",
        "    ``filt_mat``, default dimensions 0 through 2. Wraps ripser.\n",
        "    \"\"\"\n",
        "\n",
        "    b = ripser(filt_mat, distance_matrix = True, maxdim = maxdim)['dgms']\n",
        "\n",
        "    return list(zip(range(maxdim + 1), b))\n",
        "\n",
        "def betti_curves(bars, length):\n",
        "    \"\"\"\n",
        "    Takes in bars and returns the betti curves\n",
        "    \"\"\"\n",
        "\n",
        "    bettis = np.zeros((len(bars), length))\n",
        "    for i in range(bettis.shape[0]):\n",
        "        bn = bars[i][1]\n",
        "        for bar in bn:\n",
        "            birth = int(bar[0])\n",
        "            death = length+1 if np.isinf(bar[1]) else int(bar[1]+1)\n",
        "            bettis[i][birth:death] += 1\n",
        "\n",
        "    return bettis\n",
        "\n",
        "def plot_bettis(bettis):\n",
        "  \n",
        "  N = bettis.shape[1]\n",
        "  colors = ['xkcd:emerald green', 'xkcd:tealish', 'xkcd:peacock blue']\n",
        "  for i in range(3):\n",
        "    plt.plot(list(range(N)), bettis[i], color = colors[i], \n",
        "             label = '$\\\\beta_{}$'.format(i), \n",
        "             linewidth = 1)\n",
        "  plt.xlabel('Nodes')\n",
        "  plt.ylabel('Number of Cycles')\n",
        "  plt.legend()\n",
        "\n",
        "def simulate(environment, agent, num_episodes = 100, verbose = True):\n",
        "  \"\"\"Simulate agent-environment interaction for a specified number of episodes.\"\"\"\n",
        "  \n",
        "  states_NX = []\n",
        "  states_PyG = []\n",
        "  rewards = []\n",
        "  feature_values = []\n",
        "  steps = []\n",
        "\n",
        "  for _ in tqdm(range(num_episodes), disable = not verbose):\n",
        "\n",
        "    state_dict, terminal, info = environment.reset()\n",
        "\n",
        "    episode_states_NX = [state_dict['state_NX']]\n",
        "    episode_states_PyG = [state_dict['state_PyG']]\n",
        "    episode_rewards = []\n",
        "    episode_feature_values = []\n",
        "    \n",
        "    curr_step = 0\n",
        "\n",
        "    while not terminal:\n",
        "\n",
        "      action = agent.choose_action()\n",
        "      state_dict, reward, terminal, info = environment.step(action)\n",
        "\n",
        "      episode_states_NX.append(state_dict['state_NX'])\n",
        "      episode_states_PyG.append(state_dict['state_PyG'])\n",
        "      episode_rewards.append(reward)\n",
        "      episode_feature_values.append(info['feature_value'])\n",
        "      curr_step += 1\n",
        "    \n",
        "    states_NX.append(episode_states_NX)\n",
        "    states_PyG.append(episode_states_PyG)\n",
        "    rewards.append(episode_rewards)\n",
        "    feature_values.append(episode_feature_values)\n",
        "    steps.append(curr_step) # number of steps in the episode\n",
        "\n",
        "  environment.reset()\n",
        "  \n",
        "  return states_NX, states_PyG, rewards, feature_values, steps\n",
        "\n",
        "def learn_environment(environment, agent, num_steps, \n",
        "                      discount_factor = None, verbose = True):\n",
        "  \"\"\"Simulate agent-environment interaction for a number of steps specified \n",
        "  by num_steps and trains the agent if it is capable of being trained. Note that \n",
        "  the interaction ends after a finite number of steps regardless of whether the \n",
        "  ongoing episode has terminated.\"\"\"\n",
        "\n",
        "  all_episode_returns = []\n",
        "  all_episode_feature_values = []\n",
        "\n",
        "  episode_return = 0\n",
        "  episode_feature_values = []\n",
        "  state_dict, terminal, info = environment.reset()\n",
        "\n",
        "  for step in tqdm(range(num_steps), disable = not verbose):\n",
        "\n",
        "    action = agent.choose_action()\n",
        "    next_state_dict, reward, terminal, info = environment.step(action)\n",
        "    episode_return += reward\n",
        "    episode_feature_values.append(info['feature_value'])\n",
        "\n",
        "    if agent.is_trainable:\n",
        "      discount = discount_factor * (1 - terminal)\n",
        "      agent.train(state_dict, action, next_state_dict, reward, discount) \n",
        "\n",
        "    if terminal:\n",
        "      all_episode_returns.append(episode_return)\n",
        "      episode_return = 0\n",
        "      all_episode_feature_values.append(episode_feature_values)\n",
        "      episode_feature_values = []\n",
        "      state_dict, terminal, info = environment.reset()\n",
        "    else:\n",
        "      state_dict = next_state_dict\n",
        "\n",
        "  environment.reset()\n",
        "\n",
        "  return all_episode_returns, all_episode_feature_values\n",
        "\n",
        "def compute_Frobenius_norm(network):\n",
        "    \"\"\"\n",
        "        Output: Frobenius norm of all network tensors\n",
        "    \"\"\"\n",
        "    norm = 0.0\n",
        "\n",
        "    for name, param in network.named_parameters():\n",
        "        norm += torch.norm(param).data  \n",
        "               \n",
        "    return norm.item()\n",
        "\n",
        "def copy_parameters_from_to(source_network, target_network):\n",
        "  \"\"\"\n",
        "  Update the parameters of the target network by copying values from the source\n",
        "  network.\n",
        "  \"\"\"\n",
        "\n",
        "  for source, target in zip(source_network.parameters(), target_network.parameters()):\n",
        "    target.data.copy_(source.data)\n",
        "\n",
        "  return\n",
        "\n",
        "def generate_video(plotting_dict):\n",
        "\n",
        "  feature_values_random = plotting_dict['random']\n",
        "  feature_values_degree = plotting_dict['degree']\n",
        "  feature_values_greedy = plotting_dict['greedy']\n",
        "  feature_values_DQN = np.array(plotting_dict['DQN'])\n",
        "\n",
        "  xlim = feature_values_DQN.shape[1]\n",
        "  x = np.arange(xlim) # number of nodes\n",
        "\n",
        "  ylim = max(max(feature_values_random), \n",
        "             max(feature_values_degree), \n",
        "             max(feature_values_greedy), \n",
        "             np.max(feature_values_DQN))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.axis([0, xlim, 0, ylim + 0.01 * ylim])\n",
        "\n",
        "  line1, = ax.plot(x, feature_values_random, label = 'random', color = 'blue')\n",
        "  line2, = ax.plot(x, feature_values_degree, label = 'max degree', color = 'orange')\n",
        "  line3, = ax.plot(x, feature_values_greedy, label = 'greedy', color = 'green')\n",
        "  line4, = ax.plot([], [], label = 'DQN', color = 'black')\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.xlabel('Step')\n",
        "  plt.ylabel('Value')\n",
        "\n",
        "  def animate(i):\n",
        "    line4.set_data(x, feature_values_DQN[i])\n",
        "    \n",
        "  anim_handle = animation.FuncAnimation(fig, animate, \n",
        "                                        frames = len(feature_values_DQN),\n",
        "                                        interval = 100,  \n",
        "                                        blit = False, repeat = False, \n",
        "                                        repeat_delay = 10000)\n",
        "  plt.close() # do not show extra figure\n",
        "\n",
        "  return anim_handle\n",
        "\n",
        "class ReplayBuffer():\n",
        "  \n",
        "  def __init__(self, buffer_size):\n",
        "\n",
        "    self.buffer_size = buffer_size\n",
        "    self.ptr = 0 # index to latest experience in memory\n",
        "    self.num_experiences = 0 # number of experiences stored in memory\n",
        "    self.states = [None] * self.buffer_size\n",
        "    self.actions = [None] * self.buffer_size\n",
        "    self.next_states = [None] * self.buffer_size\n",
        "    self.rewards = [None] * self.buffer_size\n",
        "    self.discounts = [None] * self.buffer_size\n",
        "\n",
        "  def add(self, state, action, next_state, reward, discount):\n",
        "\n",
        "    self.states[self.ptr] = state\n",
        "    self.actions[self.ptr] = action\n",
        "    self.next_states[self.ptr] = next_state\n",
        "    self.rewards[self.ptr] = reward\n",
        "    self.discounts[self.ptr] = discount\n",
        "    \n",
        "    if self.num_experiences < self.buffer_size:\n",
        "      self.num_experiences += 1\n",
        "\n",
        "    self.ptr = (self.ptr + 1) % self.buffer_size \n",
        "    # if (ptr + 1) exceeds buffer size then overwrite older experience\n",
        "\n",
        "  def sample(self, batch_size):      \n",
        "\n",
        "    indices = np.random.choice(self.num_experiences, batch_size)   \n",
        "\n",
        "    states = [self.states[index] for index in indices] \n",
        "    actions = [self.actions[index] for index in indices] \n",
        "    next_states = [self.next_states[index] for index in indices] \n",
        "    rewards = [self.rewards[index] for index in indices] \n",
        "    discounts = [self.discounts[index] for index in indices] \n",
        "    \n",
        "    return states, actions, next_states, rewards, discounts \n",
        "\n",
        "def node_featurizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  attributes = {}\n",
        "  for node in graph_NX.nodes():\n",
        "    \n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 1).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "    neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    node_attributes = {}\n",
        "    node_attributes['degree_1'] = graph_NX.degree(node)\n",
        "    node_attributes['min_degree_1'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_1'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_1'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_1'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 2).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "    neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    node_attributes['min_degree_2'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_2'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_2'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_2'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    attributes[node] = node_attributes\n",
        "    \n",
        "  nx.set_node_attributes(graph_NX, attributes)\n",
        "\n",
        "  return graph_NX\n",
        "\n",
        "def node_defeaturizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  for (n, d) in graph_NX.nodes(data = True):\n",
        "    del d[\"degree_1\"]\n",
        "    del d[\"min_degree_1\"]\n",
        "    del d[\"max_degree_1\"]\n",
        "    del d[\"mean_degree_1\"]\n",
        "    del d[\"std_degree_1\"]\n",
        "    del d[\"min_degree_2\"]\n",
        "    del d[\"max_degree_2\"]\n",
        "    del d[\"mean_degree_2\"]\n",
        "    del d[\"std_degree_2\"]\n",
        "\n",
        "    return graph_NX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OWw9olqUmyOP"
      },
      "outputs": [],
      "source": [
        "#@title Vanilla DQN Agent\n",
        "\n",
        "class QNetworkAgent_Vanilla():\n",
        "\n",
        "  def __init__(self, environment, \n",
        "               embedding_module, q_net, \n",
        "               optimizer, \n",
        "               epsilon, epsilon_decay_rate, epsilon_min):\n",
        "    super().__init__()\n",
        "\n",
        "    self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "    self.environment = environment\n",
        "    self.embedding_module = embedding_module\n",
        "    self.q_net = q_net\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "\n",
        "    self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "    self.epsilon_decay_rate = epsilon_decay_rate\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    available_actions = self.environment.get_actions(self.environment.visited)\n",
        "\n",
        "    new_subgraph_list = [] # list to store all possible next states\n",
        "    for action in available_actions:\n",
        "      visited_nodes_new = deepcopy(self.environment.visited)\n",
        "      visited_nodes_new.append(action)\n",
        "      new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "      new_subgraph_list.append(new_subgraph)\n",
        "\n",
        "    # create a batch to allow for a single forward pass\n",
        "    new_subgraph_batch = Batch.from_data_list(new_subgraph_list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      q_values = self.q_net(self.embedding_module(new_subgraph_batch.x, \n",
        "                                                  new_subgraph_batch.edge_index, \n",
        "                                                  new_subgraph_batch.batch))\n",
        "      \n",
        "    if torch.rand(1) < self.epsilon: # explore\n",
        "      action = np.random.choice(available_actions)\n",
        "    else: # exploit\n",
        "      action_idx = torch.argmax(q_values).item()\n",
        "      action = available_actions[action_idx]\n",
        "\n",
        "    return action\n",
        "\n",
        "  def train(self, state_dict, action, next_state_dict, reward, discount):\n",
        "    \n",
        "    # (1) Build state + action (= next_state) subgraph \n",
        "    visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "    visited_nodes_new.append(action)\n",
        "    assert visited_nodes_new == next_state_dict['visited'], \"train() assertion failed.\"\n",
        "    new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "    \n",
        "    # (2) Pass next_state subgraph through ANN to get predicted q-value\n",
        "    q_prediction = self.q_net(self.embedding_module(new_subgraph.x, \n",
        "                                                   new_subgraph.edge_index))\n",
        "    \n",
        "    # (3) Compute target q-value\n",
        "    available_actions = self.environment.get_actions(next_state_dict['visited'])\n",
        "    if available_actions: # states that are terminal have no available actions\n",
        "      new_subgraphs = []\n",
        "      for action in available_actions:\n",
        "        visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "        visited_nodes_new.append(action)\n",
        "        new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "        new_subgraphs.append(new_subgraph)\n",
        "\n",
        "      batch = Batch.from_data_list(new_subgraphs)\n",
        "      with torch.no_grad():\n",
        "        q_target = self.q_net(self.embedding_module(batch.x, \n",
        "                                                    batch.edge_index, \n",
        "                                                    batch.batch))\n",
        "        q_target = q_target.max().view(-1, 1)\n",
        "        q_target = reward + discount * q_target\n",
        "      \n",
        "      # (4) Compute MSE loss between the predicted and target q-values\n",
        "      loss = F.mse_loss(q_prediction, q_target)\n",
        "\n",
        "      # (5) Backpropagate gradients\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      # (6) Decrease exploration rate\n",
        "      self.epsilon *= self.epsilon_decay_rate\n",
        "      self.epsilon = max(self.epsilon, self.epsilon_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SL-dvgspHTn7"
      },
      "outputs": [],
      "source": [
        "#@title DQN Agent\n",
        "\n",
        "class GNN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = SAGEConv(\n",
        "        hyperparameters['num_node_features'],\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        aggr = 'mean')\n",
        "    self.conv2 = SAGEConv(\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        hyperparameters['embedding_dimensions'],\n",
        "        aggr = 'mean')\n",
        "\n",
        "  def forward(self, x, edge_index, batch = None):\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.relu(x) # node embeddings\n",
        "    x = torch_geometric.nn.global_add_pool(x, batch = batch) # graph embedding\n",
        "\n",
        "    return x\n",
        "\n",
        "class QN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(hyperparameters['embedding_dimensions'], \n",
        "                         hyperparameters['QN_latent_dimensions'])\n",
        "    self.fc2 = nn.Linear(hyperparameters['QN_latent_dimensions'], 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DQNAgent():\n",
        "\n",
        "  def __init__(self, environment, \n",
        "               embedding_module, q_net, \n",
        "               replay_buffer, train_start, batch_size, \n",
        "               learn_every,\n",
        "               optimizer, \n",
        "               epsilon, epsilon_decay_rate, epsilon_min):\n",
        "    super().__init__()\n",
        "\n",
        "    self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "    self.environment = environment\n",
        "\n",
        "    self.embedding_module = embedding_module\n",
        "    self.q_net = q_net\n",
        "\n",
        "    self.target_embedding_module = deepcopy(embedding_module)\n",
        "    self.target_q_net = deepcopy(q_net)\n",
        "    \n",
        "    # disable gradients for target networks\n",
        "    for parameter in self.target_embedding_module.parameters():\n",
        "      parameter.requires_grad = False\n",
        "    for parameter in self.target_q_net.parameters():\n",
        "      parameter.requires_grad = False\n",
        "    \n",
        "    self.replay_buffer = replay_buffer\n",
        "    self.train_start = train_start # specify burn-in period\n",
        "    self.batch_size = batch_size\n",
        "    self.learn_every = learn_every # steps between updates to target nets\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "    self.epsilon_decay_rate = epsilon_decay_rate\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "    self.step = 0 \n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    available_actions = self.environment.get_actions(self.environment.visited)\n",
        "\n",
        "    new_subgraphs = [] # list to store all possible next states\n",
        "    for action in available_actions:\n",
        "      visited_nodes_new = deepcopy(self.environment.visited)\n",
        "      visited_nodes_new.append(action)\n",
        "      new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "      new_subgraphs.append(new_subgraph)\n",
        "\n",
        "    # create a batch to allow for a single forward pass\n",
        "    batch = Batch.from_data_list(new_subgraphs)\n",
        "    # gradients for the target networks are disabled\n",
        "    with torch.no_grad(): \n",
        "      q_values = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                batch.edge_index, \n",
        "                                                                batch.batch))\n",
        "\n",
        "    if torch.rand(1) < self.epsilon: # explore\n",
        "      action = np.random.choice(available_actions)\n",
        "    else: # exploit\n",
        "      action_idx = torch.argmax(q_values).item()\n",
        "      action = available_actions[action_idx]\n",
        "\n",
        "    return action\n",
        "\n",
        "  def train(self, state_dict, action, next_state_dict, reward, discount):\n",
        "\n",
        "    self.replay_buffer.add(state_dict, action, next_state_dict, reward, discount)\n",
        "    self.step += 1\n",
        "\n",
        "    if self.step < self.train_start: # inside the burn-in period\n",
        "      return\n",
        "\n",
        "    # (1) Get lists of experiences from memory\n",
        "    states, actions, next_states, rewards, discounts = self.replay_buffer.sample(self.batch_size)\n",
        "    \n",
        "    # (2) Build state + action = new subgraph (technically identical to next state)\n",
        "    new_subgraphs = []\n",
        "    for idx, state_dict in enumerate(states):\n",
        "      visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "      visited_nodes_new.append(actions[idx])\n",
        "      assert visited_nodes_new == next_states[idx]['visited'], \"train() assertion failed.\"\n",
        "      new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "      new_subgraphs.append(new_subgraph)\n",
        "    batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "    # (3) Pass batch of next_state subgraphs through ANN to get predicted q-values\n",
        "    q_predictions = self.q_net(self.embedding_module(batch.x, \n",
        "                                                     batch.edge_index, \n",
        "                                                     batch.batch))\n",
        "\n",
        "    # (4) Compute target q-values for batch\n",
        "    q_targets = []\n",
        "    for idx, next_state_dict in enumerate(next_states):\n",
        "      available_actions = self.environment.get_actions(next_state_dict['visited'])\n",
        "      if available_actions: # terminal states have no available actions\n",
        "        new_subgraphs = [] # each available action results in a new state\n",
        "        for action in available_actions:\n",
        "          visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "          visited_nodes_new.append(action)\n",
        "          new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "          new_subgraphs.append(new_subgraph)\n",
        "        batch = Batch.from_data_list(new_subgraphs)\n",
        "        with torch.no_grad(): # technically, no_grad() is unnecessary\n",
        "          q_target = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                    batch.edge_index, \n",
        "                                                                    batch.batch))\n",
        "          q_target = q_target.max().view(-1, 1) # get the largest next q-value\n",
        "          q_target = rewards[idx] + discounts[idx] * q_target\n",
        "          q_targets.append(q_target)\n",
        "      else:\n",
        "        q_targets.append(rewards[idx])\n",
        "    q_targets = torch.Tensor(q_targets).view(-1, 1)\n",
        "      \n",
        "    # (5) Compute MSE loss between predicted and target q-values\n",
        "    loss = F.mse_loss(q_predictions, q_targets).mean()\n",
        "\n",
        "    # (6) Backpropagate gradients\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # (7) Copy parameters from source to target networks\n",
        "    if self.step % self.learn_every == 0: \n",
        "      copy_parameters_from_to(self.embedding_module, self.target_embedding_module)\n",
        "      copy_parameters_from_to(self.q_net, self.target_q_net)\n",
        "      \n",
        "    # (8) Decrease exploration rate\n",
        "    self.epsilon *= self.epsilon_decay_rate\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dhp_QVr8vUW"
      },
      "outputs": [],
      "source": [
        "N = 20 # number of nodes\n",
        "\n",
        "# m = 2 # number of edges from new node\n",
        "# G = nx.barabasi_albert_graph(n = N, m = m)\n",
        "\n",
        "p = 0.4 # probability of edge creation\n",
        "G = nx.erdos_renyi_graph(n = N, p = p)\n",
        "\n",
        "nx.draw(G, with_labels = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJdoQPxA2zb0"
      },
      "outputs": [],
      "source": [
        "G_env = node_featurizer(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyM9RbdbF_6A"
      },
      "outputs": [],
      "source": [
        "# G_env = node_defeaturizer(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuNoMmO9ta2r"
      },
      "outputs": [],
      "source": [
        "# environment = GraphEnvironment(G_env, nx.average_clustering, start_node = 0)\n",
        "environment = GraphEnvironment(G_env, betti_numbers, start_node = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPOT3OcB9E9c"
      },
      "outputs": [],
      "source": [
        "# weighted_G = make_filtration_matrix(nx.to_numpy_array(G_env))\n",
        "# bars = get_barcode(weighted_G)\n",
        "# bettis = betti_curves(bars, N)\n",
        "\n",
        "# plot_bettis(bettis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLOWIscjMDPB"
      },
      "outputs": [],
      "source": [
        "num_steps = 500\n",
        "agent = RandomAgent(environment)\n",
        "all_episode_returns_random, _ = learn_environment(environment, agent, num_steps)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.plot(all_episode_returns_random, label = 'random', color = 'blue')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcZiWnC4teq2"
      },
      "outputs": [],
      "source": [
        "num_episodes = 10\n",
        "\n",
        "agent = RandomAgent(environment)\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment, \n",
        "                                                             agent, \n",
        "                                                             num_episodes)\n",
        "rewards_mean_random = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_random = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "agent = HighestDegreeAgent(environment)\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment, \n",
        "                                                             agent, \n",
        "                                                             1)\n",
        "rewards_mean_degree = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_degree = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "agent = GreedyAgent(environment)\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment, \n",
        "                                                             agent, \n",
        "                                                             1)\n",
        "rewards_mean_greedy = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_greedy = np.mean(np.array(feature_values), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N327GuhEzE-T"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {'num_node_features': 9,\n",
        "                   'GNN_latent_dimensions': 64,\n",
        "                   'embedding_dimensions': 64,\n",
        "                   'QN_latent_dimensions': 32,\n",
        "                   'buffer_size': 100000,\n",
        "                   'train_start': 320,\n",
        "                   'batch_size': 32,\n",
        "                   'learn_every': 4,\n",
        "                   'epsilon_initial': 0.1,\n",
        "                   'epsilon_decay_rate': 1,\n",
        "                   'epsilon_min': 0.1,\n",
        "                   'discount_factor': 0.75,\n",
        "                   'learning_rate': 3e-4}\n",
        "\n",
        "embedding_module = GNN(hyperparameters)\n",
        "q_net = QN(hyperparameters)\n",
        "\n",
        "agent = DQNAgent(environment, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 1, epsilon_decay_rate = 1, epsilon_min = 1)\n",
        "\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment, \n",
        "                                                             agent, \n",
        "                                                             num_episodes)\n",
        "rewards_mean_DQN = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.array(feature_values), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biN1q3vmsvLs"
      },
      "outputs": [],
      "source": [
        "plt.title('Untrained')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-sqJxrLWDL9"
      },
      "outputs": [],
      "source": [
        "epsilon = hyperparameters['epsilon_initial'] # exploration rate\n",
        "epsilon_decay_rate = hyperparameters['epsilon_decay_rate'] \n",
        "epsilon_min = hyperparameters['epsilon_min']\n",
        "\n",
        "discount_factor = hyperparameters['discount_factor']\n",
        "learning_rate = hyperparameters['learning_rate']\n",
        "# num_steps = 50000\n",
        "num_steps = 20000\n",
        "\n",
        "replay_buffer = ReplayBuffer(hyperparameters['buffer_size'])\n",
        "train_start = hyperparameters['train_start']\n",
        "batch_size = hyperparameters['batch_size']\n",
        "learn_every = hyperparameters['learn_every']\n",
        "\n",
        "optimizer = torch.optim.Adam([{'params': embedding_module.parameters()}, \n",
        "                              {'params': q_net.parameters()}], \n",
        "                             lr = learning_rate)\n",
        "agent = DQNAgent(environment, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer, train_start, batch_size, \n",
        "                 learn_every, \n",
        "                 optimizer, \n",
        "                 epsilon, epsilon_decay_rate, epsilon_min)\n",
        "\n",
        "all_episode_returns = [] \n",
        "# (1) Simultaneously train the GNN and Q-network\n",
        "# returns, feature_values = learn_environment(environment, agent, \n",
        "#                                             num_steps, \n",
        "#                                             discount_factor)\n",
        "# all_episode_returns.append(returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnTVrCxrhqRl"
      },
      "outputs": [],
      "source": [
        "def prun_test():\n",
        "  returns, feature_values = learn_environment(environment, agent, \n",
        "                                              num_steps, \n",
        "                                              discount_factor)\n",
        "  return returns, feature_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoiSN7PBhqtt"
      },
      "outputs": [],
      "source": [
        "%prun returns, feature_values = prun_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mPEnrcuc0i-E"
      },
      "outputs": [],
      "source": [
        "# @title Train GNN and Q-net alternately\n",
        "\n",
        "# num_alternate_trains = 25 # 25 works decently well\n",
        "# num_episodes_per_train_q_net = 50\n",
        "# num_episodes_per_train_GNN = 50\n",
        "\n",
        "# optimizer_GNN = torch.optim.Adam(embedding_module.parameters(), lr = 3e-4)\n",
        "# optimizer_q_net = torch.optim.Adam(q_net.parameters(), lr = 3e-4)\n",
        "\n",
        "# # (2) Alternately train the GNN and Q-network \n",
        "# for i in tqdm(range(num_alternate_trains)):\n",
        "\n",
        "#   if i % 2 == 0:\n",
        "#     agent = QNetworkAgent(environment, embedding_module, q_net, \n",
        "#                           optimizer_GNN, agent.epsilon)\n",
        "#     GNN_training_returns = learn_environment(environment, \n",
        "#                                                 agent, \n",
        "#                                                 num_episodes_per_train_GNN, \n",
        "#                                                 discount_factor)\n",
        "#     all_episode_returns.append(GNN_training_returns)\n",
        "#   else:\n",
        "#     agent = QNetworkAgent(environment, embedding_module, q_net, \n",
        "#                           optimizer_q_net, agent.epsilon)\n",
        "#     q_net_training_returns = learn_environment(environment, \n",
        "#                                                   agent, \n",
        "#                                                   num_episodes_per_train_q_net, \n",
        "#                                                   discount_factor)\n",
        "#     all_episode_returns.append(q_net_training_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMQ9GENLO94V"
      },
      "outputs": [],
      "source": [
        "all_episode_returns.append(returns)\n",
        "flat_all_episode_returns = []\n",
        "for sublist in all_episode_returns:\n",
        "  for item in sublist:\n",
        "    flat_all_episode_returns.append(item)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.plot(flat_all_episode_returns, label = 'DQN')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYHB6MiAtW0T"
      },
      "outputs": [],
      "source": [
        "get_NX_subgraph.cache_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2PeLiCnEY3c"
      },
      "outputs": [],
      "source": [
        "get_PyG_subgraph.cache_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ititd0jvtZGc"
      },
      "outputs": [],
      "source": [
        "compute_feature_value.cache_info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_neighbors.cache_info()"
      ],
      "metadata": {
        "id": "WX406NzGBRrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QxVgtcigcln1"
      },
      "outputs": [],
      "source": [
        "#@title Generate video\n",
        "\n",
        "# plotting_dict = {}\n",
        "# plotting_dict['random'] = feature_values_mean_random\n",
        "# plotting_dict['degree'] = feature_values_mean_degree\n",
        "# plotting_dict['greedy'] = feature_values_mean_greedy\n",
        "# plotting_dict['q_net_vanilla'] = [feature_values_mean_QNet_vanilla.tolist()] + feature_values_vanilla\n",
        "# plotting_dict['DQN'] = [feature_values_mean_DQN.tolist()] + feature_values\n",
        "\n",
        "# generate_video(plotting_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbLcONK91mOw"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(environment, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "\n",
        "states_NX, states_PyG, rewards, feature_values, steps = simulate(environment, agent, 1)\n",
        "rewards_mean_QNet = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.array(feature_values), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-HVHzuAJS7l"
      },
      "outputs": [],
      "source": [
        "plt.title('Trained')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InwIPw9DP7BE"
      },
      "outputs": [],
      "source": [
        "# G = nx.read_gpickle('example_graph.gpickle') # graph has degree attribute\n",
        "# nx.set_node_attributes(G, {node:node for node in range(len(G.nodes))}, \"node_ID\")\n",
        "# nx.draw(G, with_labels = True)\n",
        "\n",
        "N = 20 # number of nodes\n",
        "\n",
        "# m = 2 # number of edges from new node\n",
        "# G_unseen = nx.barabasi_albert_graph(n = N, m = m)\n",
        "\n",
        "p = 0.4 # probability of edge creation\n",
        "G_unseen = nx.erdos_renyi_graph(n = N, p = p)\n",
        "\n",
        "# nx.set_node_attributes(G_unseen, {node:node for node in range(len(G.nodes))}, \"node_ID\")\n",
        "# nx.set_node_attributes(G_unseen, {node:val for (node, val) in G.degree()}, \"degree\")\n",
        "# nx.set_node_attributes(G_unseen, nx.betweenness_centrality(G), \"betweenness_centrality\")\n",
        "# nx.set_node_attributes(G_unseen, nx.clustering(G), \"clustering_coefficient\")\n",
        "\n",
        "nx.draw(G_unseen, with_labels = True)\n",
        "\n",
        "# nx.write_gpickle(G, 'example_graph.gpickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLSEY8Lx-JFb"
      },
      "outputs": [],
      "source": [
        "G_env_unseen = node_featurizer(G_unseen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkQgr_rZPZHH"
      },
      "outputs": [],
      "source": [
        "# environment_unseen = GraphEnvironment(G_env_unseen, nx.average_clustering, start_node = 0)\n",
        "environment_unseen = GraphEnvironment(G_env_unseen, betti_numbers, start_node = 0)\n",
        "\n",
        "agent = RandomAgent(environment_unseen)\n",
        "states_NX, states_PyG, rewards, feature_values, steps = simulate(environment_unseen, \n",
        "                                                                 agent, num_episodes)\n",
        "rewards_mean_random = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_random = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "agent = HighestDegreeAgent(environment_unseen)\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment_unseen, \n",
        "                                                             agent, 1)\n",
        "rewards_mean_degree = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_degree = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "agent = GreedyAgent(environment_unseen)\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment_unseen, \n",
        "                                                             agent, 1)\n",
        "rewards_mean_greedy = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_greedy = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "agent = DQNAgent(environment_unseen, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "\n",
        "states_NX, states_PyG, rewards, feature_values, _ = simulate(environment_unseen, \n",
        "                                                             agent, \n",
        "                                                             num_episodes)\n",
        "rewards_mean_DQN = np.mean(np.array(rewards), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.array(feature_values), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NoOH9RUPywX"
      },
      "outputs": [],
      "source": [
        "plt.title('Trained - Unseen')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vANOoimX-IEz"
      },
      "outputs": [],
      "source": [
        "#@title Miscellaneous code\n",
        "\n",
        "# class Batch(NamedTuple):\n",
        "\n",
        "#   state_embedding: torch.Tensor\n",
        "#   action: torch.Tensor\n",
        "#   next_state_embedding: torch.Tensor\n",
        "#   reward: torch.Tensor\n",
        "#   discount: torch.Tensor\n",
        "\n",
        "# class ReplayBuffer():\n",
        "\n",
        "#   def __init__(self, state_dimensionality, action_dimensionality, buffer_size):\n",
        "\n",
        "#     self.buffer_size = buffer_size\n",
        "#     self.ptr = 0\n",
        "#     self.n_samples = 0\n",
        "#     self.state_embedding = torch.zeros(buffer_size, state_dimensionality, \n",
        "#                                        dtype = torch.float32)\n",
        "#     self.action = torch.zeros(buffer_size, action_dimensionality, \n",
        "#                               dtype = torch.int64)\n",
        "#     self.next_state_embedding = torch.zeros(buffer_size, state_dimensionality, \n",
        "#                                             dtype = torch.float32)\n",
        "#     self.reward = torch.zeros(buffer_size, 1, \n",
        "#                               dtype = torch.float32)\n",
        "#     self.discount = torch.zeros(buffer_size, 1, \n",
        "#                                 dtype = torch.float32)\n",
        "\n",
        "\n",
        "#   def add(self, state_embedding, action, next_state_embedding, reward, discount):\n",
        "\n",
        "#     self.state_embedding[self.ptr] = state_embedding\n",
        "#     self.action[self.ptr] = action\n",
        "#     self.next_state_embedding[self.ptr] = next_state_embedding\n",
        "#     self.reward[self.ptr] = reward\n",
        "#     self.discount[self.ptr] = discount\n",
        "    \n",
        "#     if self.n_samples < self.buffer_size:\n",
        "#       self.n_samples += 1\n",
        "\n",
        "#     self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "\n",
        "#   def sample(self, batch_size):\n",
        "\n",
        "#     idx = np.random.choice(self.n_samples, batch_size)    \n",
        "#     state_embedding = self.state_embedding[idx]\n",
        "#     action = self.action[idx]\n",
        "#     next_state_embedding = self.next_state_embedding[idx]\n",
        "#     reward = self.reward[idx]\n",
        "#     discount = self.discount[idx]\n",
        "    \n",
        "#     return Batch(state_embedding, action, next_state_embedding, reward, discount)\n",
        "\n",
        "# class Batch(NamedTuple):\n",
        "\n",
        "#   state: torch_geometric.data.data.Data\n",
        "#   action: torch.Tensor\n",
        "#   next_state: torch_geometric.data.data.Data\n",
        "#   reward: torch.Tensor\n",
        "#   discount: torch.Tensor\n",
        "\n",
        "# class ReplayBuffer():\n",
        "\n",
        "#   def __init__(self, state_dimensionality, buffer_size):\n",
        "\n",
        "#     self.buffer_size = buffer_size\n",
        "#     self.ptr = 0\n",
        "#     self.n_samples = 0\n",
        "    \n",
        "#     self.state = torch.zeros(buffer_size, state_dimensionality, dtype = torch.float32)\n",
        "#     self.action = torch.zeros(buffer_size, 1, dtype = torch.int64)\n",
        "#     self.next_state = torch.zeros(buffer_size, state_dimensionality, dtype = torch.float32)\n",
        "#     self.reward = torch.zeros(buffer_size, 1, dtype = torch.float32)\n",
        "#     self.discount = torch.zeros(buffer_size, 1, dtype = torch.float32)\n",
        "\n",
        "\n",
        "#   def add(self, state, action, next_state, reward, discount):\n",
        "\n",
        "#     self.state[self.ptr] = state\n",
        "#     self.action[self.ptr] = action\n",
        "#     self.next_state[self.ptr] = next_state\n",
        "#     self.reward[self.ptr] = reward\n",
        "#     self.discount[self.ptr] = discount\n",
        "    \n",
        "#     if self.n_samples < self.buffer_size:\n",
        "#       self.n_samples += 1\n",
        "\n",
        "#     self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "\n",
        "#   def sample(self, batch_size):\n",
        "\n",
        "#     idx = np.random.choice(self.n_samples, batch_size)    \n",
        "#     state = self.state[idx]\n",
        "#     action = self.action[idx]\n",
        "#     next_state = self.next_state[idx]\n",
        "#     reward = self.reward[idx]\n",
        "#     discount = self.discount[idx]\n",
        "    \n",
        "#     return Batch(state, action, next_state, reward, discount)\n",
        "\n",
        "# class QNetworkAgent():\n",
        "\n",
        "#   def __init__(self, environment, embedding_module, q_net, optimizer, \n",
        "#                replay_buffer, batch_size, epsilon):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "#     self.environment = environment\n",
        "#     self.embedding_module = embedding_module\n",
        "#     self.q_net = q_net\n",
        "#     self.optimizer = optimizer\n",
        "\n",
        "#     self.replay_buffer = replay_buffer\n",
        "#     self.batch_size = batch_size # number of points to sample from the buffer\n",
        "\n",
        "#     self.epsilon = epsilon\n",
        "\n",
        "#   def choose_action(self):\n",
        "\n",
        "#     available_actions = self.environment.get_actions()\n",
        "\n",
        "#     with torch.no_grad(): # gradients are not needed when selecting actions\n",
        "#       # Option 1: perform epsilon-greedy exploration\n",
        "#       if np.random.uniform() < self.epsilon:\n",
        "#         return random.choice(available_actions)\n",
        "#       # Option 2: select action greedily based on q-values\n",
        "#       else:\n",
        "#         state_embedding = self.embedding_module(self.environment.state_PyG)\n",
        "#         q_values = self.q_net(state_embedding)\n",
        "#         valid_q_values = q_values[0][available_actions]\n",
        "#         action_idx = torch.argmax(valid_q_values).item()\n",
        "#         return available_actions[action_idx]\n",
        "\n",
        "#   def train(self, state, action, next_state, reward, discount, step):\n",
        "\n",
        "#     # with torch.no_grad():\n",
        "#     #   state_embedding = self.embedding_module(state)\n",
        "#     #   next_state_embedding = self.embedding_module(next_state)\n",
        "\n",
        "#     # self.replay_buffer.add(state_embedding, action, \n",
        "#     #                        next_state_embedding, reward, discount)\n",
        "\n",
        "#     self.replay_buffer.add(state, action, next_state, reward, discount)\n",
        "\n",
        "#     if step < self.batch_size:\n",
        "#       return # do not train until the buffer has at least one data batch\n",
        "\n",
        "#     batch = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "#     # (1) Predict a q-value based on state-action pair\n",
        "#     # state_embedding = self.embedding_module(state)\n",
        "#     q_values = self.q_net(self.embedding_module(batch.state))\n",
        "#     q_predicted = q_values.gather(1, batch.action)\n",
        "\n",
        "#     # (2) Compute target q-value; no gradient is needed for the target\n",
        "#     with torch.no_grad():\n",
        "#       # next_q_values = self.q_net(batch.next_state_embedding)\n",
        "#       next_q_values = self.q_net(self.embedding_module(batch.next_state))\n",
        "#       q_target = next_q_values.max(dim = 1)[0].view(-1, 1)\n",
        "#       q_target = batch.reward + batch.discount * q_target\n",
        "\n",
        "#     # (3) Compute MSE loss between the predicted and target q-values\n",
        "#     loss = F.mse_loss(q_predicted, q_target).mean()\n",
        "\n",
        "#     # (4) Backpropagate gradients\n",
        "#     self.optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     self.optimizer.step()\n",
        "\n",
        "# num_episodes = 500\n",
        "# discount_factor = 0.25\n",
        "\n",
        "# all_episode_returns_q_net = learn_environment(environment, agent, \n",
        "#                                               num_episodes, discount_factor)\n",
        "\n",
        "# print(compute_Frobenius_norm(embedding_module), compute_Frobenius_norm(q_net))\n",
        "\n",
        "# plt.figure()\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Return')\n",
        "# plt.plot(all_episode_returns_q_net, label = 'q-net')\n",
        "# plt.legend()\n",
        "\n",
        "# states_NX, states_PyG, rewards, feature_values, steps = simulate(environment, agent, 10)\n",
        "# rewards_mean_QNet = np.mean(np.array(rewards), axis = 0)\n",
        "# feature_values_mean_QNet = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "# plt.figure()\n",
        "# plt.xlabel('Step')\n",
        "# plt.ylabel('Average Clustering')\n",
        "# plt.plot(feature_values_mean_random, label = 'random')\n",
        "# plt.plot(feature_values_mean_degree, label = 'max degree')\n",
        "# plt.plot(feature_values_mean_greedy, label = 'greedy')\n",
        "# plt.plot(feature_values_mean_QNet, label = 'q-net')\n",
        "# plt.legend()\n",
        "\n",
        "# agent = QNetworkAgent(environment, embedding_module, q_net, optimizer, epsilon)\n",
        "# all_episode_returns_q_net = learn_environment(environment, agent, \n",
        "#                                               500, discount_factor)\n",
        "\n",
        "# def learn_environment(environment, agent, num_episodes, \n",
        "#                       discount_factor = None, verbose = True):\n",
        "#   \"\"\"learn_environment() simulates agent-environment interaction for \n",
        "#   a number of steps specified by num_steps and trains the agent if it is \n",
        "#   capable of being trained. Note that the interaction ends after a finite \n",
        "#   number of steps regardless of whether the ongoing episode has terminated.\"\"\"\n",
        "\n",
        "#   all_episode_returns = []\n",
        "#   all_episode_feature_values = []\n",
        "\n",
        "#   for episode in tqdm(range(num_episodes), disable = not verbose):\n",
        "\n",
        "#     state_dict, terminal, info = environment.reset()\n",
        "#     episode_return = 0 # to track cumulative reward (i.e. return) in ongoing episode\n",
        "#     episode_feature_values = [info['feature_value']]\n",
        "\n",
        "#     while not terminal:\n",
        "#       action = agent.choose_action()\n",
        "#       next_state_dict, reward, terminal, info = environment.step(action)\n",
        "#       episode_return += reward\n",
        "#       episode_feature_values.append(info['feature_value'])\n",
        "\n",
        "#       if agent.is_trainable:\n",
        "#         discount = discount_factor * (1 - terminal)\n",
        "#         agent.train(state_dict, action, next_state_dict, reward, discount) \n",
        "\n",
        "#       state_dict = next_state_dict\n",
        "      \n",
        "#     all_episode_returns.append(episode_return)\n",
        "#     all_episode_feature_values.append(episode_feature_values)\n",
        "\n",
        "#   environment.reset()\n",
        "\n",
        "#   return all_episode_returns, all_episode_feature_values"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMBa4Z2wVJehsyEz/aCUkjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}