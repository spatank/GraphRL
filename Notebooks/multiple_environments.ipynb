{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/GraphRL/blob/main/multiple_environments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVZ0BnjpJXJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/GraphRL/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXItXVrjp9nj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import networkx as nx\n",
        "from functools import lru_cache\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from typing import NamedTuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "import torch # check version using torch.__version__ before using PyG wheels\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!{sys.executable} -m pip install -q torch-geometric\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric import utils, transforms\n",
        "\n",
        "!{sys.executable} -m pip install -q Cython\n",
        "!{sys.executable} -m pip install -q Ripser\n",
        "\n",
        "from ripser import ripser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyvCvGEpzLS8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Environments\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def get_NX_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_NX.subgraph(list(frozen_set_of_nodes))\n",
        "\n",
        "@lru_cache(maxsize = 500000)\n",
        "def get_PyG_subgraph(environment, frozen_set_of_nodes):\n",
        "\n",
        "  return environment.graph_PyG.subgraph(torch.tensor(list(frozen_set_of_nodes)))\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def compute_feature_value(environment, state_subgraph_NX):\n",
        "\n",
        "  return environment.feature_function(state_subgraph_NX)\n",
        "\n",
        "@lru_cache(maxsize = 100000)\n",
        "def get_neighbors(environment, frozen_set_of_nodes, cutoff = 1):\n",
        "  \"\"\"\n",
        "  Returns the n-th degree neighborhood of a set of nodes, where degree \n",
        "  is specified by the cutoff argument.\n",
        "  \"\"\"\n",
        "\n",
        "  nodes = list(frozen_set_of_nodes)\n",
        "  neighbors = set()\n",
        "\n",
        "  for node in nodes:\n",
        "    neighbors.update(set(nx.single_source_shortest_path_length(environment.graph_NX, \n",
        "                                                               node, \n",
        "                                                               cutoff = cutoff).keys()))\n",
        "    \n",
        "  neighbors = neighbors - set(nodes) # remove input nodes from their own neighborhood\n",
        "\n",
        "  if not neighbors:\n",
        "    neighbors = set(environment.graph_NX.nodes()) - set(environment.visited)\n",
        "\n",
        "  return list(neighbors)\n",
        "\n",
        "class GraphEnvironment():\n",
        "  \n",
        "  def __init__(self, ID, graph_NX, feature):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ID = ID # identifier for the environment\n",
        "\n",
        "    self.graph_NX = graph_NX # environment graph (NetworkX Graph object)  \n",
        "    self.graph_PyG = utils.from_networkx(graph_NX, group_node_attrs = all)\n",
        "    self.num_nodes = self.graph_NX.number_of_nodes()\n",
        "\n",
        "    self.visited = [random.choice(list(self.graph_NX.nodes()))] # list of visited nodes\n",
        "\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "\n",
        "    self.feature_function = feature # function handle to network feature-of-interest\n",
        "    self.feature_values = [self.feature_function(self.state_NX)] # list to store values of the feature-of-interest\n",
        "    \n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Execute an action in the environment, i.e. visit a new node.\n",
        "    \"\"\"\n",
        "\n",
        "    assert action in self.get_actions(self.visited), \"Invalid action!\"\n",
        "    visited_new = deepcopy(self.visited)\n",
        "    visited_new.append(action) # add new node to list of visited nodes\n",
        "    self.visited = visited_new\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    reward = self.compute_reward()\n",
        "    terminal = bool(len(self.visited) == self.graph_NX.number_of_nodes())\n",
        "\n",
        "    return self.get_state_dict(), reward, terminal, self.get_info()\n",
        "\n",
        "  def compute_reward(self):\n",
        "\n",
        "    self.feature_values.append(compute_feature_value(self, self.state_NX))\n",
        "    reward = sum(self.feature_values)/len(self.visited)\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset to initial state.\n",
        "    \"\"\"\n",
        "\n",
        "    self.visited = [random.choice(list(self.graph_NX.nodes()))] # empty the list of visited nodes\n",
        "    self.state_NX = get_NX_subgraph(self, frozenset(self.visited))\n",
        "    self.state_PyG = get_PyG_subgraph(self, frozenset(self.visited))\n",
        "    self.feature_values = [compute_feature_value(self, self.state_NX)]\n",
        "    terminal = False\n",
        "\n",
        "    return self.get_state_dict(), terminal, self.get_info()\n",
        "\n",
        "  def get_state_dict(self):\n",
        "\n",
        "    return {'visited': self.visited, \n",
        "            'state_NX': self.state_NX, \n",
        "            'state_PyG': self.state_PyG}\n",
        "      \n",
        "  def get_info(self):\n",
        "    \n",
        "    return {'environment_ID': self.ID, # useful for DQN training\n",
        "            'feature_value': compute_feature_value(self, self.state_NX)}\n",
        "  \n",
        "  def get_actions(self, nodes):\n",
        "    \"\"\" \n",
        "    Returns available actions given a list of nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    return get_neighbors(self, frozenset(nodes))\n",
        "  \n",
        "  def render(self):\n",
        "    \"\"\"\n",
        "    Render current state to the screen.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    nx.draw(self.state_NX, with_labels = True)\n",
        "\n",
        "class MultipleEnvironments():\n",
        "\n",
        "  def __init__(self, environments):\n",
        "    \n",
        "    self.environments = environments\n",
        "    self.num_environments = len(self.environments)\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    state_dicts = []\n",
        "    terminals = []\n",
        "    all_info = []\n",
        "\n",
        "    for environment in self.environments:\n",
        "      state_dict, terminal, info = environment.reset()\n",
        "      state_dicts.append(state_dict)\n",
        "      terminals.append(terminal)\n",
        "      all_info.append(info)\n",
        "\n",
        "    return state_dicts, terminals, all_info\n",
        "  \n",
        "  def step(self, actions):\n",
        "\n",
        "    state_dicts = []\n",
        "    rewards = []\n",
        "    terminals = []\n",
        "    all_info = []\n",
        "\n",
        "    for idx, environment in enumerate(self.environments):\n",
        "      state_dict, reward, terminal, info = environment.step(actions[idx])\n",
        "      state_dicts.append(state_dict)\n",
        "      rewards.append(reward)\n",
        "      terminals.append(terminal)\n",
        "      all_info.append(info)\n",
        "\n",
        "    return state_dicts, rewards, terminals, all_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koDnpGaHzOPv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Baseline Agents\n",
        "\n",
        "class RandomAgent():\n",
        "  \"\"\"RandomAgent() chooses an action at random. The agent is not deterministic.\"\"\"\n",
        "  \n",
        "  def __init__(self, environments):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.environments = environments # instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      action = random.choice(available_actions)\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "class HighestDegreeAgent():\n",
        "  \"\"\"HighestDegreeAgent() chooses the action with the highest node degree. The \n",
        "  agent is deterministic.\"\"\"\n",
        "\n",
        "  def __init__(self, environments):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = environments # instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      all_degrees = list(zip(*(environment.graph_NX.degree(available_actions))))[1]\n",
        "      action_idx = all_degrees.index(max(all_degrees)) # first largest when ties\n",
        "      action = available_actions[action_idx]\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "class GreedyAgent():\n",
        "  \"\"\"GreedyAgent() chooses the action that would result in the greatest reward.\n",
        "  The agent uses a copy of the environment to simulate each available action and \n",
        "  returns the best performing action. The agent is deterministic.\"\"\"\n",
        "\n",
        "  def __init__(self, environments):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = environments # instance of MultipleEnvironments() class\n",
        "    self.is_trainable = False # useful to manage control flow during simulations\n",
        "\n",
        "  def choose_action(self):\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      best_reward = float('-inf')\n",
        "      best_action = None\n",
        "\n",
        "      for action in available_actions:\n",
        "        environment_copy = deepcopy(environment)\n",
        "        state_dict, reward, terminal, info = environment_copy.step(action)\n",
        "\n",
        "        if reward > best_reward:\n",
        "          best_reward = reward\n",
        "          best_action = action\n",
        "\n",
        "      actions.append(best_action)\n",
        "\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DQN Agent\n",
        "\n",
        "class GNN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv1 = SAGEConv(\n",
        "        hyperparameters['num_node_features'],\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        aggr = 'mean')\n",
        "    self.conv2 = SAGEConv(\n",
        "        hyperparameters['GNN_latent_dimensions'],\n",
        "        hyperparameters['embedding_dimensions'],\n",
        "        aggr = 'mean')\n",
        "\n",
        "  def forward(self, x, edge_index, batch = None):\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.relu(x) # node embeddings\n",
        "    x = torch_geometric.nn.global_add_pool(x, batch = batch) # graph embedding\n",
        "\n",
        "    return x\n",
        "\n",
        "class QN(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(hyperparameters['embedding_dimensions'], \n",
        "                         hyperparameters['QN_latent_dimensions'])\n",
        "    self.fc2 = nn.Linear(hyperparameters['QN_latent_dimensions'], 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DQNAgent():\n",
        "\n",
        "  def __init__(self, environments, \n",
        "               embedding_module, q_net, \n",
        "               replay_buffer, train_start, batch_size, \n",
        "               learn_every,\n",
        "               optimizer, \n",
        "               epsilon, epsilon_decay_rate, epsilon_min):\n",
        "    super().__init__()\n",
        "\n",
        "    self.environments = environments # instance of MultipleEnvironments() class\n",
        "    self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "    self.embedding_module = embedding_module\n",
        "    self.q_net = q_net\n",
        "\n",
        "    self.target_embedding_module = deepcopy(embedding_module)\n",
        "    self.target_q_net = deepcopy(q_net)\n",
        "    \n",
        "    # disable gradients for target networks\n",
        "    for parameter in self.target_embedding_module.parameters():\n",
        "      parameter.requires_grad = False\n",
        "\n",
        "    for parameter in self.target_q_net.parameters():\n",
        "      parameter.requires_grad = False\n",
        "    \n",
        "    self.replay_buffer = replay_buffer\n",
        "    self.train_start = train_start # specify burn-in period\n",
        "    self.batch_size = batch_size\n",
        "    self.learn_every = learn_every # steps between updates to target nets\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "    self.epsilon_decay_rate = epsilon_decay_rate\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "    self.step = 0\n",
        "\n",
        "  def choose_action(self):\n",
        "    \"\"\"\n",
        "    Choose an action to perform for each environment in self.environments.\n",
        "    \"\"\"\n",
        "\n",
        "    actions = []\n",
        "\n",
        "    for environment in self.environments.environments:\n",
        "      available_actions = environment.get_actions(environment.visited)\n",
        "      new_subgraphs = [] # list to store all possible next states\n",
        "\n",
        "      for action in available_actions:\n",
        "        visited_nodes_new = deepcopy(environment.visited)\n",
        "        visited_nodes_new.append(action)\n",
        "        new_subgraph = get_PyG_subgraph(environment, frozenset(visited_nodes_new))\n",
        "        new_subgraphs.append(new_subgraph)\n",
        "\n",
        "      # create a batch to allow for a single forward pass\n",
        "      batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "      # gradients for the target networks are disabled\n",
        "      with torch.no_grad(): # technically redundant\n",
        "        q_values = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                  batch.edge_index, \n",
        "                                                                  batch.batch))\n",
        "      if torch.rand(1) < self.epsilon: # explore\n",
        "        action = np.random.choice(available_actions)\n",
        "      else: # exploit\n",
        "        action_idx = torch.argmax(q_values).item()\n",
        "        action = available_actions[action_idx]\n",
        "\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "  def train(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n",
        "\n",
        "    self.replay_buffer.add(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n",
        "    self.step += 1\n",
        "    \n",
        "    if self.step < self.train_start: # inside the burn-in period\n",
        "      return\n",
        "\n",
        "    # (1) Get lists of experiences from memory\n",
        "    states, actions, next_states, rewards, discounts, all_info = self.replay_buffer.sample(self.batch_size)\n",
        "    \n",
        "    # (2) Build state + action = new subgraph (technically identical to next state)\n",
        "    new_subgraphs = []\n",
        "    for idx, state_dict in enumerate(states):\n",
        "      visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "      visited_nodes_new.append(actions[idx])\n",
        "      assert visited_nodes_new == next_states[idx]['visited'], \"train() assertion failed.\"\n",
        "      new_subgraph = get_PyG_subgraph(self.environments.environments[all_info[idx]['environment_ID']], \n",
        "                                      frozenset(visited_nodes_new))\n",
        "      new_subgraphs.append(new_subgraph)\n",
        "\n",
        "    batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "    # (3) Pass batch of next_state subgraphs through ANN to get predicted q-values\n",
        "    q_predictions = self.q_net(self.embedding_module(batch.x, \n",
        "                                                     batch.edge_index, \n",
        "                                                     batch.batch))\n",
        "\n",
        "    # (4) Compute target q-values for batch\n",
        "    q_targets = []\n",
        "    for idx, next_state_dict in enumerate(next_states):\n",
        "      available_actions = self.environments.environments[all_info[idx]['environment_ID']].get_actions(next_state_dict['visited'])\n",
        "\n",
        "      if available_actions: # terminal states have no available actions\n",
        "        new_subgraphs = [] # each available action results in a new state\n",
        "\n",
        "        for action in available_actions:\n",
        "          visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "          visited_nodes_new.append(action)\n",
        "          new_subgraph = get_PyG_subgraph(self.environments.environments[all_info[idx]['environment_ID']], \n",
        "                                          frozenset(visited_nodes_new))\n",
        "          new_subgraphs.append(new_subgraph)\n",
        "\n",
        "        batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "        with torch.no_grad(): # technically, no_grad() is unnecessary\n",
        "          q_target = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "                                                                    batch.edge_index, \n",
        "                                                                    batch.batch))\n",
        "          q_target = q_target.max().view(-1, 1) # get the largest next q-value\n",
        "          q_target = rewards[idx] + discounts[idx] * q_target\n",
        "          q_targets.append(q_target)\n",
        "\n",
        "      else:\n",
        "        q_targets.append(rewards[idx])\n",
        "\n",
        "    q_targets = torch.Tensor(q_targets).view(-1, 1)\n",
        "      \n",
        "    # (5) Compute MSE loss between predicted and target q-values\n",
        "    loss = F.mse_loss(q_predictions, q_targets).mean()\n",
        "\n",
        "    # (6) Backpropagate gradients\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # (7) Copy parameters from source to target networks\n",
        "    if self.step % self.learn_every == 0: \n",
        "      copy_parameters_from_to(self.embedding_module, self.target_embedding_module)\n",
        "      copy_parameters_from_to(self.q_net, self.target_q_net)\n",
        "      \n",
        "    # (8) Decrease exploration rate\n",
        "    self.epsilon *= self.epsilon_decay_rate\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PsZuP6S0Vo68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDU0sMoB2vfd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions: Simulation\n",
        "\n",
        "def simulate(environments, agent, num_episodes = 100, verbose = True):\n",
        "  \"\"\"\n",
        "  Simulate agent in multiple environments for a specified number of episodes.\n",
        "  We do not use methods from the MultipleEnvironment() class because each \n",
        "  environment may have a different number of nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  all_rewards = []\n",
        "  all_feature_values = []\n",
        "\n",
        "  for idx, environment in enumerate(tqdm(environments.environments, \n",
        "                                         disable = not verbose)):\n",
        "    \n",
        "    state_dict, terminal, info = environment.reset()\n",
        "    environment_rewards = []\n",
        "    environment_feature_values = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "      episode_rewards = []\n",
        "      episode_feature_values = []\n",
        "      \n",
        "      while not terminal:\n",
        "        actions = agent.choose_action() \n",
        "        action = actions[idx] # agent chooses an action for each environment\n",
        "        state_dict, reward, terminal, info = environment.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        episode_feature_values.append(info['feature_value'])\n",
        "      \n",
        "      state_dict, terminal, info = environment.reset() # reset environment after use\n",
        "\n",
        "      environment_rewards.append(episode_rewards)\n",
        "      environment_feature_values.append(episode_feature_values)\n",
        "      \n",
        "    all_rewards.append(environment_rewards)\n",
        "    all_feature_values.append(environment_feature_values)\n",
        "  \n",
        "  return all_rewards, all_feature_values\n",
        "\n",
        "def learn_environments(environments, agent, num_steps, \n",
        "                       discount_factor = None, verbose = True):\n",
        "  \"\"\"\n",
        "  Train agent on multiple environments by simulating agent-environment \n",
        "  interactions for a specified number of steps.\n",
        "  \"\"\"\n",
        "\n",
        "  all_episode_returns = [[] for i in range(environments.num_environments)]\n",
        "  all_episode_feature_values = [[] for i in range(environments.num_environments)]\n",
        "\n",
        "  episode_returns = [0] * environments.num_environments\n",
        "  episode_feature_values = [[] for i in range(environments.num_environments)]\n",
        "\n",
        "  state_dicts, terminals, all_info = environments.reset()\n",
        "\n",
        "  for step in tqdm(range(num_steps), disable = not verbose):\n",
        "    actions = agent.choose_action() # choose an action for each environment\n",
        "    next_state_dicts, rewards, terminals, all_info = environments.step(actions)\n",
        "    episode_returns = [sum(x) for x in zip(rewards, episode_returns)]\n",
        "\n",
        "    for idx, info in enumerate(all_info):\n",
        "      episode_feature_values[idx].append(info['feature_value'])\n",
        "\n",
        "    if agent.is_trainable:\n",
        "      discounts = [discount_factor * (1 - terminal) for terminal in terminals]\n",
        "      agent.train(state_dicts, actions, next_state_dicts, rewards, discounts, all_info)\n",
        "      \n",
        "    state_dicts = next_state_dicts\n",
        "\n",
        "    for idx, terminal in enumerate(terminals):\n",
        "      # if terminal then gather episode results for this environment and reset\n",
        "      if terminal: \n",
        "        all_episode_returns[idx].append(episode_returns[idx])\n",
        "        episode_returns[idx] = 0\n",
        "        all_episode_feature_values[idx].append(episode_feature_values[idx])\n",
        "        episode_feature_values[idx] = []\n",
        "        state_dict, terminal, info = environments.environments[idx].reset()\n",
        "        state_dicts[idx] = state_dict\n",
        "\n",
        "  environments.reset()\n",
        "\n",
        "  return all_episode_returns, all_episode_feature_values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions: Miscellaneous\n",
        "\n",
        "def compute_Frobenius_norm(network):\n",
        "    \"\"\"\n",
        "    Compute the Frobenius norm of all network tensors.\n",
        "    \"\"\"\n",
        "    norm = 0.0\n",
        "\n",
        "    for name, param in network.named_parameters():\n",
        "        norm += torch.norm(param).data  \n",
        "               \n",
        "    return norm.item()\n",
        "\n",
        "def copy_parameters_from_to(source_network, target_network):\n",
        "  \"\"\"\n",
        "  Update the parameters of the target network by copying values from the source\n",
        "  network.\n",
        "  \"\"\"\n",
        "\n",
        "  for source, target in zip(source_network.parameters(), target_network.parameters()):\n",
        "    target.data.copy_(source.data)\n",
        "\n",
        "  return\n",
        "\n",
        "class ReplayBuffer():\n",
        "  \n",
        "  def __init__(self, buffer_size):\n",
        "\n",
        "    self.buffer_size = buffer_size\n",
        "    self.ptr = 0 # index to latest experience in memory\n",
        "    self.num_experiences = 0 # number of experiences stored in memory\n",
        "    self.states = [None] * self.buffer_size\n",
        "    self.actions = [None] * self.buffer_size\n",
        "    self.next_states = [None] * self.buffer_size\n",
        "    self.rewards = [None] * self.buffer_size\n",
        "    self.discounts = [None] * self.buffer_size\n",
        "    self.all_info = [None] * self.buffer_size\n",
        "\n",
        "  def add(self, state_dicts, actions, next_state_dicts, rewards, discounts, all_info):\n",
        "\n",
        "    # check if arguments are lists\n",
        "    if not isinstance(state_dicts, list): # i.e. a single experience\n",
        "      state_dicts = [state_dicts]\n",
        "      actions = [actions]\n",
        "      next_state_dicts = [next_state_dicts]\n",
        "      rewards = [rewards]\n",
        "      discounts = [discounts]\n",
        "      all_info = [all_info]\n",
        "\n",
        "    for i in range(len(state_dicts)):\n",
        "      self.states[self.ptr] = state_dicts[i]\n",
        "      self.actions[self.ptr] = actions[i]\n",
        "      self.next_states[self.ptr] = next_state_dicts[i]\n",
        "      self.rewards[self.ptr] = rewards[i]\n",
        "      self.discounts[self.ptr] = discounts[i]\n",
        "      self.all_info[self.ptr] = all_info[i]\n",
        "      \n",
        "      if self.num_experiences < self.buffer_size:\n",
        "        self.num_experiences += 1\n",
        "\n",
        "      self.ptr = (self.ptr + 1) % self.buffer_size \n",
        "      # if (ptr + 1) exceeds buffer size then begin overwriting older experiences\n",
        "\n",
        "  def sample(self, batch_size):      \n",
        "\n",
        "    indices = np.random.choice(self.num_experiences, batch_size)   \n",
        "    states = [self.states[index] for index in indices] \n",
        "    actions = [self.actions[index] for index in indices] \n",
        "    next_states = [self.next_states[index] for index in indices] \n",
        "    rewards = [self.rewards[index] for index in indices] \n",
        "    discounts = [self.discounts[index] for index in indices] \n",
        "    all_info = [self.all_info[index] for index in indices] \n",
        "    \n",
        "    return states, actions, next_states, rewards, discounts, all_info\n",
        "\n",
        "def generate_video(plotting_dict):\n",
        "\n",
        "  feature_values_random = plotting_dict['random']\n",
        "  feature_values_degree = plotting_dict['degree']\n",
        "  feature_values_greedy = plotting_dict['greedy']\n",
        "  feature_values_DQN = np.array(plotting_dict['DQN'])\n",
        "\n",
        "  xlim = feature_values_DQN.shape[1]\n",
        "  x = np.arange(xlim) # number of nodes\n",
        "\n",
        "  ylim = max(max(feature_values_random), \n",
        "             max(feature_values_degree), \n",
        "             max(feature_values_greedy), \n",
        "             np.max(feature_values_DQN))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.axis([0, xlim, 0, ylim + 0.01 * ylim])\n",
        "\n",
        "  line1, = ax.plot(x, feature_values_random, label = 'random', color = 'blue')\n",
        "  line2, = ax.plot(x, feature_values_degree, label = 'max degree', color = 'orange')\n",
        "  line3, = ax.plot(x, feature_values_greedy, label = 'greedy', color = 'green')\n",
        "  line4, = ax.plot([], [], label = 'DQN', color = 'black')\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  plt.xlabel('Step')\n",
        "  plt.ylabel('Value')\n",
        "\n",
        "  def animate(i):\n",
        "    line4.set_data(x, feature_values_DQN[i])\n",
        "    \n",
        "  anim_handle = animation.FuncAnimation(fig, animate, \n",
        "                                        frames = len(feature_values_DQN),\n",
        "                                        interval = 100,  \n",
        "                                        blit = False, repeat = False, \n",
        "                                        repeat_delay = 10000)\n",
        "  plt.close() # do not show extra figure\n",
        "\n",
        "  return anim_handle\n",
        "\n",
        "def node_featurizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  attributes = {}\n",
        "\n",
        "  for node in graph_NX.nodes():\n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 1).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "\n",
        "    if neighborhood:\n",
        "      neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    else: # no neighbors\n",
        "      neighborhood_degrees = [0]\n",
        "\n",
        "    node_attributes = {}\n",
        "    node_attributes['degree_1'] = graph_NX.degree(node)\n",
        "    node_attributes['min_degree_1'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_1'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_1'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_1'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    neighborhood = set(nx.single_source_shortest_path_length(graph_NX, node, cutoff = 2).keys())\n",
        "    neighborhood.remove(node) # remove node from its own neighborhood\n",
        "    neighborhood = list(neighborhood) \n",
        "\n",
        "    if neighborhood:\n",
        "      neighborhood_degrees = list(map(list, zip(*graph_NX.degree(neighborhood))))[1]\n",
        "    else: # no neighbors\n",
        "      neighborhood_degrees = [0]\n",
        "\n",
        "    node_attributes['min_degree_2'] = min(neighborhood_degrees)\n",
        "    node_attributes['max_degree_2'] = max(neighborhood_degrees)\n",
        "    node_attributes['mean_degree_2'] = float(np.mean(neighborhood_degrees))\n",
        "    node_attributes['std_degree_2'] = float(np.std(neighborhood_degrees))\n",
        "\n",
        "    attributes[node] = node_attributes\n",
        "    \n",
        "  nx.set_node_attributes(graph_NX, attributes)\n",
        "\n",
        "  return graph_NX\n",
        "\n",
        "def node_defeaturizer(graph_NX):\n",
        "\n",
        "  graph_NX = deepcopy(graph_NX)\n",
        "\n",
        "  for (n, d) in graph_NX.nodes(data = True):\n",
        "\n",
        "    del d[\"degree_1\"]\n",
        "    del d[\"min_degree_1\"]\n",
        "    del d[\"max_degree_1\"]\n",
        "    del d[\"mean_degree_1\"]\n",
        "    del d[\"std_degree_1\"]\n",
        "    del d[\"min_degree_2\"]\n",
        "    del d[\"max_degree_2\"]\n",
        "    del d[\"mean_degree_2\"]\n",
        "    del d[\"std_degree_2\"]\n",
        "\n",
        "    return graph_NX"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TsIkpourYxFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions: Rewards\n",
        "\n",
        "def make_filtration_matrix(G):\n",
        "    \"\"\"\n",
        "    Takes in adjacency matrix and returns a filtration matrix for Ripser\n",
        "    \"\"\"\n",
        "\n",
        "    N = G.shape[0]\n",
        "    weighted_G = np.ones([N, N])\n",
        "    for col in range(N):\n",
        "        weighted_G[:col, col] = weighted_G[:col, col] * col\n",
        "        weighted_G[col, :col] = weighted_G[col, :col] * col\n",
        "    weighted_G += 1 # pushes second node's identifier to 2\n",
        "    # removes diagonals, simultaneously resetting first node's identifier to 0\n",
        "    weighted_G = np.multiply(G, weighted_G) \n",
        "    # place 1 to N along the diagonal\n",
        "    np.fill_diagonal(weighted_G, list(range(1, N + 1)))\n",
        "    # set all zeros to be non-edges (i.e. at inf distance)\n",
        "    weighted_G[weighted_G == 0] = np.inf\n",
        "    # remove 1 from everywhere to ensure first node has identifier 0\n",
        "    weighted_G -= 1\n",
        "    \n",
        "    return weighted_G\n",
        "\n",
        "def betti_numbers(G, maxdim = 2, dim = 1):\n",
        "  \"\"\"\n",
        "  Given a NetworkX graph object, computes number of topological cycles \n",
        "  (i.e. Betti numbers) of various dimensions upto maxdim.\n",
        "  \"\"\"\n",
        "  adj = nx.to_numpy_array(G)\n",
        "  adj[adj == 0] = np.inf # set unconnected nodes to be infinitely apart\n",
        "  np.fill_diagonal(adj, 1) # set diagonal to 1 to indicate all nodes are born at once\n",
        "  bars = ripser(adj, distance_matrix = True, maxdim = maxdim)['dgms'] # returns barcodes\n",
        "  bars_list = list(zip(range(maxdim + 1), bars))\n",
        "  bettis_dict = dict([(dim, len(cycles)) for (dim, cycles) in bars_list])\n",
        "\n",
        "  return bettis_dict[dim] # return Betti number for dimension of interest\n",
        "\n",
        "def get_barcode(filt_mat, maxdim = 2):\n",
        "    \"\"\"\n",
        "    Calculates the persistent homology for a given filtration matrix\n",
        "    ``filt_mat``, default dimensions 0 through 2. Wraps ripser.\n",
        "    \"\"\"\n",
        "\n",
        "    b = ripser(filt_mat, distance_matrix = True, maxdim = maxdim)['dgms']\n",
        "\n",
        "    return list(zip(range(maxdim + 1), b))\n",
        "\n",
        "def betti_curves(bars, length):\n",
        "    \"\"\"\n",
        "    Takes in bars and returns the betti curves\n",
        "    \"\"\"\n",
        "\n",
        "    bettis = np.zeros((len(bars), length))\n",
        "    for i in range(bettis.shape[0]):\n",
        "        bn = bars[i][1]\n",
        "        for bar in bn:\n",
        "            birth = int(bar[0])\n",
        "            death = length+1 if np.isinf(bar[1]) else int(bar[1]+1)\n",
        "            bettis[i][birth:death] += 1\n",
        "\n",
        "    return bettis\n",
        "\n",
        "def plot_bettis(bettis):\n",
        "  \n",
        "  N = bettis.shape[1]\n",
        "  colors = ['xkcd:emerald green', 'xkcd:tealish', 'xkcd:peacock blue']\n",
        "  for i in range(3):\n",
        "    plt.plot(list(range(N)), bettis[i], color = colors[i], \n",
        "             label = '$\\\\beta_{}$'.format(i), \n",
        "             linewidth = 1)\n",
        "  plt.xlabel('Nodes')\n",
        "  plt.ylabel('Number of Cycles')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "igNip2xJstvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 20 # number of nodes\n",
        "m = 2 # number of edges from new node (Barabasi-Albert)\n",
        "p = 0.4 # probability of edge creation (Erdos-Renyi)\n",
        "\n",
        "num_environments = 10\n",
        "environments = []\n",
        "feature = nx.average_clustering\n",
        "# feature = betti_numbers\n",
        "\n",
        "for i in range(num_environments):\n",
        "  G = nx.erdos_renyi_graph(n = N, p = p)\n",
        "  # G = nx.barabasi_albert_graph(n = N, m = m)\n",
        "  G = node_featurizer(G)\n",
        "  environment = GraphEnvironment(i, G, feature)\n",
        "  environments.append(environment)\n",
        "\n",
        "environments = MultipleEnvironments(environments)"
      ],
      "metadata": {
        "id": "tcoqPfdOi3n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 500\n",
        "agent = RandomAgent(environments)\n",
        "all_episode_returns_random, _ = learn_environments(environments, agent, num_steps)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Return')\n",
        "plt.plot(np.mean(np.array(all_episode_returns_random), axis = 0), \n",
        "         label = 'random', color = 'blue')"
      ],
      "metadata": {
        "id": "5lPH8Ksdfzsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10\n",
        "\n",
        "agent = RandomAgent(environments)\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)"
      ],
      "metadata": {
        "id": "5PAGfo8WKJTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10\n",
        "\n",
        "agent = RandomAgent(environments)\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_random = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_random = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = HighestDegreeAgent(environments)\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_degree = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_degree = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = GreedyAgent(environments)\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_greedy = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_greedy = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "0vq4L_7PvVQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {'num_node_features': 9,\n",
        "                   'GNN_latent_dimensions': 64,\n",
        "                   'embedding_dimensions': 64,\n",
        "                   'QN_latent_dimensions': 32,\n",
        "                   'buffer_size': 100000,\n",
        "                   'train_start': 320,\n",
        "                   'batch_size': 32,\n",
        "                   'learn_every': 4,\n",
        "                   'epsilon_initial': 0.1,\n",
        "                   'epsilon_decay_rate': 1,\n",
        "                   'epsilon_min': 0.1,\n",
        "                   'discount_factor': 0.75,\n",
        "                   'learning_rate': 3e-4}\n",
        "\n",
        "embedding_module = GNN(hyperparameters)\n",
        "q_net = QN(hyperparameters)\n",
        "\n",
        "embedding_module_untrained = deepcopy(embedding_module)\n",
        "q_net_untrained = deepcopy(q_net)\n",
        "\n",
        "\n",
        "agent = DQNAgent(environments, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 1, epsilon_decay_rate = 1, epsilon_min = 1)\n",
        "\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_DQN = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "S0DE25dEYJdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Untrained')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "lv0ybLUvYH_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = hyperparameters['epsilon_initial'] # exploration rate\n",
        "epsilon_decay_rate = hyperparameters['epsilon_decay_rate'] \n",
        "epsilon_min = hyperparameters['epsilon_min']\n",
        "\n",
        "discount_factor = hyperparameters['discount_factor']\n",
        "learning_rate = hyperparameters['learning_rate']\n",
        "num_steps = 25000 # number of steps in each environment\n",
        "\n",
        "replay_buffer = ReplayBuffer(hyperparameters['buffer_size'])\n",
        "train_start = hyperparameters['train_start']\n",
        "batch_size = hyperparameters['batch_size']\n",
        "learn_every = hyperparameters['learn_every']\n",
        "\n",
        "optimizer = torch.optim.Adam([{'params': embedding_module.parameters()}, \n",
        "                              {'params': q_net.parameters()}], \n",
        "                             lr = learning_rate)\n",
        "agent = DQNAgent(environments, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer, train_start, batch_size, \n",
        "                 learn_every, \n",
        "                 optimizer, \n",
        "                 epsilon, epsilon_decay_rate, epsilon_min)"
      ],
      "metadata": {
        "id": "T1s4LAjJw3I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prun_test():\n",
        "\n",
        "  returns, _ = learn_environments(environments, agent, \n",
        "                                  num_steps, \n",
        "                                  discount_factor, \n",
        "                                  verbose = True)\n",
        "  return returns"
      ],
      "metadata": {
        "id": "t8TQzR5K85wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%prun returns = prun_test()"
      ],
      "metadata": {
        "id": "5GuCH1RdG-l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Return')\n",
        "plt.plot(np.mean(np.array(returns), axis = 0), \n",
        "         label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "MTFcW3aEjck3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Individual Environment Plots\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "for idx, environment_returns in enumerate(returns):\n",
        "  plt.plot(environment_returns, label = f'Env: {idx}')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3f4OWgrQ5pKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(returns)"
      ],
      "metadata": {
        "id": "DS9Oiybs58HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.plot(returns[idx], label = f'Env: {idx}')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZqdwrXzd5zwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_NX_subgraph.cache_info()"
      ],
      "metadata": {
        "id": "XzAmue91joDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_PyG_subgraph.cache_info()"
      ],
      "metadata": {
        "id": "xmK0oH_jjra6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_feature_value.cache_info()"
      ],
      "metadata": {
        "id": "eFIH5RN7jvN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_neighbors.cache_info()"
      ],
      "metadata": {
        "id": "VZ4g72TGjxup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(environments, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "\n",
        "all_rewards, all_feature_values = simulate(environments, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_DQN = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "DcQTuDY8j57S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Trained')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "GC5hwLmridM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_environments = 5\n",
        "\n",
        "environments_test = []\n",
        "\n",
        "for i in range(num_environments):\n",
        "  G = nx.erdos_renyi_graph(n = N, p = p)\n",
        "  # G = nx.barabasi_albert_graph(n = N, m = m)\n",
        "  G = node_featurizer(G)\n",
        "  environment = GraphEnvironment(i, G, feature)\n",
        "  environments_test.append(environment)\n",
        "\n",
        "environments_test = MultipleEnvironments(environments_test)"
      ],
      "metadata": {
        "id": "evu5yfOVkr9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RandomAgent(environments_test)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_random = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_random = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = HighestDegreeAgent(environments_test)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_degree = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_degree = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = GreedyAgent(environments_test)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_greedy = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_greedy = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = DQNAgent(environments_test, \n",
        "                 embedding_module, q_net, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_DQN = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_DQN = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)\n",
        "\n",
        "agent = DQNAgent(environments_test, \n",
        "                 embedding_module_untrained, q_net_untrained, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_DQN_untrained = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_DQN_untrained = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "fo9H0o9ClImY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(environments_test, \n",
        "                 embedding_module_untrained, q_net_untrained, \n",
        "                 replay_buffer = None, train_start = None, batch_size = None, \n",
        "                 learn_every = None, \n",
        "                 optimizer = None, \n",
        "                 epsilon = 0, epsilon_decay_rate = None, epsilon_min = None)\n",
        "all_rewards, all_feature_values = simulate(environments_test, \n",
        "                                           agent, \n",
        "                                           num_episodes)\n",
        "rewards_mean_DQN_untrained = np.mean(np.mean(np.array(all_rewards), axis = 0), axis = 0)\n",
        "feature_values_mean_DQN_untrained_2 = np.mean(np.mean(np.array(all_feature_values), axis = 0), axis = 0)"
      ],
      "metadata": {
        "id": "JkBSzMJf4Kv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Trained - Unseen')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN_untrained, label = 'DQN', color = 'black')\n",
        "plt.plot(feature_values_mean_DQN_untrained_2, label = 'DQN untrained', color = 'black', linestyle = 'dashed')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "XdLE1Q8Q4W0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NoOH9RUPywX"
      },
      "outputs": [],
      "source": [
        "plt.title('Trained - Unseen')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.plot(feature_values_mean_random, label = 'random', color = 'blue')\n",
        "plt.plot(feature_values_mean_degree, label = 'max degree', color = 'orange')\n",
        "plt.plot(feature_values_mean_greedy, label = 'greedy', color = 'green')\n",
        "plt.plot(feature_values_mean_DQN, label = 'DQN', color = 'black')\n",
        "plt.plot(feature_values_mean_DQN_untrained, label = 'DQN untrained', color = 'black', linestyle = 'dashed')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vANOoimX-IEz"
      },
      "outputs": [],
      "source": [
        "#@title Miscellaneous code\n",
        "\n",
        "# class Batch(NamedTuple):\n",
        "\n",
        "#   state_embedding: torch.Tensor\n",
        "#   action: torch.Tensor\n",
        "#   next_state_embedding: torch.Tensor\n",
        "#   reward: torch.Tensor\n",
        "#   discount: torch.Tensor\n",
        "\n",
        "# class ReplayBuffer():\n",
        "\n",
        "#   def __init__(self, state_dimensionality, action_dimensionality, buffer_size):\n",
        "\n",
        "#     self.buffer_size = buffer_size\n",
        "#     self.ptr = 0\n",
        "#     self.n_samples = 0\n",
        "#     self.state_embedding = torch.zeros(buffer_size, state_dimensionality, \n",
        "#                                        dtype = torch.float32)\n",
        "#     self.action = torch.zeros(buffer_size, action_dimensionality, \n",
        "#                               dtype = torch.int64)\n",
        "#     self.next_state_embedding = torch.zeros(buffer_size, state_dimensionality, \n",
        "#                                             dtype = torch.float32)\n",
        "#     self.reward = torch.zeros(buffer_size, 1, \n",
        "#                               dtype = torch.float32)\n",
        "#     self.discount = torch.zeros(buffer_size, 1, \n",
        "#                                 dtype = torch.float32)\n",
        "\n",
        "\n",
        "#   def add(self, state_embedding, action, next_state_embedding, reward, discount):\n",
        "\n",
        "#     self.state_embedding[self.ptr] = state_embedding\n",
        "#     self.action[self.ptr] = action\n",
        "#     self.next_state_embedding[self.ptr] = next_state_embedding\n",
        "#     self.reward[self.ptr] = reward\n",
        "#     self.discount[self.ptr] = discount\n",
        "    \n",
        "#     if self.n_samples < self.buffer_size:\n",
        "#       self.n_samples += 1\n",
        "\n",
        "#     self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "\n",
        "#   def sample(self, batch_size):\n",
        "\n",
        "#     idx = np.random.choice(self.n_samples, batch_size)    \n",
        "#     state_embedding = self.state_embedding[idx]\n",
        "#     action = self.action[idx]\n",
        "#     next_state_embedding = self.next_state_embedding[idx]\n",
        "#     reward = self.reward[idx]\n",
        "#     discount = self.discount[idx]\n",
        "    \n",
        "#     return Batch(state_embedding, action, next_state_embedding, reward, discount)\n",
        "\n",
        "# class Batch(NamedTuple):\n",
        "\n",
        "#   state: torch_geometric.data.data.Data\n",
        "#   action: torch.Tensor\n",
        "#   next_state: torch_geometric.data.data.Data\n",
        "#   reward: torch.Tensor\n",
        "#   discount: torch.Tensor\n",
        "\n",
        "# class ReplayBuffer():\n",
        "\n",
        "#   def __init__(self, state_dimensionality, buffer_size):\n",
        "\n",
        "#     self.buffer_size = buffer_size\n",
        "#     self.ptr = 0\n",
        "#     self.n_samples = 0\n",
        "    \n",
        "#     self.state = torch.zeros(buffer_size, state_dimensionality, dtype = torch.float32)\n",
        "#     self.action = torch.zeros(buffer_size, 1, dtype = torch.int64)\n",
        "#     self.next_state = torch.zeros(buffer_size, state_dimensionality, dtype = torch.float32)\n",
        "#     self.reward = torch.zeros(buffer_size, 1, dtype = torch.float32)\n",
        "#     self.discount = torch.zeros(buffer_size, 1, dtype = torch.float32)\n",
        "\n",
        "\n",
        "#   def add(self, state, action, next_state, reward, discount):\n",
        "\n",
        "#     self.state[self.ptr] = state\n",
        "#     self.action[self.ptr] = action\n",
        "#     self.next_state[self.ptr] = next_state\n",
        "#     self.reward[self.ptr] = reward\n",
        "#     self.discount[self.ptr] = discount\n",
        "    \n",
        "#     if self.n_samples < self.buffer_size:\n",
        "#       self.n_samples += 1\n",
        "\n",
        "#     self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "\n",
        "#   def sample(self, batch_size):\n",
        "\n",
        "#     idx = np.random.choice(self.n_samples, batch_size)    \n",
        "#     state = self.state[idx]\n",
        "#     action = self.action[idx]\n",
        "#     next_state = self.next_state[idx]\n",
        "#     reward = self.reward[idx]\n",
        "#     discount = self.discount[idx]\n",
        "    \n",
        "#     return Batch(state, action, next_state, reward, discount)\n",
        "\n",
        "# class QNetworkAgent():\n",
        "\n",
        "#   def __init__(self, environment, embedding_module, q_net, optimizer, \n",
        "#                replay_buffer, batch_size, epsilon):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "#     self.environment = environment\n",
        "#     self.embedding_module = embedding_module\n",
        "#     self.q_net = q_net\n",
        "#     self.optimizer = optimizer\n",
        "\n",
        "#     self.replay_buffer = replay_buffer\n",
        "#     self.batch_size = batch_size # number of points to sample from the buffer\n",
        "\n",
        "#     self.epsilon = epsilon\n",
        "\n",
        "#   def choose_action(self):\n",
        "\n",
        "#     available_actions = self.environment.get_actions()\n",
        "\n",
        "#     with torch.no_grad(): # gradients are not needed when selecting actions\n",
        "#       # Option 1: perform epsilon-greedy exploration\n",
        "#       if np.random.uniform() < self.epsilon:\n",
        "#         return random.choice(available_actions)\n",
        "#       # Option 2: select action greedily based on q-values\n",
        "#       else:\n",
        "#         state_embedding = self.embedding_module(self.environment.state_PyG)\n",
        "#         q_values = self.q_net(state_embedding)\n",
        "#         valid_q_values = q_values[0][available_actions]\n",
        "#         action_idx = torch.argmax(valid_q_values).item()\n",
        "#         return available_actions[action_idx]\n",
        "\n",
        "#   def train(self, state, action, next_state, reward, discount, step):\n",
        "\n",
        "#     # with torch.no_grad():\n",
        "#     #   state_embedding = self.embedding_module(state)\n",
        "#     #   next_state_embedding = self.embedding_module(next_state)\n",
        "\n",
        "#     # self.replay_buffer.add(state_embedding, action, \n",
        "#     #                        next_state_embedding, reward, discount)\n",
        "\n",
        "#     self.replay_buffer.add(state, action, next_state, reward, discount)\n",
        "\n",
        "#     if step < self.batch_size:\n",
        "#       return # do not train until the buffer has at least one data batch\n",
        "\n",
        "#     batch = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "#     # (1) Predict a q-value based on state-action pair\n",
        "#     # state_embedding = self.embedding_module(state)\n",
        "#     q_values = self.q_net(self.embedding_module(batch.state))\n",
        "#     q_predicted = q_values.gather(1, batch.action)\n",
        "\n",
        "#     # (2) Compute target q-value; no gradient is needed for the target\n",
        "#     with torch.no_grad():\n",
        "#       # next_q_values = self.q_net(batch.next_state_embedding)\n",
        "#       next_q_values = self.q_net(self.embedding_module(batch.next_state))\n",
        "#       q_target = next_q_values.max(dim = 1)[0].view(-1, 1)\n",
        "#       q_target = batch.reward + batch.discount * q_target\n",
        "\n",
        "#     # (3) Compute MSE loss between the predicted and target q-values\n",
        "#     loss = F.mse_loss(q_predicted, q_target).mean()\n",
        "\n",
        "#     # (4) Backpropagate gradients\n",
        "#     self.optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     self.optimizer.step()\n",
        "\n",
        "# num_episodes = 500\n",
        "# discount_factor = 0.25\n",
        "\n",
        "# all_episode_returns_q_net = learn_environment(environment, agent, \n",
        "#                                               num_episodes, discount_factor)\n",
        "\n",
        "# print(compute_Frobenius_norm(embedding_module), compute_Frobenius_norm(q_net))\n",
        "\n",
        "# plt.figure()\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Return')\n",
        "# plt.plot(all_episode_returns_q_net, label = 'q-net')\n",
        "# plt.legend()\n",
        "\n",
        "# states_NX, states_PyG, rewards, feature_values, steps = simulate(environment, agent, 10)\n",
        "# rewards_mean_QNet = np.mean(np.array(rewards), axis = 0)\n",
        "# feature_values_mean_QNet = np.mean(np.array(feature_values), axis = 0)\n",
        "\n",
        "# plt.figure()\n",
        "# plt.xlabel('Step')\n",
        "# plt.ylabel('Average Clustering')\n",
        "# plt.plot(feature_values_mean_random, label = 'random')\n",
        "# plt.plot(feature_values_mean_degree, label = 'max degree')\n",
        "# plt.plot(feature_values_mean_greedy, label = 'greedy')\n",
        "# plt.plot(feature_values_mean_QNet, label = 'q-net')\n",
        "# plt.legend()\n",
        "\n",
        "# agent = QNetworkAgent(environment, embedding_module, q_net, optimizer, epsilon)\n",
        "# all_episode_returns_q_net = learn_environment(environment, agent, \n",
        "#                                               500, discount_factor)\n",
        "\n",
        "# def learn_environment(environment, agent, num_episodes, \n",
        "#                       discount_factor = None, verbose = True):\n",
        "#   \"\"\"learn_environment() simulates agent-environment interaction for \n",
        "#   a number of steps specified by num_steps and trains the agent if it is \n",
        "#   capable of being trained. Note that the interaction ends after a finite \n",
        "#   number of steps regardless of whether the ongoing episode has terminated.\"\"\"\n",
        "\n",
        "#   all_episode_returns = []\n",
        "#   all_episode_feature_values = []\n",
        "\n",
        "#   for episode in tqdm(range(num_episodes), disable = not verbose):\n",
        "\n",
        "#     state_dict, terminal, info = environment.reset()\n",
        "#     episode_return = 0 # to track cumulative reward (i.e. return) in ongoing episode\n",
        "#     episode_feature_values = [info['feature_value']]\n",
        "\n",
        "#     while not terminal:\n",
        "#       action = agent.choose_action()\n",
        "#       next_state_dict, reward, terminal, info = environment.step(action)\n",
        "#       episode_return += reward\n",
        "#       episode_feature_values.append(info['feature_value'])\n",
        "\n",
        "#       if agent.is_trainable:\n",
        "#         discount = discount_factor * (1 - terminal)\n",
        "#         agent.train(state_dict, action, next_state_dict, reward, discount) \n",
        "\n",
        "#       state_dict = next_state_dict\n",
        "      \n",
        "#     all_episode_returns.append(episode_return)\n",
        "#     all_episode_feature_values.append(episode_feature_values)\n",
        "\n",
        "#   environment.reset()\n",
        "\n",
        "#   return all_episode_returns, all_episode_feature_values\n",
        "\n",
        "# def learn_environment(environment, agent, num_steps, \n",
        "#                       discount_factor = None, verbose = True):\n",
        "#   \"\"\"\n",
        "#   Simulate agent-environment interaction for a specified number of steps\n",
        "#   and train the agent if it is capable of being trained. Note that the \n",
        "#   interaction ends after the specified number of steps regardless of whether the \n",
        "#   ongoing episode has terminated.\n",
        "#   \"\"\"\n",
        "\n",
        "#   all_episode_returns = []\n",
        "#   all_episode_feature_values = []\n",
        "\n",
        "#   episode_return = 0\n",
        "#   episode_feature_values = []\n",
        "#   state_dict, terminal, info = environment.reset()\n",
        "\n",
        "#   for step in tqdm(range(num_steps), disable = not verbose):\n",
        "#     action = agent.choose_action()\n",
        "#     next_state_dict, reward, terminal, info = environment.step(action)\n",
        "#     episode_return += reward\n",
        "#     episode_feature_values.append(info['feature_value'])\n",
        "\n",
        "#     if agent.is_trainable:\n",
        "#       discount = discount_factor * (1 - terminal)\n",
        "#       agent.train(state_dict, action, next_state_dict, reward, discount, info) \n",
        "\n",
        "#     if terminal:\n",
        "#       all_episode_returns.append(episode_return)\n",
        "#       episode_return = 0\n",
        "#       all_episode_feature_values.append(episode_feature_values)\n",
        "#       episode_feature_values = []\n",
        "#       state_dict, terminal, info = environment.reset()\n",
        "\n",
        "#     else:\n",
        "#       state_dict = next_state_dict\n",
        "\n",
        "#   environment.reset()\n",
        "\n",
        "#   return all_episode_returns, all_episode_feature_values\n",
        "\n",
        "# class QNetworkAgent_Vanilla():\n",
        "\n",
        "#   def __init__(self, environment, \n",
        "#                embedding_module, q_net, \n",
        "#                optimizer, \n",
        "#                epsilon, epsilon_decay_rate, epsilon_min):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "#     self.environment = environment\n",
        "#     self.embedding_module = embedding_module\n",
        "#     self.q_net = q_net\n",
        "#     self.optimizer = optimizer\n",
        "\n",
        "\n",
        "#     self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "#     self.epsilon_decay_rate = epsilon_decay_rate\n",
        "#     self.epsilon_min = epsilon_min\n",
        "\n",
        "#   def choose_action(self):\n",
        "\n",
        "#     available_actions = self.environment.get_actions(self.environment.visited)\n",
        "\n",
        "#     new_subgraph_list = [] # list to store all possible next states\n",
        "#     for action in available_actions:\n",
        "#       visited_nodes_new = deepcopy(self.environment.visited)\n",
        "#       visited_nodes_new.append(action)\n",
        "#       new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "#       new_subgraph_list.append(new_subgraph)\n",
        "\n",
        "#     # create a batch to allow for a single forward pass\n",
        "#     new_subgraph_batch = Batch.from_data_list(new_subgraph_list)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#       q_values = self.q_net(self.embedding_module(new_subgraph_batch.x, \n",
        "#                                                   new_subgraph_batch.edge_index, \n",
        "#                                                   new_subgraph_batch.batch))\n",
        "      \n",
        "#     if torch.rand(1) < self.epsilon: # explore\n",
        "#       action = np.random.choice(available_actions)\n",
        "#     else: # exploit\n",
        "#       action_idx = torch.argmax(q_values).item()\n",
        "#       action = available_actions[action_idx]\n",
        "\n",
        "#     return action\n",
        "\n",
        "#   def train(self, state_dict, action, next_state_dict, reward, discount, info):\n",
        "    \n",
        "#     # (1) Build state + action (= next_state) subgraph \n",
        "#     visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "#     visited_nodes_new.append(action)\n",
        "#     assert visited_nodes_new == next_state_dict['visited'], \"train() assertion failed.\"\n",
        "#     new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "    \n",
        "#     # (2) Pass next_state subgraph through ANN to get predicted q-value\n",
        "#     q_prediction = self.q_net(self.embedding_module(new_subgraph.x, \n",
        "#                                                    new_subgraph.edge_index))\n",
        "    \n",
        "#     # (3) Compute target q-value\n",
        "#     available_actions = self.environment.get_actions(next_state_dict['visited'])\n",
        "#     if available_actions: # states that are terminal have no available actions\n",
        "#       new_subgraphs = []\n",
        "#       for action in available_actions:\n",
        "#         visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "#         visited_nodes_new.append(action)\n",
        "#         new_subgraph = self.environment.graph_PyG.subgraph(torch.tensor(visited_nodes_new))\n",
        "#         new_subgraphs.append(new_subgraph)\n",
        "\n",
        "#       batch = Batch.from_data_list(new_subgraphs)\n",
        "#       with torch.no_grad():\n",
        "#         q_target = self.q_net(self.embedding_module(batch.x, \n",
        "#                                                     batch.edge_index, \n",
        "#                                                     batch.batch))\n",
        "#         q_target = q_target.max().view(-1, 1)\n",
        "#         q_target = reward + discount * q_target\n",
        "      \n",
        "#       # (4) Compute MSE loss between the predicted and target q-values\n",
        "#       loss = F.mse_loss(q_prediction, q_target)\n",
        "\n",
        "#       # (5) Backpropagate gradients\n",
        "#       self.optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       self.optimizer.step()\n",
        "\n",
        "#       # (6) Decrease exploration rate\n",
        "#       self.epsilon *= self.epsilon_decay_rate\n",
        "#       self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "# class DQNAgent_SingleEnvironment():\n",
        "\n",
        "#   def __init__(self, environment, \n",
        "#                embedding_module, q_net, \n",
        "#                replay_buffer, train_start, batch_size, \n",
        "#                learn_every,\n",
        "#                optimizer, \n",
        "#                epsilon, epsilon_decay_rate, epsilon_min):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.is_trainable = True # useful to manage control flow during simulations\n",
        "\n",
        "#     self.environment = environment\n",
        "\n",
        "#     self.embedding_module = embedding_module\n",
        "#     self.q_net = q_net\n",
        "\n",
        "#     self.target_embedding_module = deepcopy(embedding_module)\n",
        "#     self.target_q_net = deepcopy(q_net)\n",
        "    \n",
        "#     # disable gradients for target networks\n",
        "#     for parameter in self.target_embedding_module.parameters():\n",
        "#       parameter.requires_grad = False\n",
        "#     for parameter in self.target_q_net.parameters():\n",
        "#       parameter.requires_grad = False\n",
        "    \n",
        "#     self.replay_buffer = replay_buffer\n",
        "#     self.train_start = train_start # specify burn-in period\n",
        "#     self.batch_size = batch_size\n",
        "#     self.learn_every = learn_every # steps between updates to target nets\n",
        "\n",
        "#     self.optimizer = optimizer\n",
        "\n",
        "#     self.epsilon = epsilon # probability with which to select a non-greedy action\n",
        "#     self.epsilon_decay_rate = epsilon_decay_rate\n",
        "#     self.epsilon_min = epsilon_min\n",
        "\n",
        "#     self.step = 0 \n",
        "\n",
        "#   def choose_action(self):\n",
        "\n",
        "#     available_actions = self.environment.get_actions(self.environment.visited)\n",
        "\n",
        "#     new_subgraphs = [] # list to store all possible next states\n",
        "#     for action in available_actions:\n",
        "#       visited_nodes_new = deepcopy(self.environment.visited)\n",
        "#       visited_nodes_new.append(action)\n",
        "#       new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "#       new_subgraphs.append(new_subgraph)\n",
        "\n",
        "#     # create a batch to allow for a single forward pass\n",
        "#     batch = Batch.from_data_list(new_subgraphs)\n",
        "#     # gradients for the target networks are disabled\n",
        "#     with torch.no_grad(): \n",
        "#       q_values = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "#                                                                 batch.edge_index, \n",
        "#                                                                 batch.batch))\n",
        "\n",
        "#     if torch.rand(1) < self.epsilon: # explore\n",
        "#       action = np.random.choice(available_actions)\n",
        "#     else: # exploit\n",
        "#       action_idx = torch.argmax(q_values).item()\n",
        "#       action = available_actions[action_idx]\n",
        "\n",
        "#     return action\n",
        "\n",
        "#   def train(self, state_dict, action, next_state_dict, reward, discount, info):\n",
        "\n",
        "#     self.replay_buffer.add(state_dict, action, \n",
        "#                            next_state_dict, \n",
        "#                            reward, discount, info)\n",
        "#     self.step += 1\n",
        "\n",
        "#     if self.step < self.train_start: # inside the burn-in period\n",
        "#       return\n",
        "\n",
        "#     # (1) Get lists of experiences from memory\n",
        "#     states, actions, next_states, rewards, discounts, info = self.replay_buffer.sample(self.batch_size)\n",
        "    \n",
        "#     # (2) Build state + action = new subgraph (technically identical to next state)\n",
        "#     new_subgraphs = []\n",
        "#     for idx, state_dict in enumerate(states):\n",
        "#       visited_nodes_new = deepcopy(state_dict['visited'])\n",
        "#       visited_nodes_new.append(actions[idx])\n",
        "#       assert visited_nodes_new == next_states[idx]['visited'], \"train() assertion failed.\"\n",
        "#       new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "#       new_subgraphs.append(new_subgraph)\n",
        "#     batch = Batch.from_data_list(new_subgraphs)\n",
        "\n",
        "#     # (3) Pass batch of next_state subgraphs through ANN to get predicted q-values\n",
        "#     q_predictions = self.q_net(self.embedding_module(batch.x, \n",
        "#                                                      batch.edge_index, \n",
        "#                                                      batch.batch))\n",
        "\n",
        "#     # (4) Compute target q-values for batch\n",
        "#     q_targets = []\n",
        "#     for idx, next_state_dict in enumerate(next_states):\n",
        "#       available_actions = self.environment.get_actions(next_state_dict['visited'])\n",
        "#       if available_actions: # terminal states have no available actions\n",
        "#         new_subgraphs = [] # each available action results in a new state\n",
        "#         for action in available_actions:\n",
        "#           visited_nodes_new = deepcopy(next_state_dict['visited'])\n",
        "#           visited_nodes_new.append(action)\n",
        "#           new_subgraph = get_PyG_subgraph(self.environment, frozenset(visited_nodes_new))\n",
        "#           new_subgraphs.append(new_subgraph)\n",
        "#         batch = Batch.from_data_list(new_subgraphs)\n",
        "#         with torch.no_grad(): # technically, no_grad() is unnecessary\n",
        "#           q_target = self.target_q_net(self.target_embedding_module(batch.x, \n",
        "#                                                                     batch.edge_index, \n",
        "#                                                                     batch.batch))\n",
        "#           q_target = q_target.max().view(-1, 1) # get the largest next q-value\n",
        "#           q_target = rewards[idx] + discounts[idx] * q_target\n",
        "#           q_targets.append(q_target)\n",
        "#       else:\n",
        "#         q_targets.append(rewards[idx])\n",
        "#     q_targets = torch.Tensor(q_targets).view(-1, 1)\n",
        "      \n",
        "#     # (5) Compute MSE loss between predicted and target q-values\n",
        "#     loss = F.mse_loss(q_predictions, q_targets).mean()\n",
        "\n",
        "#     # (6) Backpropagate gradients\n",
        "#     self.optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     self.optimizer.step()\n",
        "\n",
        "#     # (7) Copy parameters from source to target networks\n",
        "#     if self.step % self.learn_every == 0: \n",
        "#       copy_parameters_from_to(self.embedding_module, self.target_embedding_module)\n",
        "#       copy_parameters_from_to(self.q_net, self.target_q_net)\n",
        "      \n",
        "#     # (8) Decrease exploration rate\n",
        "#     self.epsilon *= self.epsilon_decay_rate\n",
        "#     self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "# run_times_2 = []\n",
        "\n",
        "# N = 20 # number of nodes\n",
        "# m = 2 # number of edges from new node (Barabasi-Albert)\n",
        "# p = 0.4 # probability of edge creation (Erdos-Renyi)\n",
        "\n",
        "# feature = nx.average_clustering\n",
        "# # feature = betti_numbers\n",
        "\n",
        "# for num_environments in range(1, 10):\n",
        "#   environments = []\n",
        "  \n",
        "#   for i in range(num_environments):\n",
        "#     G = nx.erdos_renyi_graph(n = N, p = p)\n",
        "#     # G = nx.barabasi_albert_graph(n = N, m = m)\n",
        "#     G = node_featurizer(G)\n",
        "#     environment = GraphEnvironment(i, G, feature)\n",
        "#     environments.append(environment)\n",
        "\n",
        "#   environments = MultipleEnvironments(environments)\n",
        "\n",
        "#   embedding_module = GNN(hyperparameters)\n",
        "#   q_net = QN(hyperparameters)\n",
        "#   replay_buffer = ReplayBuffer(hyperparameters['buffer_size'])\n",
        "\n",
        "#   agent = DQNAgent(environments, \n",
        "#                    embedding_module, q_net, \n",
        "#                    replay_buffer, train_start, batch_size, \n",
        "#                    learn_every, \n",
        "#                    optimizer, \n",
        "#                    epsilon, epsilon_decay_rate, epsilon_min)\n",
        "  \n",
        "#   start = time.process_time()\n",
        "#   returns, _ = learn_environments(environments, agent, \n",
        "#                                   num_steps, \n",
        "#                                   discount_factor, \n",
        "#                                   verbose = True)\n",
        "#   run_times_2.append(time.process_time() - start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyO30Ip3iyOQSu4b3HqlxI9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}